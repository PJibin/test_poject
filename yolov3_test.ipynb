{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov3 test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSiSyOyFwysf2IIqTm/mUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PJibin/test_poject/blob/main/yolov3_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('./MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc45v_7VYgjd",
        "outputId": "59e332a1-d9d1-479e-86b7-8efc4bf8b3fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd MyDrive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlFiARiRYidH",
        "outputId": "fb358555-def1-409d-cd57-435731c51d66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MyDrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JO_p07CGYehM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class preprocessing(object):\n",
        "    def __init__(self, resize_size):\n",
        "        self.resize_size = resize_size\n",
        "    \n",
        "    def resize(self, img):\n",
        "        resize_img = cv2.resize(img, (self.resize_size, self.resize_size))\n",
        "        return resize_img\n",
        "    \n",
        "    def normalize_img(self, img):\n",
        "        norm_img = img / 255.0\n",
        "        return norm_img\n",
        "    \n",
        "    def label_parsing(self, label_path):\n",
        "        info = ET.parse(label_path)\n",
        "        root = info.getroot()\n",
        "        member_object = root.findall('object')\n",
        "        label_data = []\n",
        "        labels_dict = dict(obj_class=[], xmin=[], xmax=[], ymin=[], ymax=[])\n",
        "        label_data.append(label_path)\n",
        "        for obj in member_object:\n",
        "            if obj.find('name').text == 'apple':\n",
        "                obj_class = 0\n",
        "            elif obj.find('name').text == 'banana':\n",
        "                obj_class = 1\n",
        "            elif obj.find('name').text == 'orange':\n",
        "                obj_class = 2\n",
        "            \n",
        "            labels_info = obj.find('bndbox')\n",
        "            xmin = int(labels_info.find('xmin').text)\n",
        "            xmax = int(labels_info.find('xmax').text)\n",
        "            ymin = int(labels_info.find('ymin').text)\n",
        "            ymax = int(labels_info.find('ymax').text)\n",
        "            labels_dict['xmin'].append(xmin)\n",
        "            labels_dict['xmax'].append(xmax)\n",
        "            labels_dict['ymin'].append(ymin)\n",
        "            labels_dict['ymax'].append(ymax)\n",
        "            labels_dict['obj_class'].append(obj_class)\n",
        "            label_data.append(labels_dict)\n",
        "        return label_data"
      ],
      "metadata": {
        "id": "nZHuyn2wY9Ko"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datapath = './fruit/train'\n",
        "# image_files = glob(os.path.join(datapath, '*.jpg'))\n",
        "# label_files = glob(os.path.join(datapath, '*.xml'))\n",
        "# image_files.sort(reverse=True)\n",
        "# label_files.sort(reverse=True)\n",
        "# prepro_img = preprocessing(resize_size=244)\n",
        "    \n",
        "# data = list(zip(image_files, label_files))\n",
        "# random.seed(4)\n",
        "# random.shuffle(data)\n",
        "    \n",
        "# # check the input images\n",
        "# for file in data:\n",
        "#     #print(file)\n",
        "#     image = cv2.imread(file[0])\n",
        "#     label = prepro_img.label_parsing(file[1])\n",
        "        \n",
        "#     resize_img = prepro_img.resize(image)\n",
        "#     norm_img = prepro_img.normalize_img(resize_img)\n",
        " \n",
        "#     # if (file[0].split('\\\\'))[-1] == 'apple_20.jpg':\n",
        "#     #     rect_img = image\n",
        "#     #     print(len(label[1]['obj_class']))\n",
        "#     #     for i in range(len(label[1]['obj_class'])):\n",
        "#     #         bb_color = (0, 0, 0)\n",
        "#     #         if label[1]['obj_class'][i] == 0:\n",
        "#     #             bb_color = (255, 0, 0) # blue: apple\n",
        "#     #         elif label[1]['obj_class'][i] == 1:\n",
        "#     #             bb_color = (0, 255, 0) # green: banana\n",
        "#     #         elif label[1]['obj_class'][i] == 2:\n",
        "#     #             bb_color = (0, 0, 255) # red: orange\n",
        "                \n",
        "#     #         rect_img = cv2.rectangle(rect_img, (label[1]['xmin'][i], label[1]['ymin'][i]), (label[1]['xmax'][i], label[1]['ymax'][i]), bb_color, 1)\n",
        "#     #     cv2.imwrite('bb_result.jpg', rect_img)\n",
        "     \n",
        "# with open('train.txt', 'w') as f:\n",
        "#     for file in data:\n",
        "#         label = prepro_img.label_parsing(file[1])\n",
        "#         f.write(label[0]+' ')\n",
        "#         for i in range(len(label[1]['obj_class'])):\n",
        "#             f.write('{} {} {} {} {}'.format(label[1]['obj_class'][i], label[1]['xmin'][i], label[1]['xmax'][i], label[1]['ymin'][i], label[1]['ymax'][i]))\n",
        "#         f.write('\\n')"
      ],
      "metadata": {
        "id": "7KdhXYL1ZBmX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "os.chdir(r'/content/MyDrive/MyDrive/fruit/train')\n",
        "path = r'/content/MyDrive/MyDrive/fruit/train'\n",
        "\n",
        "def xml_to_txt(path):\n",
        "    txt_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        everyrow_xml_list = []\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        everyrow_xml_list.append(path + '/' + root.find('filename').text)\n",
        "        for member in root.findall('object'):\n",
        "            xmin = str(int(member[4][0].text))\n",
        "            ymin = str(int(member[4][1].text))\n",
        "            xmax = str(int(member[4][2].text))\n",
        "            ymax = str(int(member[4][3].text))\n",
        "            if xmin==\"0\":\n",
        "                xmin=\"1\"\n",
        "            if ymin==\"0\":\n",
        "                ymin=\"1\"\n",
        "            if xmax==\"0\":\n",
        "                xmax=\"1\"\n",
        "            if ymax==\"0\":\n",
        "                ymax=\"1\"\n",
        "                \n",
        "            if member.find('name').text == 'apple':\n",
        "              obj_class = \"0\"\n",
        "            elif member.find('name').text == 'banana':\n",
        "              obj_class = \"1\"\n",
        "            elif member.find('name').text == 'orange':\n",
        "              obj_class = \"2\"\n",
        "            value = xmin+','+ymin+','+xmax+','+ymax+','+obj_class\n",
        "            everyrow_xml_list.append(value)\n",
        "        txt_list.append(everyrow_xml_list)#image_path x_min,y_min,x_max,y_max,class_id  x_min,y_min,x_max,y_max,class_id ……\n",
        "    return txt_list\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    image_path = path\n",
        "    xml2txt_list = xml_to_txt(image_path)\n",
        "    with open(r'/content/MyDrive/MyDrive/fruit/train.txt', 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f,delimiter=' ')\n",
        "        writer.writerows(xml2txt_list)\n",
        "    print('Successfully converted xml to txt.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvpfODjVVjb4",
        "outputId": "58dec64e-d4a3-4beb-e46c-fddedc8fe9be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted xml to txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(r'/content/MyDrive/MyDrive/fruit/test')\n",
        "path = r'/content/MyDrive/MyDrive/fruit/test'\n",
        "\n",
        "def xml_to_txt(path):\n",
        "    txt_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        everyrow_xml_list = []\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        everyrow_xml_list.append(path + '/' + root.find('filename').text)\n",
        "        for member in root.findall('object'):\n",
        "            xmin = str(int(member[4][0].text))\n",
        "            ymin = str(int(member[4][1].text))\n",
        "            xmax = str(int(member[4][2].text))\n",
        "            ymax = str(int(member[4][3].text))\n",
        "            if xmin==\"0\":\n",
        "                xmin=\"1\"\n",
        "            if ymin==\"0\":\n",
        "                ymin=\"1\"\n",
        "            if xmax==\"0\":\n",
        "                xmax=\"1\"\n",
        "            if ymax==\"0\":\n",
        "                ymax=\"1\"\n",
        "            if member.find('name').text == 'apple':\n",
        "              obj_class = \"0\"\n",
        "            elif member.find('name').text == 'banana':\n",
        "              obj_class = \"1\"\n",
        "            elif member.find('name').text == 'orange':\n",
        "              obj_class = \"2\"\n",
        "            value = xmin+','+ymin+','+xmax+','+ymax+','+obj_class\n",
        "            everyrow_xml_list.append(value)\n",
        "        txt_list.append(everyrow_xml_list)#image_path x_min,y_min,x_max,y_max,class_id  x_min,y_min,x_max,y_max,class_id ……\n",
        "    return txt_list\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    image_path = path\n",
        "    xml2txt_list = xml_to_txt(image_path)\n",
        "    with open(r'/content/MyDrive/MyDrive/fruit/test.txt', 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f,delimiter=' ')\n",
        "        writer.writerows(xml2txt_list)\n",
        "    print('Successfully converted xml to txt.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6dfdknka9Lj",
        "outputId": "687fef7c-30a5-4ed3-acf9-511c06ece5d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted xml to txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# datapath = './fruit/test'\n",
        "# image_files_test = glob(os.path.join(datapath, '*.jpg'))\n",
        "# label_files_test = glob(os.path.join(datapath, '*.xml'))\n",
        "# image_files_test.sort(reverse=True)\n",
        "# label_files_test.sort(reverse=True)\n",
        "# prepro_img = preprocessing(resize_size=244)\n",
        "    \n",
        "# data = list(zip(image_files_test, label_files_test))\n",
        "# random.seed(4)\n",
        "# random.shuffle(data)\n",
        "    \n",
        "# # check the input images\n",
        "# for file in data:\n",
        "#     #print(file)\n",
        "#     image = cv2.imread(file[0])\n",
        "#     label = prepro_img.label_parsing(file[1])\n",
        "        \n",
        "#     resize_img = prepro_img.resize(image)\n",
        "#     norm_img = prepro_img.normalize_img(resize_img)\n",
        " \n",
        "#     if (file[0].split('\\\\'))[-1] == 'mixed_8.jpg':\n",
        "#         rect_img = image\n",
        "#         print(len(label[1]['obj_class']))\n",
        "#         for i in range(len(label[1]['obj_class'])):\n",
        "#             bb_color = (0, 0, 0)\n",
        "#             if label[1]['obj_class'][i] == 0:\n",
        "#                 bb_color = (255, 0, 0) # blue: apple\n",
        "#             elif label[1]['obj_class'][i] == 1:\n",
        "#                 bb_color = (0, 255, 0) # green: banana\n",
        "#             elif label[1]['obj_class'][i] == 2:\n",
        "#                 bb_color = (0, 0, 255) # red: orange\n",
        "                \n",
        "#             rect_img = cv2.rectangle(rect_img, (label[1]['xmin'][i], label[1]['ymin'][i]), (label[1]['xmax'][i], label[1]['ymax'][i]), bb_color, 1)\n",
        "#         cv2.imwrite('bb_result.jpg', rect_img)\n",
        "     \n",
        "# with open('test.txt', 'w') as f:\n",
        "#     for file in data:\n",
        "#         label = prepro_img.label_parsing(file[1])\n",
        "#         f.write(label[0]+' ')\n",
        "#         for i in range(len(label[1]['obj_class'])):\n",
        "#             f.write('{} {} {} {} {}'.format(label[1]['obj_class'][i], label[1]['xmin'][i], label[1]['xmax'][i], label[1]['ymin'][i], label[1]['ymax'][i]))\n",
        "#         f.write('\\n')"
      ],
      "metadata": {
        "id": "5HvPnylJi1LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BnE9fc6WLKp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.19.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "CNbK_l4dbYmJ",
        "outputId": "34f2e8d4-2b9f-4101-9eb6-fda99a3fee27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19.0\n",
            "  Downloading numpy-1.19.0-cp37-cp37m-manylinux2010_x86_64.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 3.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.0 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.0 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "YOLO_DARKNET_WEIGHTS        = r'/content/MyDrive/MyDrive/fruit/yoloc3.weights'\n",
        "YOLO_COCO_CLASSES           = r'/content/MyDrive/MyDrive/fruit/coco.names'\n",
        "YOLO_STRIDES                = [8, 16, 32]\n",
        "YOLO_IOU_LOSS_THRESH        = 0.5\n",
        "YOLO_ANCHOR_PER_SCALE       = 3\n",
        "YOLO_MAX_BBOX_PER_SCALE     = 100\n",
        "YOLO_INPUT_SIZE             = 416\n",
        "YOLO_ANCHORS                = [[[10,  13], [16,   30], [33,   23]],\n",
        "                               [[30,  61], [62,   45], [59,  119]],\n",
        "                               [[116, 90], [156, 198], [373, 326]]]\n",
        "# Train options\n",
        "TRAIN_CLASSES               = r'/content/MyDrive/MyDrive/fruit/fruits.names'\n",
        "TRAIN_ANNOT_PATH            = r'/content/MyDrive/MyDrive/fruit/train.txt'\n",
        "TRAIN_BATCH_SIZE            = 4\n",
        "TRAIN_INPUT_SIZE            = 416\n",
        "TRAIN_DATA_AUG              = True\n",
        "TRAIN_TRANSFER              = False\n",
        "TRAIN_FROM_CHECKPOINT       = False #\"./checkpoints_furits/yolov3_custom\"\n",
        "\n",
        "TRAIN_LR_INIT               = 1e-4\n",
        "TRAIN_LR_END                = 1e-6\n",
        "TRAIN_WARMUP_EPOCHS         = 2\n",
        "TRAIN_EPOCHS                = 20\n",
        "\n",
        "# TEST options\n",
        "TEST_ANNOT_PATH             = r'/content/MyDrive/MyDrive/fruit/test.txt'\n",
        "TEST_BATCH_SIZE             = 4\n",
        "TEST_INPUT_SIZE             = 416\n",
        "TEST_DATA_AUG               = False"
      ],
      "metadata": {
        "id": "Mgbub4a_bxne"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import colorsys\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "4Ca7Dwkhhni7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo_weights(model, weights_file):\n",
        "    tf.keras.backend.clear_session() # used to reset layer names\n",
        "    # load Darknet original weights to Keras model\n",
        "    with open(weights_file, 'rb') as wf:\n",
        "        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
        "\n",
        "        j = 0\n",
        "        for i in range(75):\n",
        "            if i > 0:\n",
        "                conv_layer_name = 'conv2d_%d' %i\n",
        "            else:\n",
        "                conv_layer_name = 'conv2d'\n",
        "                \n",
        "            if j > 0:\n",
        "                bn_layer_name = 'batch_normalization_%d' %j\n",
        "            else:\n",
        "                bn_layer_name = 'batch_normalization'\n",
        "            \n",
        "            conv_layer = model.get_layer(conv_layer_name)\n",
        "            filters = conv_layer.filters\n",
        "            k_size = conv_layer.kernel_size[0]\n",
        "            in_dim = conv_layer.input_shape[-1]\n",
        "\n",
        "            if i not in [58, 66, 74]:\n",
        "                # darknet weights: [beta, gamma, mean, variance]\n",
        "                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n",
        "                # tf weights: [gamma, beta, mean, variance]\n",
        "                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
        "                bn_layer = model.get_layer(bn_layer_name)\n",
        "                j += 1\n",
        "            else:\n",
        "                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n",
        "\n",
        "            # darknet shape (out_dim, in_dim, height, width)\n",
        "            conv_shape = (filters, in_dim, k_size, k_size)\n",
        "            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
        "            # tf shape (height, width, in_dim, out_dim)\n",
        "            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
        "\n",
        "            if i not in [58, 66, 74]:\n",
        "                conv_layer.set_weights([conv_weights])\n",
        "                bn_layer.set_weights(bn_weights)\n",
        "            else:\n",
        "                conv_layer.set_weights([conv_weights, conv_bias])\n"
      ],
      "metadata": {
        "id": "qJwDDkGfhqAC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_class_names(class_file_name):\n",
        "    # loads class name from a file\n",
        "    #print(class_file_name)\n",
        "    names = {}\n",
        "    with open(class_file_name, 'r') as data:\n",
        "        for ID, name in enumerate(data):\n",
        "            names[ID] = name.strip('\\n')\n",
        "    return names"
      ],
      "metadata": {
        "id": "TQg64h9lhr7G"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_class_names(TRAIN_CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPRzWKkqhuI5",
        "outputId": "ee3717af-b216-40f5-fea2-e7fac148759d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'apple', 1: 'banana', 2: 'orange'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocess(image, target_size, gt_boxes=None):\n",
        "    ih, iw    = target_size\n",
        "    h,  w, _  = image.shape\n",
        "\n",
        "    scale = min(iw/w, ih/h)\n",
        "    nw, nh  = int(scale * w), int(scale * h)\n",
        "    image_resized = cv2.resize(image, (nw, nh))\n",
        "\n",
        "    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n",
        "    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n",
        "    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n",
        "    image_paded = image_paded / 255.\n",
        "    print(gt_boxes)\n",
        "    if gt_boxes is None:\n",
        "        return image_paded\n",
        "\n",
        "    else:\n",
        "        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n",
        "        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n",
        "        return image_paded, gt_boxes"
      ],
      "metadata": {
        "id": "p39y6WnLhxCJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def draw_bbox(image, bboxes, CLASSES=TRAIN_CLASSES, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors=''):   \n",
        "    NUM_CLASS = read_class_names(CLASSES)\n",
        "    num_classes = len(NUM_CLASS)\n",
        "    image_h, image_w, _ = image.shape\n",
        "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
        "    #print(\"hsv_tuples\", hsv_tuples)\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "\n",
        "    random.seed(0)\n",
        "    random.shuffle(colors)\n",
        "    random.seed(None)\n",
        "\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        coor = np.array(bbox[:4], dtype=np.int32)\n",
        "        score = bbox[4]\n",
        "        class_ind = int(bbox[5])\n",
        "        bbox_color = rectangle_colors if rectangle_colors != ''else colors[class_ind]\n",
        "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
        "        if bbox_thick < 1: bbox_thick = 1\n",
        "        #print(image_h, image_w, bbox_thick)\n",
        "        fontScale = 0.75 * bbox_thick\n",
        "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
        "\n",
        "        # put object rectangle\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
        "\n",
        "        if show_label:\n",
        "            # get text label\n",
        "            score_str = f' {score:.2f}' if show_confidence else '' \n",
        "            label = f'{NUM_CLASS[class_ind]}' + score_str\n",
        "\n",
        "            # get text size\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "                                                                  fontScale, thickness=bbox_thick)\n",
        "            # put filled text rectangle\n",
        "            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
        "\n",
        "            # put text above rectangle\n",
        "            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "UWfxUobghyvM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bboxes_iou(boxes1, boxes2):\n",
        "    boxes1 = np.array(boxes1)\n",
        "    boxes2 = np.array(boxes2)\n",
        "\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
        "    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area    = boxes1_area + boxes2_area - inter_area\n",
        "    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(float).eps)\n",
        "\n",
        "    return ious\n"
      ],
      "metadata": {
        "id": "i6d4ONWxh0Kp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def nms(bboxes, iou_threshold, sigma=0.3):\n",
        "\n",
        "    classes_in_img = list(set(bboxes[:, 5]))\n",
        "    best_bboxes = []\n",
        "\n",
        "    for cls in classes_in_img:\n",
        "        cls_mask = (bboxes[:, 5] == cls)\n",
        "        cls_bboxes = bboxes[cls_mask]\n",
        "        # Process 1: Determine whether the number of bounding boxes is greater than 0 \n",
        "        while len(cls_bboxes) > 0:\n",
        "            # Process 2: Select the bounding box with the highest score according to socre order A\n",
        "            max_ind = np.argmax(cls_bboxes[:, 4])\n",
        "            best_bbox = cls_bboxes[max_ind]\n",
        "            best_bboxes.append(best_bbox)\n",
        "            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n",
        "            # Process 3: Calculate this bounding box A and\n",
        "            # Remain all iou of the bounding box and remove those bounding boxes whose iou value is higher than the threshold \n",
        "            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n",
        "            weight = np.ones((len(iou),), dtype=float)\n",
        "\n",
        "            iou_mask = iou > iou_threshold\n",
        "            weight[iou_mask] = 0.0\n",
        "\n",
        "            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n",
        "            score_mask = cls_bboxes[:, 4] > 0.\n",
        "            cls_bboxes = cls_bboxes[score_mask]\n",
        "\n",
        "    return best_bboxes"
      ],
      "metadata": {
        "id": "zPmuGr84h1t7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):\n",
        "    valid_scale=[0, np.inf]\n",
        "    pred_bbox = np.array(pred_bbox)\n",
        "\n",
        "    pred_xywh = pred_bbox[:, 0:4]\n",
        "    pred_conf = pred_bbox[:, 4]\n",
        "    pred_prob = pred_bbox[:, 5:]\n",
        "\n",
        "    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n",
        "    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n",
        "                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n",
        "    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n",
        "    org_h, org_w = original_image.shape[:2]\n",
        "    resize_ratio = min(input_size / org_w, input_size / org_h)\n",
        "\n",
        "    dw = (input_size - resize_ratio * org_w) / 2\n",
        "    dh = (input_size - resize_ratio * org_h) / 2\n",
        "\n",
        "    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n",
        "    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n",
        "\n",
        "    # 3. clip some boxes those are out of range\n",
        "    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n",
        "                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n",
        "    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n",
        "    pred_coor[invalid_mask] = 0\n",
        "\n",
        "    # 4. discard some invalid boxes\n",
        "    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n",
        "    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n",
        "\n",
        "    # 5. discard boxes with low scores\n",
        "    classes = np.argmax(pred_prob, axis=-1)\n",
        "    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n",
        "    score_mask = scores > score_threshold\n",
        "    mask = np.logical_and(scale_mask, score_mask)\n",
        "    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n",
        "    #print(coors)\n",
        "\n",
        "    #print(coors[0])\n",
        "    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)"
      ],
      "metadata": {
        "id": "Fac8ifNMh3GX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_image(YoloV3, image_path, output_path, input_size=416, show=False, CLASSES=TRAIN_CLASSES, score_threshold=0.3, iou_threshold=0.45, rectangle_colors=''):\n",
        "    original_image      = cv2.imread(image_path)\n",
        "    original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    #original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n",
        "    image_data = tf.expand_dims(image_data, 0)\n",
        "\n",
        "    pred_bbox = YoloV3.predict(image_data)\n",
        "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
        "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
        "    \n",
        "    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n",
        "    #xx=np.array(bboxes)\n",
        "    #print(xx.shape)\n",
        "\n",
        "    bboxes = nms(bboxes, iou_threshold)\n",
        "    image = draw_bbox(original_image, bboxes, CLASSES=CLASSES, rectangle_colors=rectangle_colors)\n",
        "    #cv2.imwrite('./output.jpg', image)\n",
        "        \n",
        "    return image"
      ],
      "metadata": {
        "id": "qgb8kIH5h4nM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, ZeroPadding2D, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "\n",
        "STRIDES         = np.array(YOLO_STRIDES)\n",
        "ANCHORS         = (np.array(YOLO_ANCHORS).T/STRIDES).T\n",
        "IOU_LOSS_THRESH = YOLO_IOU_LOSS_THRESH"
      ],
      "metadata": {
        "id": "q86mWalKh6Yx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization(BatchNormalization):\n",
        "    # \"Frozen state\" and \"inference mode\" are two separate concepts.\n",
        "    # `layer.trainable = False` is to freeze the layer, so the layer will use\n",
        "    # stored moving `var` and `mean` in the \"inference mode\", and both `gama`\n",
        "    # and `beta` will not be updated !\n",
        "    def call(self, x, training=False):\n",
        "        if not training:\n",
        "            training = tf.constant(False)\n",
        "        training = tf.logical_and(training, self.trainable)\n",
        "        return super().call(x, training)\n",
        "\n",
        "def convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):\n",
        "    if downsample:\n",
        "        input_layer = ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n",
        "        padding = 'valid'\n",
        "        strides = 2\n",
        "    else:\n",
        "        strides = 1\n",
        "        padding = 'same'\n",
        "\n",
        "    conv = Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides,\n",
        "                  padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),\n",
        "                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
        "                  bias_initializer=tf.constant_initializer(0.))(input_layer)\n",
        "    if bn:\n",
        "        conv = BatchNormalization()(conv)\n",
        "    if activate == True:\n",
        "        conv = LeakyReLU(alpha=0.1)(conv)\n",
        "\n",
        "    return conv\n",
        "\n",
        "def residual_block(input_layer, input_channel, filter_num1, filter_num2):\n",
        "    short_cut = input_layer\n",
        "    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))\n",
        "    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))\n",
        "\n",
        "    residual_output = short_cut + conv\n",
        "    return residual_output\n",
        "\n",
        "def upsample(input_layer):\n",
        "    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')\n",
        "\n",
        "\n",
        "def darknet53(input_data):\n",
        "    input_data = convolutional(input_data, (3, 3,  3,  32))\n",
        "    input_data = convolutional(input_data, (3, 3, 32,  64), downsample=True)\n",
        "\n",
        "    for i in range(1):\n",
        "        input_data = residual_block(input_data,  64,  32, 64)\n",
        "\n",
        "    input_data = convolutional(input_data, (3, 3,  64, 128), downsample=True)\n",
        "\n",
        "    for i in range(2):\n",
        "        input_data = residual_block(input_data, 128,  64, 128)\n",
        "\n",
        "    input_data = convolutional(input_data, (3, 3, 128, 256), downsample=True)\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 256, 128, 256)\n",
        "\n",
        "    route_1 = input_data\n",
        "    input_data = convolutional(input_data, (3, 3, 256, 512), downsample=True)\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 512, 256, 512)\n",
        "\n",
        "    route_2 = input_data\n",
        "    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n",
        "\n",
        "    for i in range(4):\n",
        "        input_data = residual_block(input_data, 1024, 512, 1024)\n",
        "\n",
        "    return route_1, route_2, input_data\n",
        "\n",
        "def YOLOv3(input_layer, NUM_CLASS):\n",
        "    # After the input layer enters the Darknet-53 network, we get three branches\n",
        "    route_1, route_2, conv = darknet53(input_layer)\n",
        "    # See the orange module (DBL) in the figure above, a total of 5 Subconvolution operation\n",
        "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
        "    conv = convolutional(conv, (3, 3,  512, 1024))\n",
        "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
        "    conv = convolutional(conv, (3, 3,  512, 1024))\n",
        "    conv = convolutional(conv, (1, 1, 1024,  512))\n",
        "    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))\n",
        "    \n",
        "    # conv_lbbox is used to predict large-sized objects , Shape = [None, 13, 13, 255] \n",
        "    conv_lbbox = convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, (1, 1,  512,  256))\n",
        "    # upsample here uses the nearest neighbor interpolation method, which has the advantage that the\n",
        "    # upsampling process does not need to learn, thereby reducing the network parameter  \n",
        "    conv = upsample(conv)\n",
        "\n",
        "    conv = tf.concat([conv, route_2], axis=-1)\n",
        "    conv = convolutional(conv, (1, 1, 768, 256))\n",
        "    conv = convolutional(conv, (3, 3, 256, 512))\n",
        "    conv = convolutional(conv, (1, 1, 512, 256))\n",
        "    conv = convolutional(conv, (3, 3, 256, 512))\n",
        "    conv = convolutional(conv, (1, 1, 512, 256))\n",
        "    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))\n",
        "\n",
        "    # conv_mbbox is used to predict medium-sized objects, shape = [None, 26, 26, 255]\n",
        "    conv_mbbox = convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, (1, 1, 256, 128))\n",
        "    conv = upsample(conv)\n",
        "\n",
        "    conv = tf.concat([conv, route_1], axis=-1)\n",
        "    conv = convolutional(conv, (1, 1, 384, 128))\n",
        "    conv = convolutional(conv, (3, 3, 128, 256))\n",
        "    conv = convolutional(conv, (1, 1, 256, 128))\n",
        "    conv = convolutional(conv, (3, 3, 128, 256))\n",
        "    conv = convolutional(conv, (1, 1, 256, 128))\n",
        "    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))\n",
        "    \n",
        "    # conv_sbbox is used to predict small size objects, shape = [None, 52, 52, 255]\n",
        "    conv_sbbox = convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\n",
        "        \n",
        "    return [conv_sbbox, conv_mbbox, conv_lbbox]\n",
        "\n",
        "def Create_Yolov3(input_size=416, channels=3, training=False, CLASSES=TRAIN_CLASSES):\n",
        "    NUM_CLASS = len(read_class_names(CLASSES))\n",
        "    input_layer  = Input([input_size, input_size, channels])\n",
        "\n",
        "    conv_tensors = YOLOv3(input_layer, NUM_CLASS)\n",
        "\n",
        "    output_tensors = []\n",
        "    for i, conv_tensor in enumerate(conv_tensors):\n",
        "        pred_tensor = decode(conv_tensor, NUM_CLASS, i)\n",
        "        if training: output_tensors.append(conv_tensor)\n",
        "        output_tensors.append(pred_tensor)\n",
        "\n",
        "    YoloV3 = tf.keras.Model(input_layer, output_tensors)\n",
        "    return YoloV3\n",
        "\n",
        "def decode(conv_output, NUM_CLASS, i=0):\n",
        "    # where i = 0, 1 or 2 to correspond to the three grid scales  \n",
        "    conv_shape       = tf.shape(conv_output)\n",
        "    batch_size       = conv_shape[0]\n",
        "    output_size      = conv_shape[1]\n",
        "\n",
        "    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n",
        "\n",
        "    conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # offset of center position     \n",
        "    conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # Prediction box length and width offset\n",
        "    conv_raw_conf = conv_output[:, :, :, :, 4:5] # confidence of the prediction box\n",
        "    conv_raw_prob = conv_output[:, :, :, :, 5: ] # category probability of the prediction box \n",
        "\n",
        "    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n",
        "    y = tf.range(output_size, dtype=tf.int32)\n",
        "    y = tf.expand_dims(y, -1)\n",
        "    y = tf.tile(y, [1, output_size])\n",
        "    x = tf.range(output_size,dtype=tf.int32)\n",
        "    x = tf.expand_dims(x, 0)\n",
        "    x = tf.tile(x, [output_size, 1])\n",
        "\n",
        "    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n",
        "    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n",
        "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
        "\n",
        "    # Calculate the center position of the prediction box:\n",
        "    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n",
        "    # Calculate the length and width of the prediction box:\n",
        "    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n",
        "\n",
        "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
        "    pred_conf = tf.sigmoid(conv_raw_conf) # object box calculates the predicted confidence\n",
        "    pred_prob = tf.sigmoid(conv_raw_prob) # calculating the predicted probability category box object\n",
        "\n",
        "    # calculating the predicted probability category box object\n",
        "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n",
        "\n",
        "def bbox_iou(boxes1, boxes2):\n",
        "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
        "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
        "\n",
        "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "\n",
        "    return 1.0 * inter_area / union_area\n",
        "\n",
        "def bbox_giou(boxes1, boxes2):\n",
        "    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
        "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
        "    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
        "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
        "\n",
        "    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\n",
        "                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\n",
        "    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\n",
        "                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\n",
        "\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "\n",
        "    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = tf.maximum(right_down - left_up, 0.0)\n",
        "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area = boxes1_area + boxes2_area - inter_area\n",
        "    iou = inter_area / union_area\n",
        "\n",
        "    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n",
        "    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n",
        "    enclose_area = enclose[..., 0] * enclose[..., 1]\n",
        "    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n",
        "\n",
        "    return giou\n",
        "\n",
        "\n",
        "def compute_loss(pred, conv, label, bboxes, i=0, CLASSES=TRAIN_CLASSES):\n",
        "    NUM_CLASS = len(read_class_names(CLASSES))\n",
        "    conv_shape  = tf.shape(conv)\n",
        "    batch_size  = conv_shape[0]\n",
        "    output_size = conv_shape[1]\n",
        "    input_size  = STRIDES[i] * output_size\n",
        "    conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n",
        "\n",
        "    conv_raw_conf = conv[:, :, :, :, 4:5]\n",
        "    conv_raw_prob = conv[:, :, :, :, 5:]\n",
        "\n",
        "    pred_xywh     = pred[:, :, :, :, 0:4]\n",
        "    pred_conf     = pred[:, :, :, :, 4:5]\n",
        "\n",
        "    label_xywh    = label[:, :, :, :, 0:4]\n",
        "    respond_bbox  = label[:, :, :, :, 4:5]\n",
        "    label_prob    = label[:, :, :, :, 5:]\n",
        "\n",
        "    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\n",
        "    input_size = tf.cast(input_size, tf.float32)\n",
        "\n",
        "    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n",
        "    giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n",
        "\n",
        "    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n",
        "    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n",
        "\n",
        "    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32)\n",
        "\n",
        "    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n",
        "\n",
        "    conf_loss = conf_focal * (\n",
        "            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n",
        "            +\n",
        "            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n",
        "    )\n",
        "\n",
        "    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n",
        "\n",
        "    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n",
        "    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n",
        "    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n",
        "\n",
        "    return giou_loss, conf_loss, prob_loss"
      ],
      "metadata": {
        "id": "dCLMeqbih8Nw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "    # Dataset preprocess implementation\n",
        "    def __init__(self, dataset_type):\n",
        "        self.annot_path  = TRAIN_ANNOT_PATH if dataset_type == 'train' else TEST_ANNOT_PATH\n",
        "        self.input_sizes = TRAIN_INPUT_SIZE if dataset_type == 'train' else TEST_INPUT_SIZE\n",
        "        self.batch_size  = TRAIN_BATCH_SIZE if dataset_type == 'train' else TEST_BATCH_SIZE\n",
        "        self.data_aug    = TRAIN_DATA_AUG   if dataset_type == 'train' else TEST_DATA_AUG\n",
        "\n",
        "        self.train_input_sizes = TRAIN_INPUT_SIZE\n",
        "        self.strides = np.array(YOLO_STRIDES)\n",
        "        self.classes = read_class_names(TRAIN_CLASSES)\n",
        "        self.num_classes = len(self.classes)\n",
        "        self.anchors = (np.array(YOLO_ANCHORS).T/self.strides).T\n",
        "        self.anchor_per_scale = YOLO_ANCHOR_PER_SCALE\n",
        "        self.max_bbox_per_scale = YOLO_MAX_BBOX_PER_SCALE\n",
        "\n",
        "        self.annotations = self.load_annotations(dataset_type)\n",
        "        self.num_samples = len(self.annotations)\n",
        "        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n",
        "        self.batch_count = 0\n",
        "\n",
        "\n",
        "    def load_annotations(self, dataset_type):\n",
        "        final_annotations = []\n",
        "        with open(self.annot_path, 'r') as f:\n",
        "            txt = f.readlines()\n",
        "            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n",
        "        np.random.shuffle(annotations)\n",
        "        \n",
        "        for annotation in annotations:\n",
        "            # fully parse annotations\n",
        "            line = annotation.split()\n",
        "            image_path, index = \"\", 1\n",
        "            for i, one_line in enumerate(line):\n",
        "                if not one_line.replace(\",\",\"\").isnumeric():\n",
        "                    if image_path != \"\": image_path += \" \"\n",
        "                    image_path += one_line\n",
        "                else:\n",
        "                    index = i\n",
        "                    break\n",
        "            if not os.path.exists(image_path):\n",
        "                raise KeyError(\"%s does not exist ... \" %image_path)\n",
        "\n",
        "            final_annotations.append([image_path, line[index:]])\n",
        "\n",
        "        return final_annotations\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        with tf.device('/cpu:0'):\n",
        "            self.train_input_size = random.choice([self.train_input_sizes])\n",
        "            self.train_output_sizes = self.train_input_size // self.strides\n",
        "\n",
        "            batch_image = np.zeros((self.batch_size, self.train_input_size, self.train_input_size, 3), dtype=np.float32)\n",
        "\n",
        "            batch_label_sbbox = np.zeros((self.batch_size, self.train_output_sizes[0], self.train_output_sizes[0],\n",
        "                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n",
        "            batch_label_mbbox = np.zeros((self.batch_size, self.train_output_sizes[1], self.train_output_sizes[1],\n",
        "                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n",
        "            batch_label_lbbox = np.zeros((self.batch_size, self.train_output_sizes[2], self.train_output_sizes[2],\n",
        "                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n",
        "\n",
        "            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n",
        "            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n",
        "            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n",
        "\n",
        "            num = 0\n",
        "            if self.batch_count < self.num_batchs:\n",
        "                while num < self.batch_size:\n",
        "                    index = self.batch_count * self.batch_size + num\n",
        "                    if index >= self.num_samples: index -= self.num_samples\n",
        "                    annotation = self.annotations[index]\n",
        "                    image, bboxes = self.parse_annotation(annotation)\n",
        "                    label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)\n",
        "\n",
        "                    batch_image[num, :, :, :] = image\n",
        "                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n",
        "                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n",
        "                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n",
        "                    batch_sbboxes[num, :, :] = sbboxes\n",
        "                    batch_mbboxes[num, :, :] = mbboxes\n",
        "                    batch_lbboxes[num, :, :] = lbboxes\n",
        "                    num += 1\n",
        "                self.batch_count += 1\n",
        "                batch_smaller_target = batch_label_sbbox, batch_sbboxes\n",
        "                batch_medium_target  = batch_label_mbbox, batch_mbboxes\n",
        "                batch_larger_target  = batch_label_lbbox, batch_lbboxes\n",
        "\n",
        "                return batch_image, (batch_smaller_target, batch_medium_target, batch_larger_target)\n",
        "            else:\n",
        "                self.batch_count = 0\n",
        "                np.random.shuffle(self.annotations)\n",
        "                raise StopIteration\n",
        "\n",
        "    def random_horizontal_flip(self, image, bboxes):\n",
        "        if random.random() < 0.5:\n",
        "            _, w, _ = image.shape\n",
        "            image = image[:, ::-1, :]\n",
        "            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def random_crop(self, image, bboxes):\n",
        "        if random.random() < 0.5:\n",
        "            h, w, _ = image.shape\n",
        "            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
        "\n",
        "            max_l_trans = max_bbox[0]\n",
        "            max_u_trans = max_bbox[1]\n",
        "            max_r_trans = w - max_bbox[2]\n",
        "            max_d_trans = h - max_bbox[3]\n",
        "\n",
        "            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n",
        "            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n",
        "            crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n",
        "            crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n",
        "\n",
        "            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n",
        "\n",
        "            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n",
        "            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def random_translate(self, image, bboxes):\n",
        "        if random.random() < 0.5:\n",
        "            h, w, _ = image.shape\n",
        "            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n",
        "\n",
        "            max_l_trans = max_bbox[0]\n",
        "            max_u_trans = max_bbox[1]\n",
        "            max_r_trans = w - max_bbox[2]\n",
        "            max_d_trans = h - max_bbox[3]\n",
        "\n",
        "            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n",
        "            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n",
        "\n",
        "            M = np.array([[1, 0, tx], [0, 1, ty]])\n",
        "            image = cv2.warpAffine(image, M, (w, h))\n",
        "\n",
        "            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n",
        "            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "    def parse_annotation(self, annotation):\n",
        "\n",
        "        image_path = annotation[0]\n",
        "        image = cv2.imread(image_path)\n",
        "            \n",
        "        bboxes = np.array([list(map(int, box.split(','))) for box in annotation[1]])\n",
        "\n",
        "        if self.data_aug:\n",
        "            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\n",
        "            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\n",
        "            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image, bboxes = image_preprocess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\n",
        "        return image, bboxes\n",
        "\n",
        "    def preprocess_true_boxes(self, bboxes):\n",
        "        label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale,\n",
        "                           5 + self.num_classes)) for i in range(3)]\n",
        "        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n",
        "        bbox_count = np.zeros((3,))\n",
        "\n",
        "        for bbox in bboxes:\n",
        "            bbox_coor = bbox[:4]\n",
        "            bbox_class_ind = bbox[4]\n",
        "\n",
        "            onehot = np.zeros(self.num_classes, dtype=np.float64)\n",
        "            onehot[bbox_class_ind] = 1.0\n",
        "            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\n",
        "            deta = 0.01\n",
        "            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n",
        "\n",
        "            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n",
        "            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n",
        "\n",
        "            iou = []\n",
        "            exist_positive = False\n",
        "            for i in range(3):\n",
        "                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n",
        "                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
        "                anchors_xywh[:, 2:4] = self.anchors[i]\n",
        "\n",
        "                iou_scale = bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n",
        "                iou.append(iou_scale)\n",
        "                iou_mask = iou_scale > 0.3\n",
        "\n",
        "                if np.any(iou_mask):\n",
        "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n",
        "\n",
        "                    label[i][yind, xind, iou_mask, :] = 0\n",
        "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n",
        "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
        "                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n",
        "\n",
        "                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n",
        "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n",
        "                    bbox_count[i] += 1\n",
        "\n",
        "                    exist_positive = True\n",
        "\n",
        "            if not exist_positive:\n",
        "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
        "                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n",
        "                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n",
        "                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n",
        "\n",
        "                label[best_detect][yind, xind, best_anchor, :] = 0\n",
        "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n",
        "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
        "                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n",
        "                \n",
        "                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n",
        "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n",
        "                bbox_count[best_detect] += 1\n",
        "\n",
        "        label_sbbox, label_mbbox, label_lbbox = label\n",
        "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
        "        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batchs\n"
      ],
      "metadata": {
        "id": "ysmxgm4Kh-Ot"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = Dataset('train')\n",
        "steps_per_epoch = len(trainset)\n",
        "\n",
        "steps_per_epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNGg3a3xiAPM",
        "outputId": "5c86b3fb-1ed5-4b1e-9b2e-63fe2c7cc73f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "global TRAIN_FROM_CHECKPOINT\n",
        "input_size = YOLO_INPUT_SIZE\n",
        "Darknet_weights = YOLO_DARKNET_WEIGHTS\n",
        "\n",
        "\n",
        "save_best_only = True # saves only best model according validation loss\n",
        "save_checkpoints = False # saves all best validated checkpoints in training process (may require a lot disk space)\n",
        "\n",
        "\n",
        "trainset = Dataset('train')\n",
        "testset = Dataset('test')\n",
        "\n",
        "steps_per_epoch = len(trainset)\n",
        "global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n",
        "warmup_steps = TRAIN_WARMUP_EPOCHS * steps_per_epoch\n",
        "total_steps = TRAIN_EPOCHS * steps_per_epoch\n",
        "\n",
        "if TRAIN_TRANSFER:\n",
        "    Darknet = Create_Yolov3(input_size=input_size)\n",
        "    load_yolo_weights(Darknet, Darknet_weights) # use darknet weights\n",
        "\n",
        "yolo = Create_Yolov3(input_size=input_size, training=True, CLASSES=TRAIN_CLASSES)\n",
        "\n",
        "#TRAIN_FROM_CHECKPOINT = False\n",
        "if TRAIN_FROM_CHECKPOINT:\n",
        "    yolo.load_weights(TRAIN_FROM_CHECKPOINT)\n",
        "\n",
        "## transfer && Not use checkpoint\n",
        "if TRAIN_TRANSFER and not TRAIN_FROM_CHECKPOINT:\n",
        "    for i, l in enumerate(Darknet.layers):\n",
        "        layer_weights = l.get_weights()\n",
        "        if layer_weights != []:\n",
        "            try:\n",
        "                yolo.layers[i].set_weights(layer_weights)\n",
        "            except:\n",
        "                print(\"skipping\", yolo.layers[i].name)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def train_step(image_data, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred_result = yolo(image_data, training=True)\n",
        "        giou_loss=conf_loss=prob_loss=0\n",
        "        # optimizing process\n",
        "        for i in range(3):\n",
        "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
        "            loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=TRAIN_CLASSES)\n",
        "            giou_loss += loss_items[0]\n",
        "            conf_loss += loss_items[1]\n",
        "            prob_loss += loss_items[2]\n",
        "        total_loss = giou_loss + conf_loss + prob_loss\n",
        "        gradients = tape.gradient(total_loss, yolo.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, yolo.trainable_variables))\n",
        "\n",
        "\n",
        "        # update learning rate\n",
        "        global_steps.assign_add(1)\n",
        "        if global_steps < warmup_steps:# and not TRAIN_TRANSFER:\n",
        "            lr = global_steps / warmup_steps * TRAIN_LR_INIT\n",
        "        else:\n",
        "            lr = TRAIN_LR_END + 0.5 * (TRAIN_LR_INIT - TRAIN_LR_END)*(\n",
        "                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi)))\n",
        "        optimizer.lr.assign(lr.numpy())\n",
        "\n",
        "    return global_steps.numpy(), optimizer.lr.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()\n",
        "\n",
        "\n",
        "\n",
        "def validate_step(image_data, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred_result = yolo(image_data, training=False)\n",
        "        giou_loss=conf_loss=prob_loss=0\n",
        "\n",
        "        # optimizing process\n",
        "        for i in range(3):\n",
        "            conv, pred = pred_result[i*2], pred_result[i*2+1]\n",
        "            loss_items = compute_loss(pred, conv, *target[i], i, CLASSES=TRAIN_CLASSES)\n",
        "            giou_loss += loss_items[0]\n",
        "            conf_loss += loss_items[1]\n",
        "            prob_loss += loss_items[2]\n",
        "\n",
        "        total_loss = giou_loss + conf_loss + prob_loss\n",
        "        \n",
        "    return giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), total_loss.numpy()\n",
        "\n",
        "\n",
        "best_val_loss = 1000 # should be large at start\n",
        "\n",
        "\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    for image_data, target in trainset:\n",
        "        results = train_step(image_data, target)\n",
        "        cur_step = results[0]%steps_per_epoch\n",
        "        print(\"epoch:{:2.0f} step:{:5.0f}/{}, lr:{:.6f}, giou_loss:{:7.2f}, conf_loss:{:7.2f}, prob_loss:{:7.2f}, total_loss:{:7.2f}\"\n",
        "                  .format(epoch, cur_step, steps_per_epoch, results[1], results[2], results[3], results[4], results[5]))\n",
        "        \n",
        "    count, giou_val, conf_val, prob_val, total_val = 0., 0, 0, 0, 0\n",
        "    for image_data, target in testset:\n",
        "        results = validate_step(image_data, target)\n",
        "        count += 1\n",
        "        giou_val += results[0]\n",
        "        conf_val += results[1]\n",
        "        prob_val += results[2]\n",
        "        total_val += results[3]\n",
        "\n",
        "        \n",
        "    print(\"\\n\\ngiou_val_loss:{:7.2f}, conf_val_loss:{:7.2f}, prob_val_loss:{:7.2f}, total_val_loss:{:7.2f}\\n\\n\".\n",
        "          format(giou_val/count, conf_val/count, prob_val/count, total_val/count))\n",
        "\n",
        "    if save_best_only and best_val_loss > total_val/count: \n",
        "        yolo.save_weights(\"./chkpt_fruits/yolov3_custom\")\n",
        "        best_val_loss = total_val/count\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tPr1bjVZiCNA",
        "outputId": "3f38468f-5d58-483d-c88d-8935a663613d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[279  14 562 295   2]]\n",
            "\n",
            "\n",
            "giou_val_loss:   4.17, conf_val_loss: 576.25, prob_val_loss:   9.58, total_val_loss: 589.99\n",
            "\n",
            "\n",
            "[[ 560   80 1105  597    2]]\n",
            "[[ 99 180 566 455   1]\n",
            " [109 119 542 349   1]]\n",
            "[[  2   8 364 335   2]]\n",
            "[[ 16 206 131 305   2]\n",
            " [426 184 506 259   2]\n",
            " [400 127 498 204   2]\n",
            " [ 30  94 287 194   1]\n",
            " [ 68 136 302 231   1]\n",
            " [204  93 361 238   1]\n",
            " [232 143 368 272   1]\n",
            " [209  18 318 121   0]\n",
            " [319  51 433 156   0]]\n",
            "epoch: 4 step:    2/60, lr:0.000097, giou_loss:   7.81, conf_loss: 137.43, prob_loss:  14.01, total_loss: 159.24\n",
            "[[ 49  12 297 268   2]]\n",
            "[[ 58 221 249 410   2]\n",
            " [171 213 331 391   2]]\n",
            "[[ 37  47 472 489   0]]\n",
            "[[ 37  42 235 233   2]\n",
            " [201  19 370 192   2]]\n",
            "epoch: 4 step:    3/60, lr:0.000097, giou_loss:   2.13, conf_loss: 129.88, prob_loss:   5.86, total_loss: 137.87\n",
            "[[130 262 294 412   2]\n",
            " [156  44 347 240   0]\n",
            " [337  39 463 426   1]\n",
            " [379 157 643 485   1]\n",
            " [390  38 544 415   1]]\n",
            "[[ 15  16 169 161   2]]\n",
            "[[307  28 888 433   1]\n",
            " [300 159 875 642   1]]\n",
            "[[ 74 270 263 464   0]\n",
            " [146 183 311 368   0]\n",
            " [  6 194 175 376   0]]\n",
            "epoch: 4 step:    4/60, lr:0.000097, giou_loss:   5.86, conf_loss: 132.90, prob_loss:   7.76, total_loss: 146.51\n",
            "[[ 47 150 506 400   1]]\n",
            "[[ 87 104 506 502   2]\n",
            " [507  30 762 271   2]]\n",
            "[[ 49  14 228 184   2]]\n",
            "[[ 97  62 366 326   0]]\n",
            "epoch: 4 step:    5/60, lr:0.000097, giou_loss:   1.58, conf_loss: 127.17, prob_loss:   5.07, total_loss: 133.81\n",
            "[[156  28 378 247   2]\n",
            " [ 21  13 221 235   2]]\n",
            "[[ 76  70 470 451   2]\n",
            " [552  56 953 442   0]\n",
            " [168 496 849 858   1]]\n",
            "[[175  25 443 302   0]]\n",
            "[[123 104 209 192   2]\n",
            " [ 99  91 177 182   2]]\n",
            "epoch: 4 step:    6/60, lr:0.000097, giou_loss:   2.71, conf_loss: 127.78, prob_loss:   7.58, total_loss: 138.07\n",
            "[[  54  319 1519 1893    2]]\n",
            "[[167  72 264 177   2]]\n",
            "[[ 266  361  900 1361    1]\n",
            " [ 682  182 1207 1240    1]]\n",
            "[[ 61 261 366 567   0]\n",
            " [388 426 676 691   0]\n",
            " [304 290 587 545   0]]\n",
            "epoch: 4 step:    7/60, lr:0.000097, giou_loss:   2.32, conf_loss: 127.17, prob_loss:   5.78, total_loss: 135.27\n",
            "[[ 344    8 1302  748    2]\n",
            " [ 176  650  974 1073    2]]\n",
            "[[ 16  87 256 291   0]]\n",
            "[[ 36  39 538 242   1]]\n",
            "[[  75  135 2807 2618    1]]\n",
            "epoch: 4 step:    8/60, lr:0.000097, giou_loss:   1.33, conf_loss: 126.33, prob_loss:   2.29, total_loss: 129.95\n",
            "[[  2  15 300 303   0]]\n",
            "[[ 61 118 230 273   2]\n",
            " [180 110 340 273   2]\n",
            " [242  63 384 204   2]]\n",
            "[[129  29 271 174   0]\n",
            " [219 152 341 273   0]\n",
            " [182 268 334 392   0]\n",
            " [  1 138 167 295   0]\n",
            " [309  58 408 148   0]\n",
            " [377 205 451 280   0]\n",
            " [402  34 463  95   0]\n",
            " [395 109 460 176   0]]\n",
            "[[ 67  17 216 232   1]]\n",
            "epoch: 4 step:    9/60, lr:0.000097, giou_loss:   7.12, conf_loss: 132.45, prob_loss:  16.79, total_loss: 156.36\n",
            "[[  4   4 268 276   2]]\n",
            "[[301  75 486 167   1]\n",
            " [114  74 298 168   1]]\n",
            "[[ 13   3 160 147   0]]\n",
            "[[164  17 597 342   1]\n",
            " [312  32 686 394   1]\n",
            " [106   3 545 248   1]]\n",
            "epoch: 4 step:   10/60, lr:0.000097, giou_loss:   2.87, conf_loss: 123.79, prob_loss:   9.53, total_loss: 136.19\n",
            "[[368 109 706 470   2]\n",
            " [ 58  69 410 432   2]]\n",
            "[[  1  22 319 326   1]]\n",
            "[[  1  60 532 565   2]]\n",
            "[[  47  744  850 1565    0]\n",
            " [ 553   15  959  631    0]\n",
            " [ 336   18  856  571    0]]\n",
            "epoch: 4 step:   11/60, lr:0.000096, giou_loss:   1.77, conf_loss: 123.61, prob_loss:   3.97, total_loss: 129.34\n",
            "[[163 147 330 324   0]\n",
            " [ 19 139 196 296   0]\n",
            " [ 77  48 256 217   0]]\n",
            "[[ 31  82 157 222   0]]\n",
            "[[460 423 743 693   0]\n",
            " [368 126 616 379   0]\n",
            " [ 52 184 366 451   0]\n",
            " [243   2 495 225   0]\n",
            " [518  20 783 254   0]]\n",
            "[[ 21  75 419 490   0]]\n",
            "epoch: 4 step:   12/60, lr:0.000096, giou_loss:   5.32, conf_loss: 125.79, prob_loss:  11.29, total_loss: 142.41\n",
            "[[373 104 514 345   1]\n",
            " [298  93 382 354   1]\n",
            " [219 110 292 349   1]\n",
            " [121 117 216 354   1]]\n",
            "[[438  36 858 412   2]\n",
            " [ 38 289 985 591   1]\n",
            " [113  77 482 389   0]]\n",
            "[[ 17  18 448 459   2]]\n",
            "[[  74   63 1431 1383    0]]\n",
            "epoch: 4 step:   13/60, lr:0.000096, giou_loss:   5.38, conf_loss: 124.64, prob_loss:   9.35, total_loss: 139.37\n",
            "[[ 81 119 361 420   2]]\n",
            "[[ 46  17 382 387   2]\n",
            " [400   6 729 328   2]]\n",
            "[[ 13 124 415 213   1]\n",
            " [ 20 132 416 284   1]\n",
            " [ 29 168 426 377   1]\n",
            " [152 188 503 459   1]]\n",
            "[[ 96 220 743 584   1]]\n",
            "epoch: 4 step:   14/60, lr:0.000096, giou_loss:   3.30, conf_loss: 120.77, prob_loss:   6.54, total_loss: 130.60\n",
            "[[  8  67 185 234   0]]\n",
            "[[ 19  16 462 411   1]]\n",
            "[[  1  23 406 365   1]]\n",
            "[[ 54 167 278 409   2]]\n",
            "epoch: 4 step:   15/60, lr:0.000096, giou_loss:   0.97, conf_loss: 119.80, prob_loss:   1.45, total_loss: 122.23\n",
            "[[ 88  86 414 361   0]]\n",
            "[[291  55 588 342   0]]\n",
            "[[ 13 103 134 246   2]\n",
            " [ 34   0 140 114   2]\n",
            " [106  61 236 186   2]\n",
            " [126   1 236  69   2]\n",
            " [ 85 200 207 249   2]]\n",
            "[[406 259 670 572   2]\n",
            " [359 107 679 362   0]\n",
            " [ 55  43 604 345   1]]\n",
            "epoch: 4 step:   16/60, lr:0.000096, giou_loss:   4.93, conf_loss: 121.43, prob_loss:   7.88, total_loss: 134.23\n",
            "[[ 33  55 291 303   2]\n",
            " [334  17 633 316   2]]\n",
            "[[   1  135 1181  601    1]]\n",
            "[[547  13 843 285   1]\n",
            " [ 86  43 354 386   1]\n",
            " [169  61 458 405   1]\n",
            " [198 141 458 535   1]\n",
            " [412 128 582 554   1]\n",
            " [478 127 769 462   1]\n",
            " [432  42 805 323   1]]\n",
            "[[158  54 488 386   2]]\n",
            "epoch: 4 step:   17/60, lr:0.000096, giou_loss:   5.69, conf_loss: 122.19, prob_loss:  15.70, total_loss: 143.59\n",
            "[[224  44 912 564   1]\n",
            " [ 57  44 486 564   1]]\n",
            "[[  24    6  596 1006    1]]\n",
            "[[348  26 804 395   2]]\n",
            "[[198 380 363 575   2]\n",
            " [356 375 549 598   0]\n",
            " [ 58 593 603 846   1]]\n",
            "epoch: 4 step:   18/60, lr:0.000096, giou_loss:   2.80, conf_loss: 118.03, prob_loss:   8.20, total_loss: 129.04\n",
            "[[  5 174 562 408   1]]\n",
            "[[ 599  190 1022  597    2]\n",
            " [1066  287 1450  714    2]\n",
            " [1010  673 1490 1105    2]]\n",
            "[[514 561 882 950   2]]\n",
            "[[141  35 315 210   0]\n",
            " [ 18  22 168 190   0]]\n",
            "epoch: 4 step:   19/60, lr:0.000096, giou_loss:   2.78, conf_loss: 118.10, prob_loss:   6.27, total_loss: 127.15\n",
            "[[   0  456  955 1092    1]\n",
            " [   0  660  781 1168    1]\n",
            " [ 319  171 1499 1044    1]]\n",
            "[[473 312 783 628   2]\n",
            " [ 16 175 622 392   1]\n",
            " [ 22 233 672 488   1]\n",
            " [ 70  10 332 303   0]]\n",
            "[[ 45  41 533 348   1]]\n",
            "[[ 352   21 1231  907    2]]\n",
            "epoch: 4 step:   20/60, lr:0.000096, giou_loss:   2.69, conf_loss: 116.43, prob_loss:   5.04, total_loss: 124.17\n",
            "[[228 117 439 313   2]\n",
            " [277 356 486 574   2]\n",
            " [ 97 436 309 635   2]\n",
            " [172 521 374 691   2]]\n",
            "[[101  86 429 388   1]\n",
            " [147 163 545 420   1]\n",
            " [161  80 473 275   1]]\n",
            "[[ 11  17 417 436   0]]\n",
            "[[602 238 881 519   2]\n",
            " [411  81 721 339   2]\n",
            " [364 328 697 565   2]]\n",
            "epoch: 4 step:   21/60, lr:0.000096, giou_loss:   4.73, conf_loss: 119.05, prob_loss:  13.33, total_loss: 137.11\n",
            "[[ 12  45 153 202   0]\n",
            " [152  40 254 149   0]\n",
            " [103 108 214 217   0]]\n",
            "[[110  19 535 428   2]]\n",
            "[[ 88   9 750 712   2]]\n",
            "[[ 36  77 586 629   0]\n",
            " [428  83 869 551   0]\n",
            " [665  93 942 492   0]]\n",
            "epoch: 4 step:   22/60, lr:0.000096, giou_loss:   3.44, conf_loss: 114.54, prob_loss:   9.92, total_loss: 127.90\n",
            "[[  5  31 287 276   0]]\n",
            "[[224  61 549 390   0]\n",
            " [  8  28 279 319   0]]\n",
            "[[ 265    1 1070  214    1]]\n",
            "[[ 102   99 1969 1919    2]]\n",
            "epoch: 4 step:   23/60, lr:0.000096, giou_loss:   1.02, conf_loss: 112.45, prob_loss:   3.74, total_loss: 117.21\n",
            "[[114 289 290 459   0]\n",
            " [282 296 449 448   0]\n",
            " [ 57 284 211 424   0]\n",
            " [481 239 615 387   0]]\n",
            "[[182   5 543 296   2]]\n",
            "[[   3  183 1510  959    1]]\n",
            "[[  1  18 294 134   1]]\n",
            "epoch: 4 step:   24/60, lr:0.000096, giou_loss:   3.38, conf_loss: 113.84, prob_loss:   6.77, total_loss: 123.99\n",
            "[[ 470  160  992  494    1]\n",
            " [ 628    0 1169  297    1]\n",
            " [   1    2  412  343    1]\n",
            " [   1  248  260  668    1]\n",
            " [ 229  448  710  673    1]\n",
            " [ 822  479 1198  673    1]]\n",
            "[[ 15  12 247 182   1]\n",
            " [ 82  43 267 216   1]\n",
            " [147  56 297 246   1]\n",
            " [265  75 351 290   1]]\n",
            "[[ 65  52 311 313   2]]\n",
            "[[ 85 137 488 545   2]]\n",
            "epoch: 4 step:   25/60, lr:0.000096, giou_loss:   5.99, conf_loss: 115.20, prob_loss:  11.74, total_loss: 132.93\n",
            "[[  74  221 1116  712    1]\n",
            " [ 289  224 1232  751    1]]\n",
            "[[ 37   5 348 276   1]\n",
            " [ 11  65 272 353   1]]\n",
            "[[125 196 295 374   0]\n",
            " [197 144 363 328   0]]\n",
            "[[ 59  72 287 305   2]]\n",
            "epoch: 4 step:   26/60, lr:0.000096, giou_loss:   2.26, conf_loss: 110.60, prob_loss:   4.41, total_loss: 117.26\n",
            "[[  87  163 2543 4053    1]]\n",
            "[[ 91  49 945 927   0]]\n",
            "[[176  70 855 818   0]]\n",
            "[[135 135 358 349   2]\n",
            " [336 127 563 338   0]\n",
            " [ 27 162 383 359   1]]\n",
            "epoch: 4 step:   27/60, lr:0.000096, giou_loss:   1.95, conf_loss: 110.34, prob_loss:   4.52, total_loss: 116.81\n",
            "[[ 22   1 257 245   0]]\n",
            "[[119  72 613 319   1]\n",
            " [142  80 368 306   1]]\n",
            "[[ 67  27 669 480   1]\n",
            " [ 74   3 645 300   1]\n",
            " [ 97   7 608 216   1]\n",
            " [500 147 699 480   1]]\n",
            "[[   4  322  757 1004    0]]\n",
            "epoch: 4 step:   28/60, lr:0.000095, giou_loss:   2.77, conf_loss: 109.97, prob_loss:   5.96, total_loss: 118.70\n",
            "[[  7   5 207 193   0]]\n",
            "[[130 175 331 380   2]]\n",
            "[[ 16  40 202 222   0]]\n",
            "[[197  14 375 192   2]\n",
            " [  4  28 172 171   0]\n",
            " [ 63 245 375 369   1]]\n",
            "epoch: 4 step:   29/60, lr:0.000095, giou_loss:   1.66, conf_loss: 107.95, prob_loss:   3.07, total_loss: 112.68\n",
            "[[309  56 533 287   1]\n",
            " [ 40  82 413 268   1]\n",
            " [ 73   7 448 155   1]]\n",
            "[[ 71  18 258 220   2]]\n",
            "[[  2  85 175 257   2]\n",
            " [118  91 403 374   1]\n",
            " [161   3 325 164   0]]\n",
            "[[ 338    8 1651 1075    1]]\n",
            "epoch: 4 step:   30/60, lr:0.000095, giou_loss:   2.86, conf_loss: 108.60, prob_loss:   4.31, total_loss: 115.78\n",
            "[[106 156 586 360   1]]\n",
            "[[ 13   1 376 348   0]]\n",
            "[[ 63 161 182 276   0]\n",
            " [203 181 334 306   0]\n",
            " [259  64 365 159   0]\n",
            " [326   1 461 105   0]\n",
            " [415  46 536 162   0]\n",
            " [319 104 433 211   0]]\n",
            "[[  7 290 271 542   2]\n",
            " [713 311 984 578   2]\n",
            " [523 256 776 483   2]\n",
            " [263 290 497 498   2]]\n",
            "epoch: 4 step:   31/60, lr:0.000095, giou_loss:   8.07, conf_loss: 115.12, prob_loss:  12.83, total_loss: 136.02\n",
            "[[ 67 163 441 541   2]\n",
            " [209 134 866 348   1]\n",
            " [263 267 849 551   1]]\n",
            "[[204 136 286 224   2]\n",
            " [146 133 219 208   2]\n",
            " [ 16  19 246 115   1]\n",
            " [  5 154  99 245   0]]\n",
            "[[108 271 487 620   0]]\n",
            "[[124 199 557 628   0]]\n",
            "epoch: 4 step:   32/60, lr:0.000095, giou_loss:   3.66, conf_loss: 109.08, prob_loss:   6.28, total_loss: 119.02\n",
            "[[261 278 864 889   2]]\n",
            "[[ 72 168 284 373   2]]\n",
            "[[ 47  14 363 307   1]\n",
            " [140   6 512 181   1]\n",
            " [127   2 493 261   1]\n",
            " [159   8 502 212   1]]\n",
            "[[ 80 128 438 486   2]\n",
            " [ 16 106 613 563   1]]\n",
            "epoch: 4 step:   33/60, lr:0.000095, giou_loss:   1.55, conf_loss: 104.28, prob_loss:   3.19, total_loss: 109.02\n",
            "[[410  23 761 396   0]\n",
            " [ 37  47 444 478   0]]\n",
            "[[498  10 718 211   0]\n",
            " [535 202 753 438   0]\n",
            " [406 162 605 396   0]\n",
            " [  1 283 183 498   0]\n",
            " [113 355 290 552   0]\n",
            " [214 224 455 448   0]\n",
            " [415 335 597 528   0]\n",
            " [133 410 345 583   0]]\n",
            "[[ 75  85 308 304   2]]\n",
            "[[ 58  13 226 183   0]]\n",
            "epoch: 4 step:   34/60, lr:0.000095, giou_loss:   5.00, conf_loss: 110.03, prob_loss:  10.57, total_loss: 125.60\n",
            "[[218 202 546 526   0]\n",
            " [265 487 583 752   0]\n",
            " [  0 509 197 755   0]\n",
            " [ 73 218 332 506   0]\n",
            " [492 102 799 453   0]]\n",
            "[[109  34 302 236   0]]\n",
            "[[ 526    1 1226  708    0]]\n",
            "[[141  32 606 488   2]]\n",
            "epoch: 4 step:   35/60, lr:0.000095, giou_loss:   2.59, conf_loss: 106.43, prob_loss:   6.27, total_loss: 115.29\n",
            "[[ 77  10 590 522   2]]\n",
            "[[1107 1290 1733 2428    1]]\n",
            "[[ 66   1 151  95   1]\n",
            " [  1  30 134 110   1]\n",
            " [149  27 195  99   1]\n",
            " [119   7 179  91   1]]\n",
            "[[157  78 516 411   1]\n",
            " [244  34 682 377   1]\n",
            " [263   3 728 221   1]]\n",
            "epoch: 4 step:   36/60, lr:0.000095, giou_loss:   3.69, conf_loss: 107.58, prob_loss:   9.93, total_loss: 121.20\n",
            "[[ 37  21 324 307   0]]\n",
            "[[ 90  92 709 706   0]]\n",
            "[[  8  15 331 349   0]]\n",
            "[[ 27  48 237 310   1]]\n",
            "epoch: 4 step:   37/60, lr:0.000095, giou_loss:   0.80, conf_loss: 103.09, prob_loss:   0.85, total_loss: 104.74\n",
            "[[204 154 624 596   0]]\n",
            "[[ 60 167 530 609   0]]\n",
            "[[ 41  67 712 324   1]]\n",
            "[[304  68 773 402   0]\n",
            " [  3   3 475 402   0]]\n",
            "epoch: 4 step:   38/60, lr:0.000095, giou_loss:   1.56, conf_loss: 101.48, prob_loss:   2.39, total_loss: 105.43\n",
            "[[ 328  372 1090 1107    2]]\n",
            "[[143  23 340 793   1]]\n",
            "[[263  43 564 363   2]]\n",
            "[[127  93 630 339   1]]\n",
            "epoch: 4 step:   39/60, lr:0.000095, giou_loss:   1.64, conf_loss: 101.89, prob_loss:   3.08, total_loss: 106.61\n",
            "[[ 43  46 317 313   2]]\n",
            "[[ 354   71  777  497    2]\n",
            " [  42  155  522  639    0]\n",
            " [ 152  243 1041  792    1]]\n",
            "[[289 295 527 573   2]\n",
            " [ 11 285 267 556   2]]\n",
            "[[ 706  257 1440  978    2]\n",
            " [ 172  181  860  843    2]]\n",
            "epoch: 4 step:   40/60, lr:0.000095, giou_loss:   2.28, conf_loss: 101.04, prob_loss:   8.67, total_loss: 111.98\n",
            "[[318  86 558 330   2]\n",
            " [ 55  81 309 348   2]]\n",
            "[[184  60 696 617   0]]\n",
            "[[ 175  368 1576 1759    2]]\n",
            "[[ 60  49 256 294   1]]\n",
            "epoch: 4 step:   41/60, lr:0.000095, giou_loss:   1.12, conf_loss:  99.50, prob_loss:   2.88, total_loss: 103.50\n",
            "[[ 38   9 184 152   2]\n",
            " [ 27 151 333 233   1]\n",
            " [188   4 325 149   0]]\n",
            "[[ 53  99 355 413   0]\n",
            " [473 185 752 438   0]\n",
            " [166 246 443 488   0]\n",
            " [613 159 853 427   0]]\n",
            "[[  2  39 580 551   2]]\n",
            "[[ 24  13 273 245   2]]\n",
            "epoch: 4 step:   42/60, lr:0.000095, giou_loss:   2.87, conf_loss: 100.95, prob_loss:   5.67, total_loss: 109.49\n",
            "[[ 87 265 694 868   0]]\n",
            "[[510 256 743 523   0]\n",
            " [334 344 573 568   0]\n",
            " [  5 238 209 470   0]\n",
            " [188 190 418 405   0]\n",
            " [437 175 630 326   0]\n",
            " [296  21 528 215   0]\n",
            " [530  54 695 219   0]]\n",
            "[[244 104 414 392   1]\n",
            " [ 88 127 200 443   1]\n",
            " [ 17  87 168 344   1]\n",
            " [185 140 296 457   1]]\n",
            "[[ 52   7 174 130   2]\n",
            " [139   5 275 149   2]]\n",
            "epoch: 4 step:   43/60, lr:0.000095, giou_loss:   7.26, conf_loss: 106.06, prob_loss:  18.01, total_loss: 131.34\n",
            "[[ 163  294 1003 1130    2]]\n",
            "[[  14  456  821 1395    1]\n",
            " [ 176  405 1141 1083    1]\n",
            " [ 288  280 1186  770    1]\n",
            " [ 242   93 1023  672    1]]\n",
            "[[130  70 307 233   0]]\n",
            "[[ 26  45 209 235   2]]\n",
            "epoch: 4 step:   44/60, lr:0.000094, giou_loss:   2.24, conf_loss:  98.53, prob_loss:   4.86, total_loss: 105.63\n",
            "[[ 553  292 1286 1067    2]\n",
            " [  21  255  714  987    2]]\n",
            "[[ 13 340 305 456   1]]\n",
            "[[103 138 491 313   1]]\n",
            "[[  92    3 1906  926    1]]\n",
            "epoch: 4 step:   45/60, lr:0.000094, giou_loss:   1.84, conf_loss:  96.83, prob_loss:   3.61, total_loss: 102.28\n",
            "[[ 27 208 579 482   1]]\n",
            "[[ 22 112 384 477   0]]\n",
            "[[  28  161 1007 1156    0]]\n",
            "[[ 76  26 245 134   1]]\n",
            "epoch: 4 step:   46/60, lr:0.000094, giou_loss:   0.93, conf_loss:  95.52, prob_loss:   1.26, total_loss:  97.71\n",
            "[[ 18   5 173 152   0]]\n",
            "[[  7   1 314 299   2]]\n",
            "[[ 43  18 250 226   0]]\n",
            "[[  8  61 347 263   1]]\n",
            "epoch: 4 step:   47/60, lr:0.000094, giou_loss:   0.95, conf_loss:  96.14, prob_loss:   1.15, total_loss:  98.24\n",
            "[[  5 117 431 331   1]]\n",
            "[[276  57 872 724   1]]\n",
            "[[  1 250 485 726   0]\n",
            " [284  48 651 521   0]]\n",
            "[[  5  34 409 178   1]]\n",
            "epoch: 4 step:   48/60, lr:0.000094, giou_loss:   1.64, conf_loss:  95.13, prob_loss:   1.55, total_loss:  98.32\n",
            "[[294 127 543 379   0]\n",
            " [ 46 112 326 385   0]]\n",
            "[[131  56 394 344   0]\n",
            " [375  23 634 275   0]]\n",
            "[[   9    1 1582  624    1]]\n",
            "[[195 168 959 510   1]\n",
            " [ 23 149 673 692   1]\n",
            " [243 125 999 423   1]]\n",
            "epoch: 4 step:   49/60, lr:0.000094, giou_loss:   2.62, conf_loss:  95.29, prob_loss:   4.32, total_loss: 102.23\n",
            "[[120  37 299 209   0]]\n",
            "[[ 169   18  786  638    2]\n",
            " [ 744  106 1332  710    2]]\n",
            "[[ 31  39 555 397   1]]\n",
            "[[ 12  34 344 363   2]]\n",
            "epoch: 4 step:   50/60, lr:0.000094, giou_loss:   1.21, conf_loss:  93.71, prob_loss:   2.40, total_loss:  97.31\n",
            "[[ 158  116 1264 1143    0]]\n",
            "[[ 720  502 1098  913    2]\n",
            " [  72  181  794  609    1]\n",
            " [  94  313  764  931    1]]\n",
            "[[ 84  43 403 155   1]]\n",
            "[[277 137 486 358   2]\n",
            " [ 79  99 284 351   2]\n",
            " [189   1 408 171   2]]\n",
            "epoch: 4 step:   51/60, lr:0.000094, giou_loss:   2.32, conf_loss:  94.19, prob_loss:   4.74, total_loss: 101.25\n",
            "[[ 330  268  679  636    2]\n",
            " [ 579  305 1008  656    2]\n",
            " [ 811  245 1155  588    2]\n",
            " [ 555   29  995  387    2]]\n",
            "[[163  43 589 374   1]]\n",
            "[[344  76 640 394   0]]\n",
            "[[ 326  396 1297 1180    1]\n",
            " [ 639  455 1263 1054    0]]\n",
            "epoch: 4 step:   52/60, lr:0.000094, giou_loss:   3.13, conf_loss:  95.30, prob_loss:   7.68, total_loss: 106.11\n",
            "[[248  36 635 229   1]\n",
            " [ 15 108 164 270   1]\n",
            " [102 155 644 344   1]]\n",
            "[[ 79 135 255 324   0]]\n",
            "[[ 93 129 516 551   0]\n",
            " [361  71 685 397   0]]\n",
            "[[135   6 571 243   1]]\n",
            "epoch: 4 step:   53/60, lr:0.000094, giou_loss:   3.91, conf_loss:  94.31, prob_loss:   5.58, total_loss: 103.80\n",
            "[[ 21   4 271 259   0]]\n",
            "[[ 57 100 300 323   2]]\n",
            "[[ 376  168 1483  923    1]\n",
            " [ 509  587 1780 1268    1]\n",
            " [ 207    1 1182  611    1]\n",
            " [  63    1  927  550    1]]\n",
            "[[476 148 613 307   2]\n",
            " [486  73 622 212   2]\n",
            " [147 143 279 288   2]\n",
            " [105  78 254 208   2]\n",
            " [243 100 386 244   2]\n",
            " [310 152 461 294   2]\n",
            " [392  57 525 183   2]]\n",
            "epoch: 4 step:   54/60, lr:0.000094, giou_loss:   6.54, conf_loss:  98.17, prob_loss:  12.86, total_loss: 117.57\n",
            "[[ 75  44 257 255   0]]\n",
            "[[134  98 390 328   2]\n",
            " [  3   0 218 186   2]\n",
            " [380  46 579 296   2]\n",
            " [184  24 396 208   2]\n",
            " [281 271 545 397   2]]\n",
            "[[173  36 566 438   2]]\n",
            "[[ 26  98 362 461   2]]\n",
            "epoch: 4 step:   55/60, lr:0.000094, giou_loss:   2.64, conf_loss:  93.36, prob_loss:   8.94, total_loss: 104.94\n",
            "[[ 18  12 270 271   2]]\n",
            "[[ 19  52 473 508   2]]\n",
            "[[  9  13 901 500   1]]\n",
            "[[  2   2 231 226   2]]\n",
            "epoch: 4 step:   56/60, lr:0.000094, giou_loss:   1.01, conf_loss:  90.88, prob_loss:   3.31, total_loss:  95.21\n",
            "[[ 76  33 322 258   0]\n",
            " [347  30 534 280   0]\n",
            " [198   5 419 220   0]]\n",
            "[[177  39 281 146   2]]\n",
            "[[ 39  21 462 350   1]]\n",
            "[[ 83  25 390 357   0]]\n",
            "epoch: 4 step:   57/60, lr:0.000094, giou_loss:   2.40, conf_loss:  91.49, prob_loss:   2.80, total_loss:  96.69\n",
            "[[199  67 309 177   0]\n",
            " [131  40 232 135   0]\n",
            " [ 23  53 127 164   0]]\n",
            "[[ 523   26 1172  730    0]]\n",
            "[[ 41 260 943 738   1]]\n",
            "[[ 11 149 430 343   1]\n",
            " [102  86 424 223   1]\n",
            " [108  31 383 166   1]]\n",
            "epoch: 4 step:   58/60, lr:0.000094, giou_loss:   2.52, conf_loss:  91.88, prob_loss:   4.49, total_loss:  98.89\n",
            "[[373 181 685 469   2]\n",
            " [105 333 930 529   1]\n",
            " [115 452 958 660   1]]\n",
            "[[179 196 542 685   1]]\n",
            "[[133   6 637 533   0]]\n",
            "[[ 28  47 656 221   1]\n",
            " [ 14 135 601 445   1]\n",
            " [  1  43 442 337   1]]\n",
            "epoch: 4 step:   59/60, lr:0.000093, giou_loss:   3.09, conf_loss:  90.61, prob_loss:   4.63, total_loss:  98.33\n",
            "[[113  94 569 580   2]]\n",
            "[[100 107 584 397   1]]\n",
            "[[ 23  15 219 198   0]]\n",
            "[[ 14  44 270 300   0]]\n",
            "epoch: 4 step:    0/60, lr:0.000093, giou_loss:   1.50, conf_loss:  88.84, prob_loss:   1.18, total_loss:  91.52\n",
            "[[ 92  48 405 174   1]\n",
            " [ 30  67 253 282   1]\n",
            " [ 76  76 368 225   1]]\n",
            "[[397 213 961 793   0]\n",
            " [ 78 176 590 665   0]]\n",
            "[[ 289  215 1943 1112    1]\n",
            " [   9  264 1522 1332    1]\n",
            " [ 406  189 1999  768    1]]\n",
            "[[337  83 579 338   2]\n",
            " [134 148 360 404   0]\n",
            " [173 228 746 528   1]]\n",
            "epoch: 4 step:    1/60, lr:0.000093, giou_loss:   3.62, conf_loss:  90.96, prob_loss:   6.72, total_loss: 101.30\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[227  62 723 500   0]]\n",
            "[[150 165 923 983   2]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[145  36 421 286   2]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "\n",
            "\n",
            "giou_val_loss:   4.15, conf_val_loss: 292.06, prob_val_loss:   9.61, total_val_loss: 305.81\n",
            "\n",
            "\n",
            "[[ 811  226 2276 1800    2]]\n",
            "[[ 10   4 164 149   2]]\n",
            "[[ 145   11 1458 1078    1]]\n",
            "[[ 84  43 403 155   1]]\n",
            "epoch: 5 step:    2/60, lr:0.000093, giou_loss:   1.23, conf_loss:  87.18, prob_loss:   2.01, total_loss:  90.41\n",
            "[[  1  37 405 181   1]]\n",
            "[[307  44 556 296   0]\n",
            " [ 59  29 339 302   0]]\n",
            "[[438  36 858 412   2]\n",
            " [ 38 289 985 591   1]\n",
            " [113  77 482 389   0]]\n",
            "[[222  87 734 644   0]]\n",
            "epoch: 5 step:    3/60, lr:0.000093, giou_loss:   2.36, conf_loss:  87.68, prob_loss:   2.77, total_loss:  92.81\n",
            "[[206  80 564 438   2]\n",
            " [142  58 739 515   1]]\n",
            "[[ 95  60 589 307   1]\n",
            " [118  68 344 294   1]]\n",
            "[[201   4 443 259   2]\n",
            " [420  69 646 325   0]\n",
            " [ 34 149 607 449   1]]\n",
            "[[114  42 347 261   2]]\n",
            "epoch: 5 step:    4/60, lr:0.000093, giou_loss:   3.27, conf_loss:  89.15, prob_loss:  10.39, total_loss: 102.81\n",
            "[[213  31 459 256   0]\n",
            " [  1  28 188 278   0]\n",
            " [116   3 337 218   0]]\n",
            "[[247  52 458 248   2]\n",
            " [296 291 505 509   2]\n",
            " [116 371 328 570   2]\n",
            " [191 456 393 626   2]]\n",
            "[[ 34  47 258 289   2]]\n",
            "[[ 74 178 468 559   2]\n",
            " [550 164 951 550   0]\n",
            " [166 604 847 966   1]]\n",
            "epoch: 5 step:    5/60, lr:0.000093, giou_loss:   4.31, conf_loss:  90.96, prob_loss:   7.99, total_loss: 103.26\n",
            "[[ 26   1 831 214   1]]\n",
            "[[ 38 174 595 408   1]]\n",
            "[[ 10  76 317 408   0]]\n",
            "[[191  86 743 360   1]]\n",
            "epoch: 5 step:    6/60, lr:0.000093, giou_loss:   1.15, conf_loss:  84.27, prob_loss:   0.93, total_loss:  86.35\n",
            "[[ 15  23 347 352   2]]\n",
            "[[ 25  14 208 204   2]]\n",
            "[[169 171 765 838   1]]\n",
            "[[564   4 860 276   1]\n",
            " [103  34 371 377   1]\n",
            " [186  52 475 396   1]\n",
            " [215 132 475 526   1]\n",
            " [429 119 599 545   1]\n",
            " [495 118 786 453   1]\n",
            " [449  33 822 314   1]]\n",
            "epoch: 5 step:    7/60, lr:0.000093, giou_loss:   5.63, conf_loss:  90.17, prob_loss:   9.07, total_loss: 104.87\n",
            "[[202 137 411 358   2]\n",
            " [  4  99 209 351   2]\n",
            " [114   1 333 171   2]]\n",
            "[[ 32   3 343 274   1]\n",
            " [  6  63 267 351   1]]\n",
            "[[ 28  63 226 254   2]\n",
            " [192  40 361 213   2]]\n",
            "[[  9 259 301 375   1]]\n",
            "epoch: 5 step:    8/60, lr:0.000093, giou_loss:   2.56, conf_loss:  85.17, prob_loss:   8.38, total_loss:  96.11\n",
            "[[199  82 384 174   1]\n",
            " [ 12  81 196 175   1]]\n",
            "[[310   2 735 411   2]]\n",
            "[[ 23  30 271 286   2]]\n",
            "[[ 741  361 1390 1065    0]]\n",
            "epoch: 5 step:    9/60, lr:0.000093, giou_loss:   2.45, conf_loss:  85.52, prob_loss:   4.49, total_loss:  92.46\n",
            "[[ 83  40 975 527   1]]\n",
            "[[ 606  196 1209  807    2]]\n",
            "[[ 95  63 597 266   1]]\n",
            "[[ 31  27 633 480   1]\n",
            " [ 55   3 626 300   1]\n",
            " [ 92   7 603 216   1]\n",
            " [  1 147 200 480   1]]\n",
            "epoch: 5 step:   10/60, lr:0.000093, giou_loss:   2.09, conf_loss:  83.87, prob_loss:   3.23, total_loss:  89.18\n",
            "[[ 23  59 353 391   2]]\n",
            "[[ 18   1 320 315   0]\n",
            " [438  87 717 340   0]\n",
            " [131 148 408 390   0]\n",
            " [578  61 818 329   0]]\n",
            "[[117 258 587 700   0]]\n",
            "[[ 68  65 209 306   1]\n",
            " [200  54 284 315   1]\n",
            " [290  71 363 310   1]\n",
            " [366  78 461 315   1]]\n",
            "epoch: 5 step:   11/60, lr:0.000093, giou_loss:   5.58, conf_loss:  87.72, prob_loss:   8.09, total_loss: 101.40\n",
            "[[249   9 637 184   1]]\n",
            "[[  18   64 1375 1384    0]]\n",
            "[[ 74  55 253 225   2]]\n",
            "[[ 110  730  913 1551    0]\n",
            " [   1    1  407  617    0]\n",
            " [ 104    4  624  557    0]]\n",
            "epoch: 5 step:   12/60, lr:0.000092, giou_loss:   1.73, conf_loss:  83.10, prob_loss:   4.19, total_loss:  89.02\n",
            "[[303  83 772 417   0]\n",
            " [  2  18 474 417   0]]\n",
            "[[140  22 314 197   0]\n",
            " [ 17   9 167 177   0]]\n",
            "[[147 218 336 412   0]\n",
            " [ 99 131 264 316   0]\n",
            " [235 142 404 324   0]]\n",
            "[[ 43 128 714 385   1]]\n",
            "epoch: 5 step:   13/60, lr:0.000092, giou_loss:   2.88, conf_loss:  84.08, prob_loss:   6.00, total_loss:  92.96\n",
            "[[ 37  29 214 196   0]]\n",
            "[[ 100  151 1079 1146    0]]\n",
            "[[ 549   43 1282  818    2]\n",
            " [  17    6  710  738    2]]\n",
            "[[205 209 967 944   2]]\n",
            "epoch: 5 step:   14/60, lr:0.000092, giou_loss:   1.14, conf_loss:  81.37, prob_loss:   2.64, total_loss:  85.15\n",
            "[[ 24  18 878 896   0]]\n",
            "[[ 30  29 465 471   0]]\n",
            "[[269 244 433 394   2]\n",
            " [295  26 486 222   0]\n",
            " [476  21 602 408   1]\n",
            " [518 139 782 467   1]\n",
            " [529  20 683 397   1]]\n",
            "[[ 95  29 579 319   1]]\n",
            "epoch: 5 step:   15/60, lr:0.000092, giou_loss:   4.47, conf_loss:  83.60, prob_loss:   8.93, total_loss:  97.00\n",
            "[[ 131   14 1010  900    2]]\n",
            "[[ 13   4 242 228   2]]\n",
            "[[  19  347  397  758    2]\n",
            " [ 323   26 1045  454    1]\n",
            " [ 353  158 1023  776    1]]\n",
            "[[ 11  17 158 161   0]]\n",
            "epoch: 5 step:   16/60, lr:0.000092, giou_loss:   1.50, conf_loss:  81.44, prob_loss:   3.86, total_loss:  86.81\n",
            "[[  4  48 216 253   2]]\n",
            "[[ 21 137 107 225   2]\n",
            " [ 53 124 131 215   2]]\n",
            "[[ 433   34 1186  716    0]]\n",
            "[[  14  524  821 1463    1]\n",
            " [ 176  473 1141 1151    1]\n",
            " [ 288  348 1186  838    1]\n",
            " [ 242  161 1023  740    1]]\n",
            "epoch: 5 step:   17/60, lr:0.000092, giou_loss:   2.79, conf_loss:  81.98, prob_loss:   6.35, total_loss:  91.12\n",
            "[[ 317  168 1424  923    1]\n",
            " [ 450  587 1721 1268    1]\n",
            " [ 148    1 1123  611    1]\n",
            " [   4    1  868  550    1]]\n",
            "[[ 57  81 425 470   2]]\n",
            "[[ 36   4 191 151   0]]\n",
            "[[ 630  147 1202 1147    1]]\n",
            "epoch: 5 step:   18/60, lr:0.000092, giou_loss:   2.72, conf_loss:  79.79, prob_loss:   5.77, total_loss:  88.29\n",
            "[[239  91 412 263   2]\n",
            " [ 11  97 296 380   1]\n",
            " [ 89   9 253 170   0]]\n",
            "[[   0  456  955 1092    1]\n",
            " [   0  660  781 1168    1]\n",
            " [ 319  171 1499 1044    1]]\n",
            "[[ 70  25 290 226   0]\n",
            " [ 35 217 253 453   0]\n",
            " [183 177 382 411   0]\n",
            " [605 298 787 513   0]\n",
            " [498 370 675 567   0]\n",
            " [333 239 574 463   0]\n",
            " [191 350 373 543   0]\n",
            " [443 425 655 598   0]]\n",
            "[[ 470  161  992  495    1]\n",
            " [ 628    1 1169  298    1]\n",
            " [   1    3  412  344    1]\n",
            " [   1  249  260  669    1]\n",
            " [ 229  449  710  674    1]\n",
            " [ 822  480 1198  674    1]]\n",
            "epoch: 5 step:   19/60, lr:0.000092, giou_loss:   9.86, conf_loss:  92.73, prob_loss:  15.40, total_loss: 117.99\n",
            "[[ 53 148 955 626   1]]\n",
            "[[  2   1 702 708   0]]\n",
            "[[ 168    1 1126  741    2]\n",
            " [   0  643  798 1066    2]]\n",
            "[[338  58 985 422   1]]\n",
            "epoch: 5 step:   20/60, lr:0.000092, giou_loss:   1.65, conf_loss:  79.03, prob_loss:   2.00, total_loss:  82.69\n",
            "[[  16   56 1058  547    1]\n",
            " [ 231   59 1174  586    1]]\n",
            "[[ 48  22 454 441   0]]\n",
            "[[ 82 244 431 612   2]\n",
            " [331 281 760 632   2]\n",
            " [563 221 907 564   2]\n",
            " [307   5 747 363   2]]\n",
            "[[233 104 840 707   0]]\n",
            "epoch: 5 step:   21/60, lr:0.000092, giou_loss:   3.05, conf_loss:  82.56, prob_loss:   5.66, total_loss:  91.27\n",
            "[[  57  196 1711 1093    1]\n",
            " [ 478  245 1991 1313    1]\n",
            " [   1  170 1594  749    1]]\n",
            "[[354  56 578 287   1]\n",
            " [ 85  82 458 268   1]\n",
            " [118   7 493 155   1]]\n",
            "[[177  52 374 822   1]]\n",
            "[[ 73  32 386 158   1]\n",
            " [ 11  51 234 266   1]\n",
            " [ 57  60 349 209   1]]\n",
            "epoch: 5 step:   22/60, lr:0.000092, giou_loss:   3.59, conf_loss:  79.51, prob_loss:   5.52, total_loss:  88.62\n",
            "[[  2  13 284 258   0]]\n",
            "[[ 28 141 390 506   0]]\n",
            "[[ 15  18 302 304   0]]\n",
            "[[141 148 564 570   0]\n",
            " [409  90 733 416   0]]\n",
            "epoch: 5 step:   23/60, lr:0.000092, giou_loss:   1.57, conf_loss:  77.76, prob_loss:   3.55, total_loss:  82.89\n",
            "[[125 103 246 246   2]\n",
            " [119   0 225 114   2]\n",
            " [ 23  61 153 186   2]\n",
            " [ 23   1 133  69   2]\n",
            " [ 52 200 174 249   2]]\n",
            "[[  0  92 405 434   1]]\n",
            "[[499  83 955 569   2]]\n",
            "[[106 156 586 360   1]]\n",
            "epoch: 5 step:   24/60, lr:0.000092, giou_loss:   2.78, conf_loss:  80.25, prob_loss:   9.74, total_loss:  92.77\n",
            "[[ 46  59 397 432   0]\n",
            " [363  83 770 514   0]]\n",
            "[[ 469   17  892  443    2]\n",
            " [ 157  101  637  585    0]\n",
            " [ 267  189 1156  738    1]]\n",
            "[[  8  15 331 349   0]]\n",
            "[[ 50   4 509 254   1]]\n",
            "epoch: 5 step:   25/60, lr:0.000091, giou_loss:   1.60, conf_loss:  76.77, prob_loss:   4.35, total_loss:  82.72\n",
            "[[  3  25 253 280   0]]\n",
            "[[ 64 201 183 316   0]\n",
            " [204 221 335 346   0]\n",
            " [260 104 366 199   0]\n",
            " [327  41 462 145   0]\n",
            " [416  86 537 202   0]\n",
            " [320 144 434 251   0]]\n",
            "[[ 27  25 204 188   0]]\n",
            "[[   1  135 1181  601    1]]\n",
            "epoch: 5 step:   26/60, lr:0.000091, giou_loss:   5.73, conf_loss:  82.06, prob_loss:  10.36, total_loss:  98.15\n",
            "[[  45  346  309  598    2]\n",
            " [ 751  367 1022  634    2]\n",
            " [ 561  312  814  539    2]\n",
            " [ 301  346  535  554    2]]\n",
            "[[ 38  81 363 410   0]\n",
            " [308  48 579 339   0]]\n",
            "[[ 12  12 212 200   0]]\n",
            "[[  52  297  892 1133    2]]\n",
            "epoch: 5 step:   27/60, lr:0.000091, giou_loss:   3.88, conf_loss:  79.40, prob_loss:   9.24, total_loss:  92.52\n",
            "[[186  58 409 272   2]\n",
            " [387  50 614 261   0]\n",
            " [ 78  85 434 282   1]]\n",
            "[[ 94  36 713 650   0]]\n",
            "[[150  37 418 314   0]]\n",
            "[[ 20   6 381 297   2]]\n",
            "epoch: 5 step:   28/60, lr:0.000091, giou_loss:   1.27, conf_loss:  75.65, prob_loss:   4.17, total_loss:  81.08\n",
            "[[  5   3 285 304   2]]\n",
            "[[339 160 577 438   2]\n",
            " [ 61 150 317 421   2]]\n",
            "[[ 88 120 767 868   0]]\n",
            "[[170 192 858 712   1]\n",
            " [  3 192 432 712   1]]\n",
            "epoch: 5 step:   29/60, lr:0.000091, giou_loss:   1.60, conf_loss:  75.00, prob_loss:   3.66, total_loss:  80.26\n",
            "[[ 65 108 629 688   0]\n",
            " [436  71 948 560   0]]\n",
            "[[  37  223 1438 1614    2]]\n",
            "[[ 21  27 319 315   0]]\n",
            "[[162 153 327 348   2]\n",
            " [320 148 513 371   0]\n",
            " [ 22 366 567 619   1]]\n",
            "epoch: 5 step:   30/60, lr:0.000091, giou_loss:   2.56, conf_loss:  76.64, prob_loss:   6.06, total_loss:  85.26\n",
            "[[168 207 531 696   1]]\n",
            "[[112  29 515 437   2]]\n",
            "[[ 63  44 319 300   0]]\n",
            "[[ 49  72 580 577   2]]\n",
            "epoch: 5 step:   31/60, lr:0.000091, giou_loss:   1.03, conf_loss:  73.85, prob_loss:   1.46, total_loss:  76.34\n",
            "[[  6  60 299 176   1]]\n",
            "[[ 37  52 205 222   0]]\n",
            "[[184  95 380 340   1]]\n",
            "[[ 90  68 336 329   2]]\n",
            "epoch: 5 step:   32/60, lr:0.000091, giou_loss:   1.42, conf_loss:  73.75, prob_loss:   3.48, total_loss:  78.65\n",
            "[[136 199 569 628   0]]\n",
            "[[231  57 547 350   1]\n",
            " [ 82  49 454 224   1]\n",
            " [101  45 467 304   1]\n",
            " [ 92  51 435 255   1]]\n",
            "[[ 59 205 226 382   0]\n",
            " [193 197 370 354   0]\n",
            " [133 106 312 275   0]]\n",
            "[[108 277 223 376   2]\n",
            " [518 255 598 330   2]\n",
            " [492 198 590 275   2]\n",
            " [122 165 379 265   1]\n",
            " [160 207 394 302   1]\n",
            " [296 164 453 309   1]\n",
            " [324 214 460 343   1]\n",
            " [301  89 410 192   0]\n",
            " [411 122 525 227   0]]\n",
            "epoch: 5 step:   33/60, lr:0.000091, giou_loss:  11.18, conf_loss:  84.00, prob_loss:  16.36, total_loss: 111.54\n",
            "[[  10  625 2466 4515    1]]\n",
            "[[575 432 858 702   0]\n",
            " [483 135 731 388   0]\n",
            " [167 193 481 460   0]\n",
            " [358  11 610 234   0]\n",
            " [633  29 898 263   0]]\n",
            "[[137   7 573 244   1]]\n",
            "[[475 318 785 634   2]\n",
            " [ 18 181 624 398   1]\n",
            " [ 24 239 674 494   1]\n",
            " [ 72  16 334 309   0]]\n",
            "epoch: 5 step:   34/60, lr:0.000091, giou_loss:   5.51, conf_loss:  79.14, prob_loss:   6.36, total_loss:  91.01\n",
            "[[141 177 311 465   1]\n",
            " [355 200 467 516   1]\n",
            " [387 160 538 417   1]\n",
            " [259 213 370 530   1]]\n",
            "[[ 73  13 260 215   2]]\n",
            "[[291  55 588 342   0]]\n",
            "[[ 38  43 245 251   0]]\n",
            "epoch: 5 step:   35/60, lr:0.000091, giou_loss:   5.27, conf_loss:  76.09, prob_loss:   5.89, total_loss:  87.25\n",
            "[[196 306 575 655   0]]\n",
            "[[369  96 825 465   2]]\n",
            "[[  3  33 434 474   2]]\n",
            "[[ 37  17 665 191   1]\n",
            " [ 92 105 679 415   1]\n",
            " [251  13 692 307   1]]\n",
            "epoch: 5 step:   36/60, lr:0.000091, giou_loss:   2.19, conf_loss:  72.45, prob_loss:   4.72, total_loss:  79.35\n",
            "[[ 18 283 502 759   0]\n",
            " [301  81 668 554   0]]\n",
            "[[216   1 720 528   0]]\n",
            "[[116  19 423 317   2]]\n",
            "[[ 10  63 498 370   1]]\n",
            "epoch: 5 step:   37/60, lr:0.000090, giou_loss:   1.05, conf_loss:  71.39, prob_loss:   1.73, total_loss:  74.18\n",
            "[[197  94 398 299   2]]\n",
            "[[206  33 384 211   2]\n",
            " [ 13  47 181 190   0]\n",
            " [ 72 264 384 388   1]]\n",
            "[[  7  32 256 264   2]]\n",
            "[[109  48 633 406   1]]\n",
            "epoch: 5 step:   38/60, lr:0.000090, giou_loss:   1.37, conf_loss:  72.02, prob_loss:   3.04, total_loss:  76.43\n",
            "[[ 37  41 223 223   0]]\n",
            "[[ 69  83 238 191   1]]\n",
            "[[212  90 403 279   2]\n",
            " [325  82 485 260   2]]\n",
            "[[128  33 631 279   1]]\n",
            "epoch: 5 step:   39/60, lr:0.000090, giou_loss:   1.52, conf_loss:  71.40, prob_loss:   2.39, total_loss:  75.30\n",
            "[[ 49  32 318 296   0]]\n",
            "[[  1  55 194 257   0]]\n",
            "[[  5  56 115 166   0]\n",
            " [ 82  29 183 124   0]\n",
            " [187  42 291 153   0]]\n",
            "[[114 289 290 459   0]\n",
            " [282 296 449 448   0]\n",
            " [ 57 284 211 424   0]\n",
            " [481 239 615 387   0]]\n",
            "epoch: 5 step:   40/60, lr:0.000090, giou_loss:   4.46, conf_loss:  75.61, prob_loss:  12.54, total_loss:  92.61\n",
            "[[149   4 392 227   2]]\n",
            "[[ 59  72 287 305   2]]\n",
            "[[ 568  282 1185  902    2]\n",
            " [  22  370  610  974    2]]\n",
            "[[183 230 947 572   1]\n",
            " [ 11 211 661 754   1]\n",
            " [231 187 987 485   1]]\n",
            "epoch: 5 step:   41/60, lr:0.000090, giou_loss:   2.26, conf_loss:  70.56, prob_loss:   6.25, total_loss:  79.07\n",
            "[[ 52   7 174 130   2]\n",
            " [139   5 275 149   2]]\n",
            "[[  1 213 427 427   1]]\n",
            "[[ 17  10 291 277   2]]\n",
            "[[  9  34 590 439   1]\n",
            " [ 22 165 597 648   1]]\n",
            "epoch: 5 step:   42/60, lr:0.000090, giou_loss:   1.85, conf_loss:  71.00, prob_loss:   3.54, total_loss:  76.39\n",
            "[[ 42  86 282 330   2]\n",
            " [291  81 545 348   2]]\n",
            "[[108  44 531 373   1]]\n",
            "[[ 15   1 480 457   2]]\n",
            "[[379  90 717 451   2]\n",
            " [ 69  50 421 413   2]]\n",
            "epoch: 5 step:   43/60, lr:0.000090, giou_loss:   1.23, conf_loss:  68.85, prob_loss:   4.17, total_loss:  74.25\n",
            "[[ 22  17 420 432   0]]\n",
            "[[ 59  71 461 160   1]\n",
            " [ 66  79 462 231   1]\n",
            " [ 75 115 472 324   1]\n",
            " [198 135 549 406   1]]\n",
            "[[  8 141 427 335   1]\n",
            " [ 14  78 336 215   1]\n",
            " [ 55  23 330 158   1]]\n",
            "[[201  48 627 379   1]]\n",
            "epoch: 5 step:   44/60, lr:0.000090, giou_loss:   3.23, conf_loss:  71.38, prob_loss:   3.77, total_loss:  78.39\n",
            "[[285  38 548 326   0]\n",
            " [ 45   5 304 257   0]]\n",
            "[[184   6 416 176   1]\n",
            " [164  37 349 210   1]\n",
            " [134  50 284 240   1]\n",
            " [ 80  69 166 284   1]]\n",
            "[[531 255 764 522   0]\n",
            " [355 343 594 567   0]\n",
            " [ 26 237 230 469   0]\n",
            " [209 189 439 404   0]\n",
            " [458 174 651 325   0]\n",
            " [317  20 549 214   0]\n",
            " [551  53 716 218   0]]\n",
            "[[244 202 663 600   2]\n",
            " [664 128 919 369   2]]\n",
            "epoch: 5 step:   45/60, lr:0.000090, giou_loss:   7.03, conf_loss:  77.42, prob_loss:  11.04, total_loss:  95.49\n",
            "[[334  16 646 304   2]\n",
            " [ 66 168 891 364   1]\n",
            " [ 76 287 919 495   1]]\n",
            "[[212  57 381 212   2]\n",
            " [102  49 262 212   2]\n",
            " [ 58   2 200 143   2]]\n",
            "[[ 12  42 276 314   2]]\n",
            "[[536 381 815 662   2]\n",
            " [345 224 655 482   2]\n",
            " [298 471 631 708   2]]\n",
            "epoch: 5 step:   46/60, lr:0.000090, giou_loss:   4.66, conf_loss:  72.54, prob_loss:  11.02, total_loss:  88.22\n",
            "[[ 447   69 1025  581    2]]\n",
            "[[  56  193 1563  969    1]]\n",
            "[[ 22 233 286 546   2]\n",
            " [ 13  81 333 336   0]\n",
            " [ 88  17 637 319   1]]\n",
            "[[204 154 624 596   0]]\n",
            "epoch: 5 step:   47/60, lr:0.000090, giou_loss:   1.67, conf_loss:  68.14, prob_loss:   2.61, total_loss:  72.42\n",
            "[[455 147 592 306   2]\n",
            " [465  72 601 211   2]\n",
            " [126 142 258 287   2]\n",
            " [ 84  77 233 207   2]\n",
            " [222  99 365 243   2]\n",
            " [289 151 440 293   2]\n",
            " [371  56 504 182   2]]\n",
            "[[ 76  69 412 439   2]\n",
            " [430  58 759 380   2]]\n",
            "[[171  43 833 746   2]]\n",
            "[[ 25 149 201 338   0]]\n",
            "epoch: 5 step:   48/60, lr:0.000090, giou_loss:   5.84, conf_loss:  73.42, prob_loss:  13.73, total_loss:  92.99\n",
            "[[160  37 496 400   2]]\n",
            "[[457 101 831 479   2]\n",
            " [ 32  72 689 286   1]\n",
            " [ 49 205 635 489   1]]\n",
            "[[167  29 346 201   0]]\n",
            "[[121  20 270 235   1]]\n",
            "epoch: 5 step:   49/60, lr:0.000089, giou_loss:   1.79, conf_loss:  68.90, prob_loss:   2.47, total_loss:  73.16\n",
            "[[ 38  43 339 363   2]]\n",
            "[[353  76 712 409   1]\n",
            " [187  32 625 375   1]\n",
            " [141   1 606 219   1]]\n",
            "[[ 130  800  764 1800    1]\n",
            " [ 546  621 1071 1679    1]]\n",
            "[[151  72 544 474   2]]\n",
            "epoch: 5 step:   50/60, lr:0.000089, giou_loss:   2.61, conf_loss:  68.34, prob_loss:   4.37, total_loss:  75.32\n",
            "[[  23   77  757  798    2]\n",
            " [ 603    1 1291  663    2]]\n",
            "[[ 24  25 220 208   0]]\n",
            "[[ 48  51 502 507   2]]\n",
            "[[ 16  13 142 153   0]]\n",
            "epoch: 5 step:   51/60, lr:0.000089, giou_loss:   1.20, conf_loss:  67.22, prob_loss:   2.76, total_loss:  71.18\n",
            "[[140 136 466 411   0]]\n",
            "[[  13   52 1827  975    1]]\n",
            "[[  7  35 346 237   1]]\n",
            "[[ 17  42 562 559   2]]\n",
            "epoch: 5 step:   52/60, lr:0.000089, giou_loss:   1.31, conf_loss:  66.32, prob_loss:   0.99, total_loss:  68.62\n",
            "[[141 193 311 371   0]\n",
            " [ 73 141 239 325   0]]\n",
            "[[137  99 393 329   2]\n",
            " [  6   1 221 187   2]\n",
            " [383  47 582 297   2]\n",
            " [187  25 399 209   2]\n",
            " [284 272 548 398   2]]\n",
            "[[  3 144  85 232   2]\n",
            " [ 70 141 143 216   2]\n",
            " [ 43  27 273 123   1]\n",
            " [190 162 284 253   0]]\n",
            "[[ 38 157 343 463   0]\n",
            " [365 322 653 587   0]\n",
            " [281 186 564 441   0]]\n",
            "epoch: 5 step:   53/60, lr:0.000089, giou_loss:   6.58, conf_loss:  73.42, prob_loss:  11.71, total_loss:  91.71\n",
            "[[323   1 686 348   0]]\n",
            "[[ 41  12 293 271   2]]\n",
            "[[ 343 1208  969 2346    1]]\n",
            "[[  7  48 265 296   2]\n",
            " [308  10 607 309   2]]\n",
            "epoch: 5 step:   54/60, lr:0.000089, giou_loss:   1.73, conf_loss:  66.14, prob_loss:   4.76, total_loss:  72.63\n",
            "[[ 203  431 1174 1215    1]\n",
            " [ 237  490  861 1089    0]]\n",
            "[[ 54  72 604 624   0]\n",
            " [446  78 887 546   0]\n",
            " [683  88 960 487   0]]\n",
            "[[ 63  48 273 310   1]]\n",
            "[[ 670   56 1093  463    2]\n",
            " [ 242  153  626  580    2]\n",
            " [ 202  539  682  971    2]]\n",
            "epoch: 5 step:   55/60, lr:0.000089, giou_loss:   3.55, conf_loss:  68.92, prob_loss:   6.04, total_loss:  78.51\n",
            "[[236 203 703 478   1]\n",
            " [260 142 693 372   1]]\n",
            "[[ 62 106 575 618   2]]\n",
            "[[225  47 658 372   1]\n",
            " [136  62 510 424   1]\n",
            " [277  33 716 278   1]]\n",
            "[[130  61 272 206   0]\n",
            " [220 184 342 305   0]\n",
            " [183 300 335 424   0]\n",
            " [  2 170 168 327   0]\n",
            " [310  90 409 180   0]\n",
            " [378 237 452 312   0]\n",
            " [403  66 464 127   0]\n",
            " [396 141 461 208   0]]\n",
            "epoch: 5 step:   56/60, lr:0.000089, giou_loss:   6.35, conf_loss:  73.16, prob_loss:   9.81, total_loss:  89.32\n",
            "[[  1   7 319 311   1]]\n",
            "[[  25   51 2757 2534    1]]\n",
            "[[ 87  32 530 427   1]]\n",
            "[[344  76 640 394   0]]\n",
            "epoch: 5 step:   57/60, lr:0.000089, giou_loss:   1.02, conf_loss:  65.85, prob_loss:   1.33, total_loss:  68.20\n",
            "[[  2  33  99 138   2]]\n",
            "[[218 126 546 450   0]\n",
            " [265 411 583 676   0]\n",
            " [  0 433 197 679   0]\n",
            " [ 73 142 332 430   0]\n",
            " [492  26 799 377   0]]\n",
            "[[ 84 132 266 343   0]]\n",
            "[[ 36  35 364 337   1]\n",
            " [ 82 112 480 369   1]\n",
            " [ 96  29 408 224   1]]\n",
            "epoch: 5 step:   58/60, lr:0.000089, giou_loss:   3.81, conf_loss:  68.08, prob_loss:   4.91, total_loss:  76.80\n",
            "[[ 28  26 169 183   0]\n",
            " [168  21 270 130   0]\n",
            " [119  89 230 198   0]]\n",
            "[[ 66  20 151 114   1]\n",
            " [  1  49 134 129   1]\n",
            " [149  46 195 118   1]\n",
            " [119  26 179 110   1]]\n",
            "[[ 336  116 1442 1143    0]]\n",
            "[[ 28   3 415 196   1]\n",
            " [499  75 648 237   1]\n",
            " [ 19 122 561 311   1]]\n",
            "epoch: 5 step:   59/60, lr:0.000089, giou_loss:   4.48, conf_loss:  68.74, prob_loss:  10.47, total_loss:  83.69\n",
            "[[ 19  26 259 230   0]]\n",
            "[[ 70  37 305 281   0]]\n",
            "[[ 12  20 374 347   2]]\n",
            "[[137  55 359 274   2]\n",
            " [  2  40 202 262   2]]\n",
            "epoch: 5 step:    0/60, lr:0.000088, giou_loss:   1.78, conf_loss:  64.72, prob_loss:   3.92, total_loss:  70.43\n",
            "[[ 102   99 1969 1919    2]]\n",
            "[[ 350    1 1923  624    1]]\n",
            "[[103 117 207 224   2]]\n",
            "[[160   6 306 149   2]\n",
            " [ 11 148 317 230   1]\n",
            " [ 19   1 156 146   0]]\n",
            "epoch: 5 step:    1/60, lr:0.000088, giou_loss:   1.62, conf_loss:  63.57, prob_loss:   2.32, total_loss:  67.51\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[145  36 421 286   2]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "[[166  92 638 559   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[174  20 364 199   2]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[150 165 923 983   2]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[203 115 584 452   1]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "\n",
            "\n",
            "giou_val_loss:   4.09, conf_val_loss: 156.82, prob_val_loss:   9.99, total_val_loss: 170.89\n",
            "\n",
            "\n",
            "[[  25  377  832 1316    1]\n",
            " [ 187  326 1152 1004    1]\n",
            " [ 299  201 1197  691    1]\n",
            " [ 253   14 1034  593    1]]\n",
            "[[167 276 651 752   0]\n",
            " [  1  74 368 547   0]]\n",
            "[[ 36  38 271 282   0]]\n",
            "[[ 27  82 353 357   0]]\n",
            "epoch: 6 step:    2/60, lr:0.000088, giou_loss:   2.20, conf_loss:  64.88, prob_loss:   6.65, total_loss:  73.72\n",
            "[[ 572   59 1150  571    2]]\n",
            "[[226  31 494 308   0]]\n",
            "[[  63   27 1717  924    1]\n",
            " [ 484   76 1997 1144    1]\n",
            " [   7    1 1600  580    1]]\n",
            "[[  43   82 1616  705    1]]\n",
            "epoch: 6 step:    3/60, lr:0.000088, giou_loss:   1.68, conf_loss:  64.02, prob_loss:   2.11, total_loss:  67.81\n",
            "[[  4  12 187 202   2]]\n",
            "[[105  96 559 552   2]]\n",
            "[[ 47  25 299 284   2]]\n",
            "[[ 630  147 1202 1147    1]]\n",
            "epoch: 6 step:    4/60, lr:0.000088, giou_loss:   1.56, conf_loss:  63.00, prob_loss:   4.13, total_loss:  68.69\n",
            "[[ 35 407 840 620   1]]\n",
            "[[ 45  82 470 491   2]]\n",
            "[[164 218 353 412   0]\n",
            " [236 131 401 316   0]\n",
            " [ 96 142 265 324   0]]\n",
            "[[  8  48 338 380   2]]\n",
            "epoch: 6 step:    5/60, lr:0.000088, giou_loss:   2.24, conf_loss:  63.43, prob_loss:   2.59, total_loss:  68.26\n",
            "[[142 148 307 343   2]\n",
            " [300 143 493 366   0]\n",
            " [  2 361 547 614   1]]\n",
            "[[ 47  17 290 240   2]]\n",
            "[[119  72 613 319   1]\n",
            " [142  80 368 306   1]]\n",
            "[[   8   71  741  846    2]\n",
            " [ 580   34 1273  766    2]]\n",
            "epoch: 6 step:    6/60, lr:0.000088, giou_loss:   3.20, conf_loss:  63.78, prob_loss:  10.31, total_loss:  77.29\n",
            "[[  99  730  902 1551    0]\n",
            " [ 605    1 1011  617    0]\n",
            " [ 388    4  908  557    0]]\n",
            "[[516 264 749 531   0]\n",
            " [340 352 579 576   0]\n",
            " [ 11 246 215 478   0]\n",
            " [194 198 424 413   0]\n",
            " [443 183 636 334   0]\n",
            " [302  29 534 223   0]\n",
            " [536  62 701 227   0]]\n",
            "[[548  69 884 439   2]\n",
            " [201  58 530 380   2]]\n",
            "[[ 12  27 262 282   0]]\n",
            "epoch: 6 step:    7/60, lr:0.000088, giou_loss:   5.55, conf_loss:  72.38, prob_loss:  14.16, total_loss:  92.09\n",
            "[[ 42  28 477 470   0]]\n",
            "[[ 23  54 200 217   0]]\n",
            "[[ 562  123 1296  844    2]\n",
            " [  28   47  716  709    2]]\n",
            "[[129  14 316 216   2]]\n",
            "epoch: 6 step:    8/60, lr:0.000088, giou_loss:   0.90, conf_loss:  61.11, prob_loss:   2.44, total_loss:  64.44\n",
            "[[369  96 825 465   2]]\n",
            "[[156  67 325 222   2]\n",
            " [ 46  59 206 222   2]\n",
            " [  2  12 144 153   2]]\n",
            "[[  2  94 203 299   2]]\n",
            "[[ 97  89 485 264   1]]\n",
            "epoch: 6 step:    9/60, lr:0.000088, giou_loss:   1.90, conf_loss:  63.27, prob_loss:   4.55, total_loss:  69.73\n",
            "[[464 200 583 315   0]\n",
            " [312 220 443 345   0]\n",
            " [281 103 387 198   0]\n",
            " [185  40 320 144   0]\n",
            " [110  85 231 201   0]\n",
            " [213 143 327 250   0]]\n",
            "[[ 55  34 579 392   1]]\n",
            "[[ 33  26 369 389   2]]\n",
            "[[129  50 326 820   1]]\n",
            "epoch: 6 step:   10/60, lr:0.000087, giou_loss:   5.32, conf_loss:  67.05, prob_loss:   7.60, total_loss:  79.97\n",
            "[[ 23   8 191 178   0]]\n",
            "[[ 42  58 280 336   2]\n",
            " [302  48 558 319   2]]\n",
            "[[194 102 418 344   2]]\n",
            "[[  9  40 249 284   2]\n",
            " [258  35 512 302   2]]\n",
            "epoch: 6 step:   11/60, lr:0.000087, giou_loss:   1.29, conf_loss:  60.59, prob_loss:   2.34, total_loss:  64.22\n",
            "[[  78    2 2810 2485    1]]\n",
            "[[111 126 281 304   0]\n",
            " [ 43  74 209 258   0]]\n",
            "[[ 184  221 1226  712    1]\n",
            " [  68  224 1011  751    1]]\n",
            "[[733 247 997 499   2]\n",
            " [ 20 268 291 535   2]\n",
            " [228 213 481 440   2]\n",
            " [507 247 741 455   2]]\n",
            "epoch: 6 step:   12/60, lr:0.000087, giou_loss:   3.96, conf_loss:  65.15, prob_loss:   5.49, total_loss:  74.59\n",
            "[[345  47 591 272   0]\n",
            " [133  44 320 294   0]\n",
            " [248  19 469 234   0]]\n",
            "[[537 424 820 694   0]\n",
            " [445 127 693 380   0]\n",
            " [129 185 443 452   0]\n",
            " [320   3 572 226   0]\n",
            " [595  21 860 255   0]]\n",
            "[[232  96 465 315   2]]\n",
            "[[ 822  381 1448 1519    1]]\n",
            "epoch: 6 step:   13/60, lr:0.000087, giou_loss:   5.75, conf_loss:  66.98, prob_loss:   7.11, total_loss:  79.85\n",
            "[[ 66  19 509 414   1]]\n",
            "[[316 203 595 484   2]\n",
            " [125  46 435 304   2]\n",
            " [ 78 293 411 530   2]]\n",
            "[[  9   1 241 171   1]\n",
            " [ 76  32 261 205   1]\n",
            " [141  45 291 235   1]\n",
            " [259  64 345 279   1]]\n",
            "[[  92    3 1906  926    1]]\n",
            "epoch: 6 step:   14/60, lr:0.000087, giou_loss:   4.63, conf_loss:  65.07, prob_loss:   7.21, total_loss:  76.92\n",
            "[[ 83 114 220 273   2]\n",
            " [ 74  39 210 178   2]\n",
            " [417 109 549 254   2]\n",
            " [442  44 591 174   2]\n",
            " [310  66 453 210   2]\n",
            " [235 118 386 260   2]\n",
            " [171  23 304 149   2]]\n",
            "[[338  69 676 430   2]\n",
            " [ 28  29 380 392   2]]\n",
            "[[321  40 689 429   2]]\n",
            "[[124  62 393 326   0]]\n",
            "epoch: 6 step:   15/60, lr:0.000087, giou_loss:   5.49, conf_loss:  66.93, prob_loss:  10.48, total_loss:  82.90\n",
            "[[308  43 889 448   1]\n",
            " [301 174 876 657   1]]\n",
            "[[582 163 956 541   2]\n",
            " [157 134 814 348   1]\n",
            " [174 267 760 551   1]]\n",
            "[[ 16  11 265 243   2]]\n",
            "[[204   1 708 528   0]]\n",
            "epoch: 6 step:   16/60, lr:0.000087, giou_loss:   2.39, conf_loss:  60.63, prob_loss:   3.09, total_loss:  66.11\n",
            "[[  3  25 150 169   0]]\n",
            "[[  3 175 405 264   1]\n",
            " [ 10 183 406 335   1]\n",
            " [ 19 219 416 428   1]\n",
            " [142 239 493 510   1]]\n",
            "[[  4  92 409 434   1]]\n",
            "[[158  27 594 264   1]]\n",
            "epoch: 6 step:   17/60, lr:0.000087, giou_loss:   1.82, conf_loss:  60.14, prob_loss:   2.65, total_loss:  64.61\n",
            "[[197  55 793 722   1]]\n",
            "[[373 104 514 345   1]\n",
            " [298  93 382 354   1]\n",
            " [219 110 292 349   1]\n",
            " [121 117 216 354   1]]\n",
            "[[ 53  78 605 352   1]]\n",
            "[[ 19   1 719 708   0]]\n",
            "epoch: 6 step:   18/60, lr:0.000087, giou_loss:   4.36, conf_loss:  62.46, prob_loss:   6.24, total_loss:  73.05\n",
            "[[ 317  168 1424  923    1]\n",
            " [ 450  587 1721 1268    1]\n",
            " [ 148    1 1123  611    1]\n",
            " [   4    1  868  550    1]]\n",
            "[[ 112  255 1577 1829    2]]\n",
            "[[251   7 493 262   2]\n",
            " [ 48  72 274 328   0]\n",
            " [ 87 152 660 452   1]]\n",
            "[[ 15  81 125 191   0]\n",
            " [ 92  54 193 149   0]\n",
            " [197  67 301 178   0]]\n",
            "epoch: 6 step:   19/60, lr:0.000087, giou_loss:   4.41, conf_loss:  62.07, prob_loss:   6.52, total_loss:  73.01\n",
            "[[ 10  39 361 412   0]\n",
            " [327  63 734 494   0]]\n",
            "[[  8  62 185 229   0]]\n",
            "[[ 101  370 1003  848    1]]\n",
            "[[221 117 579 475   2]\n",
            " [ 46  95 643 552   1]]\n",
            "epoch: 6 step:   20/60, lr:0.000087, giou_loss:   2.14, conf_loss:  59.85, prob_loss:   2.21, total_loss:  64.21\n",
            "[[142 211 312 499   1]\n",
            " [356 234 468 550   1]\n",
            " [388 194 539 451   1]\n",
            " [260 247 371 564   1]]\n",
            "[[ 661  111 1310  815    0]]\n",
            "[[122  57 729 660   0]]\n",
            "[[  3  31 285 276   0]]\n",
            "epoch: 6 step:   21/60, lr:0.000086, giou_loss:   4.76, conf_loss:  61.10, prob_loss:   5.49, total_loss:  71.34\n",
            "[[117  82 302 174   1]\n",
            " [305  81 489 175   1]]\n",
            "[[ 367  827 1001 1827    1]\n",
            " [  60  648  585 1706    1]]\n",
            "[[ 14 313 324 629   2]\n",
            " [175 176 781 393   1]\n",
            " [125 234 775 489   1]\n",
            " [465  11 727 304   0]]\n",
            "[[179 102 407 335   2]]\n",
            "epoch: 6 step:   22/60, lr:0.000086, giou_loss:   4.93, conf_loss:  61.70, prob_loss:   8.35, total_loss:  74.99\n",
            "[[127  15 438 286   1]\n",
            " [203  75 464 363   1]]\n",
            "[[415 261 720 567   0]\n",
            " [105 426 393 691   0]\n",
            " [194 290 477 545   0]]\n",
            "[[ 833  334 1450  954    2]\n",
            " [ 287  422  875 1026    2]]\n",
            "[[180  79 402 298   2]\n",
            " [ 45  64 245 286   2]]\n",
            "epoch: 6 step:   23/60, lr:0.000086, giou_loss:   3.76, conf_loss:  59.86, prob_loss:   7.50, total_loss:  71.12\n",
            "[[ 65  52 311 313   2]]\n",
            "[[ 424  149  736  437    2]\n",
            " [ 156  301  981  497    1]\n",
            " [ 166  420 1009  628    1]]\n",
            "[[ 212  144 1191 1139    0]]\n",
            "[[ 66  36 151 130   1]\n",
            " [  1  65 134 145   1]\n",
            " [149  62 195 134   1]\n",
            " [119  42 179 126   1]]\n",
            "epoch: 6 step:   24/60, lr:0.000086, giou_loss:   3.50, conf_loss:  60.41, prob_loss:   4.83, total_loss:  68.74\n",
            "[[106   7 228 130   2]\n",
            " [  5   5 141 149   2]]\n",
            "[[ 16   8 171 155   0]]\n",
            "[[ 43   3 430 196   1]\n",
            " [514  75 663 237   1]\n",
            " [ 34 122 576 311   1]]\n",
            "[[120  12 436 305   1]\n",
            " [213   4 585 179   1]\n",
            " [200   0 566 259   1]\n",
            " [232   6 575 210   1]]\n",
            "epoch: 6 step:   25/60, lr:0.000086, giou_loss:   2.86, conf_loss:  59.19, prob_loss:   5.14, total_loss:  67.20\n",
            "[[152   6 298 149   2]\n",
            " [  3 148 309 230   1]\n",
            " [ 11   1 148 146   0]]\n",
            "[[ 22  54 448 268   1]]\n",
            "[[ 26  26 313 312   0]]\n",
            "[[ 53 236 317 549   2]\n",
            " [ 44  84 364 339   0]\n",
            " [119  20 668 322   1]]\n",
            "epoch: 6 step:   26/60, lr:0.000086, giou_loss:   2.67, conf_loss:  57.79, prob_loss:   1.68, total_loss:  62.14\n",
            "[[ 31  82 157 222   0]]\n",
            "[[  3   1 310 299   2]]\n",
            "[[  7  18 236 242   2]]\n",
            "[[117  68 380 356   0]\n",
            " [361  35 620 287   0]]\n",
            "epoch: 6 step:   27/60, lr:0.000086, giou_loss:   1.43, conf_loss:  57.09, prob_loss:   2.04, total_loss:  60.56\n",
            "[[404 173 827 595   0]\n",
            " [235 115 559 441   0]]\n",
            "[[ 10  42 403 444   2]]\n",
            "[[ 47  81 592 598   2]]\n",
            "[[ 598  402 1553 1038    1]\n",
            " [ 772  606 1553 1114    1]\n",
            " [  54  117 1234  990    1]]\n",
            "epoch: 6 step:   28/60, lr:0.000086, giou_loss:   2.37, conf_loss:  57.18, prob_loss:   3.32, total_loss:  62.87\n",
            "[[ 54   5 361 337   0]]\n",
            "[[ 12 292 176 442   2]\n",
            " [ 38  74 229 270   0]\n",
            " [219  69 345 456   1]\n",
            " [261 187 525 515   1]\n",
            " [272  68 426 445   1]]\n",
            "[[ 232  330  610  741    2]\n",
            " [ 536    9 1258  437    1]\n",
            " [ 566  141 1236  759    1]]\n",
            "[[ 22  35 506 325   1]]\n",
            "epoch: 6 step:   29/60, lr:0.000086, giou_loss:   5.11, conf_loss:  59.54, prob_loss:   9.14, total_loss:  73.80\n",
            "[[  6  51 462 537   2]]\n",
            "[[ 82 316 461 665   0]]\n",
            "[[269  32 597 334   1]\n",
            " [153 109 551 366   1]\n",
            " [225  26 537 221   1]]\n",
            "[[ 84  71 446 398   2]]\n",
            "epoch: 6 step:   30/60, lr:0.000086, giou_loss:   2.56, conf_loss:  56.52, prob_loss:   3.16, total_loss:  62.24\n",
            "[[ 27  18 655 192   1]\n",
            " [ 13 106 600 416   1]\n",
            " [  0  14 441 308   1]]\n",
            "[[122  56 625 302   1]]\n",
            "[[ 33  38 912 924   2]]\n",
            "[[ 39  55 445 474   0]]\n",
            "epoch: 6 step:   31/60, lr:0.000085, giou_loss:   1.63, conf_loss:  55.44, prob_loss:   1.12, total_loss:  58.18\n",
            "[[  9   4 432 333   1]]\n",
            "[[ 13 340 305 456   1]]\n",
            "[[ 83 246 198 345   2]\n",
            " [493 224 573 299   2]\n",
            " [467 167 565 244   2]\n",
            " [ 97 134 354 234   1]\n",
            " [135 176 369 271   1]\n",
            " [271 133 428 278   1]\n",
            " [299 183 435 312   1]\n",
            " [276  58 385 161   0]\n",
            " [386  91 500 196   0]]\n",
            "[[ 75  36 469 417   2]\n",
            " [551  22 952 408   0]\n",
            " [167 462 848 824   1]]\n",
            "epoch: 6 step:   32/60, lr:0.000085, giou_loss:   9.46, conf_loss:  63.50, prob_loss:  12.94, total_loss:  85.90\n",
            "[[ 10  20 258 276   2]]\n",
            "[[ 13  12 332 124   1]]\n",
            "[[ 198   82 1304 1109    0]]\n",
            "[[  16  271  778 1006    2]]\n",
            "epoch: 6 step:   33/60, lr:0.000085, giou_loss:   1.17, conf_loss:  54.58, prob_loss:   1.46, total_loss:  57.21\n",
            "[[143  87 563 529   0]]\n",
            "[[ 16  30 290 297   2]]\n",
            "[[ 29  66 287 314   2]\n",
            " [330  28 629 327   2]]\n",
            "[[  15   85 1372 1405    0]]\n",
            "epoch: 6 step:   34/60, lr:0.000085, giou_loss:   0.76, conf_loss:  56.02, prob_loss:   2.51, total_loss:  59.29\n",
            "[[ 284  429  887 1040    2]]\n",
            "[[  8 107 687 855   0]]\n",
            "[[ 25 159 589 739   0]\n",
            " [396 122 908 611   0]]\n",
            "[[193  65 375 276   0]]\n",
            "epoch: 6 step:   35/60, lr:0.000085, giou_loss:   1.12, conf_loss:  54.02, prob_loss:   3.41, total_loss:  58.55\n",
            "[[  5  92 562 326   1]]\n",
            "[[ 43  68 229 250   0]]\n",
            "[[ 354  344 1107 1026    0]]\n",
            "[[ 14  44 270 300   0]]\n",
            "epoch: 6 step:   36/60, lr:0.000085, giou_loss:   0.87, conf_loss:  53.93, prob_loss:   1.91, total_loss:  56.71\n",
            "[[136   2 347 198   2]\n",
            " [ 89 241 298 459   2]\n",
            " [266 321 478 520   2]\n",
            " [201 406 403 576   2]]\n",
            "[[132  98 591 348   1]]\n",
            "[[ 12  27 416 171   1]]\n",
            "[[222  61 400 239   2]\n",
            " [ 29  75 197 218   0]\n",
            " [ 88 292 400 416   1]]\n",
            "epoch: 6 step:   37/60, lr:0.000085, giou_loss:   4.30, conf_loss:  57.87, prob_loss:   5.22, total_loss:  67.38\n",
            "[[141  18 315 193   0]\n",
            " [ 18   5 168 173   0]]\n",
            "[[  27  263 1428 1654    2]]\n",
            "[[ 12 128 431 322   1]\n",
            " [ 18  65 340 202   1]\n",
            " [ 59  10 334 145   1]]\n",
            "[[ 33  31 346 157   1]\n",
            " [185  50 408 265   1]\n",
            " [ 70  59 362 208   1]]\n",
            "epoch: 6 step:   38/60, lr:0.000085, giou_loss:   3.18, conf_loss:  55.78, prob_loss:   3.70, total_loss:  62.67\n",
            "[[267  57 665 472   0]]\n",
            "[[ 24  20 178 165   2]]\n",
            "[[  20   13 2476 3903    1]]\n",
            "[[ 99 178 266 355   0]\n",
            " [233 170 410 327   0]\n",
            " [173  79 352 248   0]]\n",
            "epoch: 6 step:   39/60, lr:0.000085, giou_loss:   1.82, conf_loss:  54.54, prob_loss:   2.02, total_loss:  58.37\n",
            "[[ 44  41 936 528   1]]\n",
            "[[342  83 701 416   1]\n",
            " [176  39 614 382   1]\n",
            " [130   8 595 226   1]]\n",
            "[[ 168    1 1126  741    2]\n",
            " [   0  643  798 1066    2]]\n",
            "[[ 16  87 256 291   0]]\n",
            "epoch: 6 step:   40/60, lr:0.000084, giou_loss:   2.12, conf_loss:  55.02, prob_loss:   2.36, total_loss:  59.49\n",
            "[[ 94  11 520 342   1]]\n",
            "[[ 35   7 242 215   0]]\n",
            "[[ 22 104 143 247   2]\n",
            " [ 43   1 149 115   2]\n",
            " [115  62 245 187   2]\n",
            " [135   2 245  70   2]\n",
            " [ 94 201 216 250   2]]\n",
            "[[  13   46 1193  512    1]]\n",
            "epoch: 6 step:   41/60, lr:0.000084, giou_loss:   2.57, conf_loss:  55.43, prob_loss:   4.02, total_loss:  62.02\n",
            "[[ 93  23 179 111   2]\n",
            " [ 69  10 147 101   2]]\n",
            "[[356  52 906 604   0]\n",
            " [ 73  58 514 526   0]\n",
            " [  0  68 277 467   0]]\n",
            "[[ 93  46 289 291   1]]\n",
            "[[ 35  64 204 172   1]]\n",
            "epoch: 6 step:   42/60, lr:0.000084, giou_loss:   2.33, conf_loss:  53.97, prob_loss:   3.30, total_loss:  59.60\n",
            "[[ 36   7 300 279   2]]\n",
            "[[130  24 234 131   2]]\n",
            "[[121 161 588 436   1]\n",
            " [145 100 578 330   1]]\n",
            "[[280 146 642 511   0]]\n",
            "epoch: 6 step:   43/60, lr:0.000084, giou_loss:   1.72, conf_loss:  52.89, prob_loss:   1.50, total_loss:  56.11\n",
            "[[ 18   4 314 276   1]\n",
            " [507  34 775 377   1]\n",
            " [403  52 692 396   1]\n",
            " [403 132 663 526   1]\n",
            " [279 119 449 545   1]\n",
            " [ 92 118 383 453   1]\n",
            " [ 56  33 429 314   1]]\n",
            "[[151 138 682 643   2]]\n",
            "[[ 95  63 597 266   1]]\n",
            "[[  4  41 202 232   2]\n",
            " [168  18 337 191   2]]\n",
            "epoch: 6 step:   44/60, lr:0.000084, giou_loss:   6.04, conf_loss:  59.25, prob_loss:   7.54, total_loss:  72.83\n",
            "[[ 87  27 689 480   1]\n",
            " [ 94   3 665 300   1]\n",
            " [117   7 628 216   1]\n",
            " [520 147 719 480   1]]\n",
            "[[ 207  160  729  494    1]\n",
            " [  30    0  571  297    1]\n",
            " [ 787    2 1198  343    1]\n",
            " [ 939  248 1198  668    1]\n",
            " [ 489  448  970  673    1]\n",
            " [   1  479  377  673    1]]\n",
            "[[168  88 856 608   1]\n",
            " [  1  88 430 608   1]]\n",
            "[[ 70  25 290 226   0]\n",
            " [ 35 217 253 453   0]\n",
            " [183 177 382 411   0]\n",
            " [605 298 787 513   0]\n",
            " [498 370 675 567   0]\n",
            " [333 239 574 463   0]\n",
            " [191 350 373 543   0]\n",
            " [443 425 655 598   0]]\n",
            "epoch: 6 step:   45/60, lr:0.000084, giou_loss:   8.94, conf_loss:  64.95, prob_loss:  15.74, total_loss:  89.62\n",
            "[[ 52 164 816 506   1]\n",
            " [338 145 988 688   1]\n",
            " [ 12 121 768 419   1]]\n",
            "[[381 100 557 270   0]\n",
            " [222 107 389 259   0]\n",
            " [460  95 614 235   0]\n",
            " [ 56  50 190 198   0]]\n",
            "[[ 485   43  908  469    2]\n",
            " [ 173  127  653  611    0]\n",
            " [ 283  215 1172  764    1]]\n",
            "[[ 55   8 204 223   1]]\n",
            "epoch: 6 step:   46/60, lr:0.000084, giou_loss:   4.63, conf_loss:  56.44, prob_loss:   5.09, total_loss:  66.16\n",
            "[[ 13  31 223 293   1]]\n",
            "[[ 27  38 388 329   2]]\n",
            "[[  93  116 1600  892    1]]\n",
            "[[ 21  27 319 315   0]]\n",
            "epoch: 6 step:   47/60, lr:0.000084, giou_loss:   0.79, conf_loss:  52.51, prob_loss:   0.78, total_loss:  54.08\n",
            "[[  5  70 474 404   0]\n",
            " [303   5 775 404   0]]\n",
            "[[147  15 578 456   2]]\n",
            "[[  1  55 194 257   0]]\n",
            "[[ 21  38 992 822   1]\n",
            " [ 55  97 679 696   0]]\n",
            "epoch: 6 step:   48/60, lr:0.000084, giou_loss:   1.79, conf_loss:  52.63, prob_loss:   2.34, total_loss:  56.76\n",
            "[[243  95 568 424   0]\n",
            " [ 27  62 298 353   0]]\n",
            "[[ 279   10 1592 1077    1]]\n",
            "[[130 128 309 298   2]]\n",
            "[[ 35 124 548 636   2]]\n",
            "epoch: 6 step:   49/60, lr:0.000084, giou_loss:   1.44, conf_loss:  52.33, prob_loss:   2.22, total_loss:  55.99\n",
            "[[254 139 582 463   0]\n",
            " [217 424 535 689   0]\n",
            " [603 446 800 692   0]\n",
            " [468 155 727 443   0]\n",
            " [  1  39 308 390   0]]\n",
            "[[207 159 289 247   2]\n",
            " [149 156 222 231   2]\n",
            " [ 19  42 249 138   1]\n",
            " [  8 177 102 268   0]]\n",
            "[[129  34 271 179   0]\n",
            " [219 157 341 278   0]\n",
            " [182 273 334 397   0]\n",
            " [  1 143 167 300   0]\n",
            " [309  63 408 153   0]\n",
            " [377 210 451 285   0]\n",
            " [402  39 463 100   0]\n",
            " [395 114 460 181   0]]\n",
            "[[339 227 851 784   0]]\n",
            "epoch: 6 step:   50/60, lr:0.000083, giou_loss:  10.17, conf_loss:  65.74, prob_loss:  15.33, total_loss:  91.24\n",
            "[[ 10  48 189 220   0]]\n",
            "[[ 969  133 1392  540    2]\n",
            " [ 541  230  925  657    2]\n",
            " [ 501  616  981 1048    2]]\n",
            "[[108  51 962 929   0]]\n",
            "[[194  25 495 345   2]]\n",
            "epoch: 6 step:   51/60, lr:0.000083, giou_loss:   2.94, conf_loss:  53.81, prob_loss:   5.65, total_loss:  62.40\n",
            "[[ 42  12 507 468   2]]\n",
            "[[ 31   7 693 710   2]]\n",
            "[[ 269  227 1109 1063    2]]\n",
            "[[ 33 139 242 360   2]\n",
            " [235 101 440 353   2]\n",
            " [111   3 330 173   2]]\n",
            "epoch: 6 step:   52/60, lr:0.000083, giou_loss:   1.70, conf_loss:  51.65, prob_loss:   8.33, total_loss:  61.67\n",
            "[[286   3 649 350   0]]\n",
            "[[  4 128 675 385   1]]\n",
            "[[307 219 530 433   2]\n",
            " [102 211 329 422   0]\n",
            " [282 246 638 443   1]]\n",
            "[[  3   2 296 118   1]]\n",
            "epoch: 6 step:   53/60, lr:0.000083, giou_loss:   2.23, conf_loss:  51.99, prob_loss:   4.21, total_loss:  58.43\n",
            "[[  30   39 1897 1859    2]]\n",
            "[[124 199 557 628   0]]\n",
            "[[ 26  22 222 205   0]]\n",
            "[[181 138 477 456   0]]\n",
            "epoch: 6 step:   54/60, lr:0.000083, giou_loss:   0.82, conf_loss:  50.29, prob_loss:   0.83, total_loss:  51.95\n",
            "[[  1   4 281 305   2]]\n",
            "[[ 240  306  589  674    2]\n",
            " [ 489  343  918  694    2]\n",
            " [ 721  283 1065  626    2]\n",
            " [ 465   67  905  425    2]]\n",
            "[[ 23 116 199 305   0]]\n",
            "[[  5  14 408 422   2]]\n",
            "epoch: 6 step:   55/60, lr:0.000083, giou_loss:   3.09, conf_loss:  53.69, prob_loss:   9.65, total_loss:  66.43\n",
            "[[375 134 624 386   0]\n",
            " [127 119 407 392   0]]\n",
            "[[505  47 807 361   0]\n",
            " [108 133 387 386   0]\n",
            " [417 194 694 436   0]\n",
            " [  7 107 247 375   0]]\n",
            "[[ 44  38 185 195   0]\n",
            " [184  33 286 142   0]\n",
            " [135 101 246 210   0]]\n",
            "[[ 37  52 369 381   2]]\n",
            "epoch: 6 step:   56/60, lr:0.000083, giou_loss:   3.65, conf_loss:  53.15, prob_loss:   4.23, total_loss:  61.03\n",
            "[[240  49 673 374   1]\n",
            " [388  64 762 426   1]\n",
            " [182  35 621 280   1]]\n",
            "[[189  99 445 329   2]\n",
            " [361   1 576 187   2]\n",
            " [  0  47 199 297   2]\n",
            " [183  25 395 209   2]\n",
            " [ 34 272 298 398   2]]\n",
            "[[ 56 155 280 386   1]\n",
            " [176 181 549 367   1]\n",
            " [141 106 516 254   1]]\n",
            "[[ 33 145 652 759   0]]\n",
            "epoch: 6 step:   57/60, lr:0.000083, giou_loss:   3.91, conf_loss:  54.70, prob_loss:   6.58, total_loss:  65.19\n",
            "[[  1 122 174 294   2]\n",
            " [117 128 402 411   1]\n",
            " [160  40 324 201   0]]\n",
            "[[ 47   7 144 112   2]]\n",
            "[[ 51 234 521 676   0]]\n",
            "[[ 460   76  880  452    2]\n",
            " [  60  329 1007  631    1]\n",
            " [ 135  117  504  429    0]]\n",
            "epoch: 6 step:   58/60, lr:0.000083, giou_loss:   2.54, conf_loss:  52.34, prob_loss:   4.55, total_loss:  59.43\n",
            "[[ 19  21 337 325   1]]\n",
            "[[ 75  21 563 328   1]]\n",
            "[[ 12  12 212 200   0]]\n",
            "[[ 46  80 343 367   0]]\n",
            "epoch: 6 step:   59/60, lr:0.000082, giou_loss:   0.91, conf_loss:  49.18, prob_loss:   1.00, total_loss:  51.08\n",
            "[[105  56 752 420   1]]\n",
            "[[ 46  85 409 574   1]]\n",
            "[[  6  43 345 245   1]]\n",
            "[[ 90   3 570 207   1]]\n",
            "epoch: 6 step:    0/60, lr:0.000082, giou_loss:   1.51, conf_loss:  49.86, prob_loss:   2.16, total_loss:  53.53\n",
            "[[ 16   4 339 338   0]]\n",
            "[[217  87 636 485   2]\n",
            " [637  13 892 254   2]]\n",
            "[[210  94 401 283   2]\n",
            " [128  86 288 264   2]]\n",
            "[[ 75 154 287 359   2]]\n",
            "epoch: 6 step:    1/60, lr:0.000082, giou_loss:   1.53, conf_loss:  49.65, prob_loss:   4.88, total_loss:  56.07\n",
            "[[ 85 138 553 352   1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[155 105 453 436   0]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "\n",
            "\n",
            "giou_val_loss:   4.23, conf_val_loss:  91.67, prob_val_loss:  10.30, total_val_loss: 106.19\n",
            "\n",
            "\n",
            "[[258  22 694 259   1]]\n",
            "[[408  46 972 626   0]\n",
            " [ 89   9 601 498   0]]\n",
            "[[532  57 868 427   2]\n",
            " [185  46 514 368   2]]\n",
            "[[399 238 678 519   2]\n",
            " [559  81 869 339   2]\n",
            " [583 328 916 565   2]]\n",
            "epoch: 7 step:    2/60, lr:0.000082, giou_loss:   3.05, conf_loss:  52.38, prob_loss:   5.60, total_loss:  61.02\n",
            "[[ 77 235 510 664   0]]\n",
            "[[117  57 597 261   1]]\n",
            "[[  2  63 230 296   2]]\n",
            "[[ 67 163 441 541   2]\n",
            " [209 134 866 348   1]\n",
            " [263 267 849 551   1]]\n",
            "epoch: 7 step:    3/60, lr:0.000082, giou_loss:   2.32, conf_loss:  49.44, prob_loss:   3.03, total_loss:  54.79\n",
            "[[151  98 407 328   2]\n",
            " [ 20   0 235 186   2]\n",
            " [397  46 596 296   2]\n",
            " [201  24 413 208   2]\n",
            " [298 271 562 397   2]]\n",
            "[[ 387  505 1004 1125    2]\n",
            " [ 962  593 1550 1197    2]]\n",
            "[[ 57  19 183 159   0]]\n",
            "[[ 47  61 282 305   0]]\n",
            "epoch: 7 step:    4/60, lr:0.000082, giou_loss:   2.62, conf_loss:  50.94, prob_loss:   4.97, total_loss:  58.53\n",
            "[[  7  36 438 477   2]]\n",
            "[[  8  16 282 283   2]]\n",
            "[[ 87 136 173 224   2]\n",
            " [ 63 123 141 214   2]]\n",
            "[[  11    4 1191  470    1]]\n",
            "epoch: 7 step:    5/60, lr:0.000082, giou_loss:   1.48, conf_loss:  48.91, prob_loss:   3.24, total_loss:  53.63\n",
            "[[  61  191 1715 1088    1]\n",
            " [ 482  240 1995 1308    1]\n",
            " [   5  165 1598  744    1]]\n",
            "[[ 130  800  764 1800    1]\n",
            " [ 546  621 1071 1679    1]]\n",
            "[[509 125 811 439   0]\n",
            " [112 211 391 464   0]\n",
            " [421 272 698 514   0]\n",
            " [ 11 185 251 453   0]]\n",
            "[[  1  32 187 214   0]]\n",
            "epoch: 7 step:    6/60, lr:0.000082, giou_loss:   4.37, conf_loss:  51.94, prob_loss:   6.11, total_loss:  62.42\n",
            "[[  5  49 507 252   1]]\n",
            "[[236  10 414 188   2]\n",
            " [ 43  24 211 167   0]\n",
            " [102 241 414 365   1]]\n",
            "[[ 56  28 253 798   1]]\n",
            "[[100  21 407 353   0]]\n",
            "epoch: 7 step:    7/60, lr:0.000082, giou_loss:   1.85, conf_loss:  48.56, prob_loss:   1.32, total_loss:  51.73\n",
            "[[356  59 906 611   0]\n",
            " [ 73  65 514 533   0]\n",
            " [  0  75 277 474   0]]\n",
            "[[  6  47 653 411   1]]\n",
            "[[245  69 710 525   2]]\n",
            "[[206   3 569 350   0]]\n",
            "epoch: 7 step:    8/60, lr:0.000081, giou_loss:   1.52, conf_loss:  48.32, prob_loss:   1.85, total_loss:  51.69\n",
            "[[ 184  221 1226  712    1]\n",
            " [  68  224 1011  751    1]]\n",
            "[[ 75  21 563 328   1]]\n",
            "[[ 816  403 1165  771    2]\n",
            " [ 487  440  916  791    2]\n",
            " [ 340  380  684  723    2]\n",
            " [ 500  164  940  522    2]]\n",
            "[[ 34   9 469 451   0]]\n",
            "epoch: 7 step:    9/60, lr:0.000081, giou_loss:   1.95, conf_loss:  50.55, prob_loss:   4.27, total_loss:  56.77\n",
            "[[ 240   67  812 1067    1]]\n",
            "[[390 391 815 800   2]]\n",
            "[[143  72 471 374   1]\n",
            " [189 149 587 406   1]\n",
            " [203  66 515 261   1]]\n",
            "[[433 161 552 276   0]\n",
            " [281 181 412 306   0]\n",
            " [250  64 356 159   0]\n",
            " [154   1 289 105   0]\n",
            " [ 79  46 200 162   0]\n",
            " [182 104 296 211   0]]\n",
            "epoch: 7 step:   10/60, lr:0.000081, giou_loss:   6.02, conf_loss:  55.81, prob_loss:   9.05, total_loss:  70.89\n",
            "[[ 34 261 298 574   2]\n",
            " [ 25 109 345 364   0]\n",
            " [100  45 649 347   1]]\n",
            "[[305  76 774 410   0]\n",
            " [  4  11 476 410   0]]\n",
            "[[  0  92 405 434   1]]\n",
            "[[489   6 785 278   1]\n",
            " [ 28  36 296 379   1]\n",
            " [111  54 400 398   1]\n",
            " [140 134 400 528   1]\n",
            " [354 121 524 547   1]\n",
            " [420 120 711 455   1]\n",
            " [374  35 747 316   1]]\n",
            "epoch: 7 step:   11/60, lr:0.000081, giou_loss:   5.33, conf_loss:  52.87, prob_loss:   9.65, total_loss:  67.84\n",
            "[[  3  10 179 199   0]]\n",
            "[[ 28  30 454 244   1]]\n",
            "[[ 54   8 209 155   0]]\n",
            "[[  3  14 296 130   1]]\n",
            "epoch: 7 step:   12/60, lr:0.000081, giou_loss:   1.10, conf_loss:  47.04, prob_loss:   1.35, total_loss:  49.48\n",
            "[[125  58 302 221   0]]\n",
            "[[144  46 340 291   1]]\n",
            "[[159  45 520 336   2]]\n",
            "[[ 423    2 1736 1069    1]]\n",
            "epoch: 7 step:   13/60, lr:0.000081, giou_loss:   0.94, conf_loss:  46.74, prob_loss:   2.25, total_loss:  49.93\n",
            "[[111 103 232 246   2]\n",
            " [105   0 211 114   2]\n",
            " [  9  61 139 186   2]\n",
            " [  9   1 119  69   2]\n",
            " [ 38 200 160 249   2]]\n",
            "[[ 76  16 600 374   1]]\n",
            "[[ 14   8 278 280   2]]\n",
            "[[ 25 111 198 283   2]\n",
            " [141 117 426 400   1]\n",
            " [184  29 348 190   0]]\n",
            "epoch: 7 step:   14/60, lr:0.000081, giou_loss:   3.52, conf_loss:  50.21, prob_loss:   5.86, total_loss:  59.60\n",
            "[[  87   99 1444 1419    0]]\n",
            "[[ 68   8 255 210   2]]\n",
            "[[ 45   1 130  95   1]\n",
            " [ 62  30 195 110   1]\n",
            " [  1  27  47  99   1]\n",
            " [ 17   7  77  91   1]]\n",
            "[[ 336  116 1442 1143    0]]\n",
            "epoch: 7 step:   15/60, lr:0.000081, giou_loss:   2.36, conf_loss:  48.00, prob_loss:   3.16, total_loss:  53.53\n",
            "[[ 85 137 488 545   2]]\n",
            "[[ 76 120 213 279   2]\n",
            " [ 67  45 203 184   2]\n",
            " [410 115 542 260   2]\n",
            " [435  50 584 180   2]\n",
            " [303  72 446 216   2]\n",
            " [228 124 379 266   2]\n",
            " [164  29 297 155   2]]\n",
            "[[ 766  351 1144  762    2]\n",
            " [ 118   30  840  458    1]\n",
            " [ 140  162  810  780    1]]\n",
            "[[184  71 382 262   2]\n",
            " [ 49  48 218 221   2]]\n",
            "epoch: 7 step:   16/60, lr:0.000081, giou_loss:   5.94, conf_loss:  52.83, prob_loss:  11.48, total_loss:  70.25\n",
            "[[ 346  188 1151  401    1]]\n",
            "[[ 63  70 296 289   2]]\n",
            "[[110  20 569 270   1]]\n",
            "[[278 436 561 706   0]\n",
            " [405 139 653 392   0]\n",
            " [655 197 969 464   0]\n",
            " [526  15 778 238   0]\n",
            " [238  33 503 267   0]]\n",
            "epoch: 7 step:   17/60, lr:0.000080, giou_loss:   3.52, conf_loss:  50.49, prob_loss:   3.29, total_loss:  57.31\n",
            "[[110 187 280 365   0]\n",
            " [ 42 135 208 319   0]]\n",
            "[[329  15 785 384   2]]\n",
            "[[ 62  15 381 127   1]]\n",
            "[[ 69  83 238 191   1]]\n",
            "epoch: 7 step:   18/60, lr:0.000080, giou_loss:   2.04, conf_loss:  47.71, prob_loss:   0.72, total_loss:  50.46\n",
            "[[ 207  161  729  495    1]\n",
            " [  30    1  571  298    1]\n",
            " [ 787    3 1198  344    1]\n",
            " [ 939  249 1198  669    1]\n",
            " [ 489  449  970  674    1]\n",
            " [   1  480  377  674    1]]\n",
            "[[ 225  190 1196  974    1]\n",
            " [ 538  249 1162  848    0]]\n",
            "[[ 439    3 1397  743    2]\n",
            " [ 271  645 1069 1068    2]]\n",
            "[[ 92 201 259 378   0]\n",
            " [226 193 403 350   0]\n",
            " [166 102 345 271   0]]\n",
            "epoch: 7 step:   19/60, lr:0.000080, giou_loss:   5.63, conf_loss:  53.15, prob_loss:   8.42, total_loss:  67.19\n",
            "[[141  35 315 210   0]\n",
            " [ 18  22 168 190   0]]\n",
            "[[ 24  28 128 135   2]]\n",
            "[[ 54  71 474 513   0]]\n",
            "[[324  48 662 409   2]\n",
            " [ 14   8 366 371   2]]\n",
            "epoch: 7 step:   20/60, lr:0.000080, giou_loss:   1.23, conf_loss:  45.74, prob_loss:   4.15, total_loss:  51.12\n",
            "[[153  53 375 272   2]\n",
            " [ 18  38 218 260   2]]\n",
            "[[182  27 451 291   0]]\n",
            "[[ 822  381 1448 1519    1]]\n",
            "[[ 47  52 379 381   2]]\n",
            "epoch: 7 step:   21/60, lr:0.000080, giou_loss:   1.67, conf_loss:  47.17, prob_loss:   5.44, total_loss:  54.27\n",
            "[[220  52 517 339   0]]\n",
            "[[ 88   8 684 675   1]]\n",
            "[[143   4 454 275   1]\n",
            " [219  64 480 352   1]]\n",
            "[[349 227 861 784   0]]\n",
            "epoch: 7 step:   22/60, lr:0.000080, giou_loss:   1.67, conf_loss:  45.71, prob_loss:   1.91, total_loss:  49.29\n",
            "[[139 313 435 631   0]]\n",
            "[[ 267  208 1107 1044    2]]\n",
            "[[  7  19 320 145   1]\n",
            " [159  38 382 253   1]\n",
            " [ 44  47 336 196   1]]\n",
            "[[ 33  21 229 204   0]]\n",
            "epoch: 7 step:   23/60, lr:0.000080, giou_loss:   1.59, conf_loss:  45.54, prob_loss:   0.93, total_loss:  48.06\n",
            "[[120  10 332 215   2]]\n",
            "[[224 137 433 358   2]\n",
            " [ 26  99 231 351   2]\n",
            " [136   1 355 171   2]]\n",
            "[[ 63  81 682 695   0]]\n",
            "[[ 77  10 684 613   0]]\n",
            "epoch: 7 step:   24/60, lr:0.000080, giou_loss:   1.48, conf_loss:  45.73, prob_loss:   2.55, total_loss:  49.76\n",
            "[[241 179 430 373   0]\n",
            " [313  92 478 277   0]\n",
            " [173 103 342 285   0]]\n",
            "[[ 50  67 729 815   0]]\n",
            "[[149 138 537 313   1]]\n",
            "[[184 314 542 672   2]\n",
            " [120 292 717 749   1]]\n",
            "epoch: 7 step:   25/60, lr:0.000079, giou_loss:   2.40, conf_loss:  46.36, prob_loss:   3.79, total_loss:  52.54\n",
            "[[ 59  33 461 122   1]\n",
            " [ 66  41 462 193   1]\n",
            " [ 75  77 472 286   1]\n",
            " [198  97 549 368   1]]\n",
            "[[  6  44 410 188   1]]\n",
            "[[205  42 448 265   2]]\n",
            "[[ 20   9 269 241   2]]\n",
            "epoch: 7 step:   26/60, lr:0.000079, giou_loss:   1.84, conf_loss:  45.56, prob_loss:   3.86, total_loss:  51.26\n",
            "[[306 124 618 412   2]\n",
            " [ 61 276 886 472   1]\n",
            " [ 33 395 876 603   1]]\n",
            "[[ 538    1 1238  708    0]]\n",
            "[[562 218 924 583   0]]\n",
            "[[ 159  233 1013 1111    0]]\n",
            "epoch: 7 step:   27/60, lr:0.000079, giou_loss:   1.93, conf_loss:  45.26, prob_loss:   3.44, total_loss:  50.62\n",
            "[[118  72 631 584   2]]\n",
            "[[ 387    1  810  427    2]\n",
            " [  75   85  555  569    0]\n",
            " [ 185  173 1074  722    1]]\n",
            "[[  1  83 672 340   1]]\n",
            "[[ 436  278 1039  889    2]]\n",
            "epoch: 7 step:   28/60, lr:0.000079, giou_loss:   1.63, conf_loss:  44.44, prob_loss:   1.29, total_loss:  47.35\n",
            "[[ 94 181 285 370   2]\n",
            " [ 12 173 172 351   2]]\n",
            "[[326  52 467 293   1]\n",
            " [251  41 335 302   1]\n",
            " [172  58 245 297   1]\n",
            " [ 74  65 169 302   1]]\n",
            "[[253  29 502 281   0]\n",
            " [  5  14 285 287   0]]\n",
            "[[ 81 157 612 662   2]]\n",
            "epoch: 7 step:   29/60, lr:0.000079, giou_loss:   4.99, conf_loss:  48.18, prob_loss:  12.28, total_loss:  65.45\n",
            "[[137   9 580 404   1]]\n",
            "[[102 128 521 526   2]\n",
            " [522  54 777 295   2]]\n",
            "[[ 39  55 445 474   0]]\n",
            "[[  38    3 1852  926    1]]\n",
            "epoch: 7 step:   30/60, lr:0.000079, giou_loss:   1.10, conf_loss:  44.86, prob_loss:   1.97, total_loss:  47.93\n",
            "[[ 78   2 385 300   2]]\n",
            "[[317 156 541 387   1]\n",
            " [ 48 182 421 368   1]\n",
            " [ 81 107 456 255   1]]\n",
            "[[ 59 148 747 668   1]\n",
            " [485 148 914 668   1]]\n",
            "[[ 10 136 429 330   1]\n",
            " [101  73 423 210   1]\n",
            " [107  18 382 153   1]]\n",
            "epoch: 7 step:   31/60, lr:0.000079, giou_loss:   3.20, conf_loss:  46.89, prob_loss:   7.18, total_loss:  57.27\n",
            "[[132  14 333 219   2]]\n",
            "[[156  10 582 341   1]]\n",
            "[[471 330 781 646   2]\n",
            " [ 14 193 620 410   1]\n",
            " [ 20 251 670 506   1]\n",
            " [ 68  28 330 321   0]]\n",
            "[[135  19 465 351   2]]\n",
            "epoch: 7 step:   32/60, lr:0.000079, giou_loss:   2.18, conf_loss:  45.12, prob_loss:   4.19, total_loss:  51.49\n",
            "[[  5  57 499 304   1]\n",
            " [ 28  65 254 291   1]]\n",
            "[[ 465   74  885  450    2]\n",
            " [  65  327 1012  629    1]\n",
            " [ 140  115  509  427    0]]\n",
            "[[ 33  31 420 224   1]\n",
            " [504 103 653 265   1]\n",
            " [ 24 150 566 339   1]]\n",
            "[[193  19 335 164   0]\n",
            " [123 142 245 263   0]\n",
            " [130 258 282 382   0]\n",
            " [297 128 463 285   0]\n",
            " [ 56  48 155 138   0]\n",
            " [ 13 195  87 270   0]\n",
            " [  1  24  62  85   0]\n",
            " [  4  99  69 166   0]]\n",
            "epoch: 7 step:   33/60, lr:0.000079, giou_loss:   8.72, conf_loss:  53.82, prob_loss:  13.94, total_loss:  76.49\n",
            "[[ 50  22 191 179   0]\n",
            " [190  17 292 126   0]\n",
            " [141  85 252 194   0]]\n",
            "[[ 82 104 251 259   2]\n",
            " [201  96 361 259   2]\n",
            " [263  49 405 190   2]]\n",
            "[[190  12 422 182   1]\n",
            " [170  43 355 216   1]\n",
            " [140  56 290 246   1]\n",
            " [ 86  75 172 290   1]]\n",
            "[[ 188  177 1761  800    1]]\n",
            "epoch: 7 step:   34/60, lr:0.000078, giou_loss:   3.71, conf_loss:  47.53, prob_loss:   5.62, total_loss:  56.86\n",
            "[[ 16  11 170 156   2]]\n",
            "[[ 21  16 273 275   2]]\n",
            "[[ 19  21 337 325   1]]\n",
            "[[108 215 575 490   1]\n",
            " [132 154 565 384   1]]\n",
            "epoch: 7 step:   35/60, lr:0.000078, giou_loss:   1.20, conf_loss:  43.77, prob_loss:   1.82, total_loss:  46.79\n",
            "[[ 661  111 1310  815    0]]\n",
            "[[  3 164 182 334   2]]\n",
            "[[  1  17 148 161   0]]\n",
            "[[ 18  15 341 349   0]]\n",
            "epoch: 7 step:   36/60, lr:0.000078, giou_loss:   0.89, conf_loss:  42.46, prob_loss:   2.11, total_loss:  45.46\n",
            "[[ 23  15 191 185   0]]\n",
            "[[  25   72 1892 1892    2]]\n",
            "[[228  40 596 429   2]]\n",
            "[[ 63  44 319 300   0]]\n",
            "epoch: 7 step:   37/60, lr:0.000078, giou_loss:   0.98, conf_loss:  42.84, prob_loss:   2.33, total_loss:  46.16\n",
            "[[  92  640 2548 4530    1]]\n",
            "[[313 153 478 348   2]\n",
            " [127 148 320 371   0]\n",
            " [ 73 366 618 619   1]]\n",
            "[[ 349   97  772  504    2]\n",
            " [ 816  194 1200  621    2]\n",
            " [ 760  580 1240 1012    2]]\n",
            "[[503 145 897 526   2]\n",
            " [ 20 131 421 517   0]\n",
            " [124 571 805 933   1]]\n",
            "epoch: 7 step:   38/60, lr:0.000078, giou_loss:   5.02, conf_loss:  46.91, prob_loss:   6.43, total_loss:  58.37\n",
            "[[  9  19 257 275   2]]\n",
            "[[  6  18 206 206   0]]\n",
            "[[ 99  83 458 416   1]\n",
            " [186  39 624 382   1]\n",
            " [205   8 670 226   1]]\n",
            "[[733 247 997 499   2]\n",
            " [ 20 268 291 535   2]\n",
            " [228 213 481 440   2]\n",
            " [507 247 741 455   2]]\n",
            "epoch: 7 step:   39/60, lr:0.000078, giou_loss:   3.71, conf_loss:  47.32, prob_loss:   6.00, total_loss:  57.03\n",
            "[[  74  155 1581  931    1]]\n",
            "[[  14  129 2746 2612    1]]\n",
            "[[ 22  47 358 410   2]]\n",
            "[[  4  61 187 251   2]]\n",
            "epoch: 7 step:   40/60, lr:0.000078, giou_loss:   1.22, conf_loss:  42.84, prob_loss:   3.61, total_loss:  47.67\n",
            "[[ 77 180 440 669   1]]\n",
            "[[ 26  31 136 141   0]\n",
            " [103   4 204  99   0]\n",
            " [208  17 312 128   0]]\n",
            "[[193  33 439 258   0]\n",
            " [464  30 651 280   0]\n",
            " [315   5 536 220   0]]\n",
            "[[ 29  42 499 484   0]]\n",
            "epoch: 7 step:   41/60, lr:0.000078, giou_loss:   3.15, conf_loss:  45.67, prob_loss:   9.79, total_loss:  58.61\n",
            "[[ 97  27 989 514   1]]\n",
            "[[ 117  121  851  842    2]\n",
            " [ 697   45 1385  707    2]]\n",
            "[[ 15 231 499 707   0]\n",
            " [298  29 665 502   0]]\n",
            "[[ 98  59 291 261   0]]\n",
            "epoch: 7 step:   42/60, lr:0.000077, giou_loss:   1.60, conf_loss:  43.03, prob_loss:   2.97, total_loss:  47.61\n",
            "[[ 328   32 1207  918    2]]\n",
            "[[ 597   75 1175  587    2]]\n",
            "[[ 63  17 343 318   2]]\n",
            "[[ 26 340 318 456   1]]\n",
            "epoch: 7 step:   43/60, lr:0.000077, giou_loss:   1.34, conf_loss:  42.70, prob_loss:   2.54, total_loss:  46.59\n",
            "[[  6   1 235 225   2]]\n",
            "[[246 136 469 350   2]\n",
            " [ 41 128 268 339   0]\n",
            " [221 163 577 360   1]]\n",
            "[[200 158 282 246   2]\n",
            " [142 155 215 230   2]\n",
            " [ 12  41 242 137   1]\n",
            " [  1 176  95 267   0]]\n",
            "[[165  52 389 294   2]]\n",
            "epoch: 7 step:   44/60, lr:0.000077, giou_loss:   3.04, conf_loss:  44.96, prob_loss:   9.74, total_loss:  57.75\n",
            "[[ 86 378 988 856   1]]\n",
            "[[ 46  48 195 263   1]]\n",
            "[[ 40 267 155 366   2]\n",
            " [450 245 530 320   2]\n",
            " [424 188 522 265   2]\n",
            " [ 54 155 311 255   1]\n",
            " [ 92 197 326 292   1]\n",
            " [228 154 385 299   1]\n",
            " [256 204 392 333   1]\n",
            " [233  79 342 182   0]\n",
            " [343 112 457 217   0]]\n",
            "[[ 643  364 1396 1046    0]]\n",
            "epoch: 7 step:   45/60, lr:0.000077, giou_loss:   7.26, conf_loss:  49.59, prob_loss:  10.40, total_loss:  67.25\n",
            "[[ 110  730  913 1551    0]\n",
            " [   1    1  407  617    0]\n",
            " [ 104    4  624  557    0]]\n",
            "[[135 146 514 495   0]]\n",
            "[[  77  230  841  572    1]\n",
            " [ 363  211 1013  754    1]\n",
            " [  37  187  793  485    1]]\n",
            "[[  4   5 632 179   1]\n",
            " [ 59  93 646 403   1]\n",
            " [218   1 659 295   1]]\n",
            "epoch: 7 step:   46/60, lr:0.000077, giou_loss:   2.93, conf_loss:  44.57, prob_loss:   4.66, total_loss:  52.16\n",
            "[[ 46  13 253 221   0]]\n",
            "[[ 23  22 208 114   1]\n",
            " [211  21 395 115   1]]\n",
            "[[611 234 775 384   2]\n",
            " [558  16 749 212   0]\n",
            " [442  11 568 398   1]\n",
            " [262 129 526 457   1]\n",
            " [361  10 515 387   1]]\n",
            "[[141  42 442 362   2]]\n",
            "epoch: 7 step:   47/60, lr:0.000077, giou_loss:   5.43, conf_loss:  46.34, prob_loss:   7.60, total_loss:  59.36\n",
            "[[ 87  27 689 480   1]\n",
            " [ 94   3 665 300   1]\n",
            " [117   7 628 216   1]\n",
            " [520 147 719 480   1]]\n",
            "[[103   2 765 705   2]]\n",
            "[[ 15 127 366 500   0]\n",
            " [332 151 739 582   0]]\n",
            "[[ 67  59 570 305   1]]\n",
            "epoch: 7 step:   48/60, lr:0.000077, giou_loss:   1.77, conf_loss:  42.86, prob_loss:   3.20, total_loss:  47.83\n",
            "[[  3  10 426 339   1]]\n",
            "[[202  37 470 314   0]]\n",
            "[[ 535   61 1268  836    2]\n",
            " [   3   24  696  756    2]]\n",
            "[[  2   9 284 254   0]]\n",
            "epoch: 7 step:   49/60, lr:0.000077, giou_loss:   0.96, conf_loss:  41.67, prob_loss:   1.25, total_loss:  43.88\n",
            "[[142  39 388 300   2]]\n",
            "[[ 360  175 1467  930    1]\n",
            " [ 493  594 1764 1275    1]\n",
            " [ 191    8 1166  618    1]\n",
            " [  47    8  911  557    1]]\n",
            "[[ 38  44 325 330   0]]\n",
            "[[ 30  67 207 234   0]]\n",
            "epoch: 7 step:   50/60, lr:0.000076, giou_loss:   1.63, conf_loss:  42.61, prob_loss:   2.82, total_loss:  47.06\n",
            "[[  8  43 105 148   2]]\n",
            "[[ 27  24 389 351   2]]\n",
            "[[ 79 157 405 432   0]]\n",
            "[[189   7 335 150   2]\n",
            " [ 40 149 346 231   1]\n",
            " [ 48   2 185 147   0]]\n",
            "epoch: 7 step:   51/60, lr:0.000076, giou_loss:   2.08, conf_loss:  40.89, prob_loss:   1.64, total_loss:  44.61\n",
            "[[ 48  59 286 337   2]\n",
            " [308  49 564 320   2]]\n",
            "[[  7  21 346 223   1]]\n",
            "[[  13  464  968 1100    1]\n",
            " [  13  668  794 1176    1]\n",
            " [ 332  179 1512 1052    1]]\n",
            "[[ 20  76 345 405   0]\n",
            " [290  43 561 334   0]]\n",
            "epoch: 7 step:   52/60, lr:0.000076, giou_loss:   2.46, conf_loss:  41.96, prob_loss:   3.37, total_loss:  47.79\n",
            "[[391  48 847 534   2]]\n",
            "[[ 22  56 476 512   2]]\n",
            "[[398 196 703 502   0]\n",
            " [ 88 361 376 626   0]\n",
            " [177 225 460 480   0]]\n",
            "[[273  46 455 257   0]]\n",
            "epoch: 7 step:   53/60, lr:0.000076, giou_loss:   2.30, conf_loss:  41.32, prob_loss:   3.33, total_loss:  46.95\n",
            "[[ 41  15 281 259   2]\n",
            " [290  10 544 277   2]]\n",
            "[[218 163 546 487   0]\n",
            " [265 448 583 713   0]\n",
            " [  0 470 197 716   0]\n",
            " [ 73 179 332 467   0]\n",
            " [492  63 799 414   0]]\n",
            "[[307  37 570 325   0]\n",
            " [ 67   4 326 256   0]]\n",
            "[[ 55  20 234 192   0]]\n",
            "epoch: 7 step:   54/60, lr:0.000076, giou_loss:   3.97, conf_loss:  44.12, prob_loss:   4.78, total_loss:  52.87\n",
            "[[ 358  352 1759 1743    2]]\n",
            "[[ 504  208 1266  943    2]]\n",
            "[[1238  149 2703 1723    2]]\n",
            "[[   9  570  554 1087    2]]\n",
            "epoch: 7 step:   55/60, lr:0.000076, giou_loss:   1.09, conf_loss:  40.06, prob_loss:   2.16, total_loss:  43.32\n",
            "[[170  76 412 331   2]\n",
            " [389 141 615 397   0]\n",
            " [  3 221 576 521   1]]\n",
            "[[133 173 556 595   0]\n",
            " [401 115 725 441   0]]\n",
            "[[  22  547  829 1486    1]\n",
            " [ 184  496 1149 1174    1]\n",
            " [ 296  371 1194  861    1]\n",
            " [ 250  184 1031  763    1]]\n",
            "[[  6 110 563 344   1]]\n",
            "epoch: 7 step:   56/60, lr:0.000076, giou_loss:   3.71, conf_loss:  44.17, prob_loss:   5.47, total_loss:  53.35\n",
            "[[144  82 696 356   1]]\n",
            "[[146   6 650 533   0]]\n",
            "[[368  73 626 321   2]\n",
            " [ 26  35 325 334   2]]\n",
            "[[ 52 257 285 524   0]\n",
            " [222 345 461 569   0]\n",
            " [586 239 790 471   0]\n",
            " [377 191 607 406   0]\n",
            " [165 176 358 327   0]\n",
            " [267  22 499 216   0]\n",
            " [100  55 265 220   0]]\n",
            "epoch: 7 step:   57/60, lr:0.000076, giou_loss:   3.93, conf_loss:  46.04, prob_loss:   5.22, total_loss:  55.19\n",
            "[[ 16  87 256 291   0]]\n",
            "[[102  28 272 316   1]\n",
            " [316  51 428 367   1]\n",
            " [348  11 499 268   1]\n",
            " [220  64 331 381   1]]\n",
            "[[180  50 496 343   1]\n",
            " [ 31  42 403 217   1]\n",
            " [ 50  38 416 297   1]\n",
            " [ 41  44 384 248   1]]\n",
            "[[ 26   7 419 409   2]]\n",
            "epoch: 7 step:   58/60, lr:0.000075, giou_loss:   4.85, conf_loss:  44.37, prob_loss:   4.53, total_loss:  53.75\n",
            "[[  8  15 130 138   2]\n",
            " [ 95  13 231 157   2]]\n",
            "[[116 104 600 394   1]]\n",
            "[[ 10   6 591 411   1]\n",
            " [ 23 137 598 620   1]]\n",
            "[[ 283  189 1262 1184    0]]\n",
            "epoch: 7 step:   59/60, lr:0.000075, giou_loss:   1.24, conf_loss:  40.66, prob_loss:   1.91, total_loss:  43.82\n",
            "[[ 99  29 497 444   0]]\n",
            "[[ 38  17 258 218   0]\n",
            " [  3 209 221 445   0]\n",
            " [151 169 350 403   0]\n",
            " [573 290 755 505   0]\n",
            " [466 362 643 559   0]\n",
            " [301 231 542 455   0]\n",
            " [159 342 341 535   0]\n",
            " [411 417 623 590   0]]\n",
            "[[ 21  27 319 315   0]]\n",
            "[[ 86  19 519 344   1]\n",
            " [234  34 608 396   1]\n",
            " [ 28   5 467 250   1]]\n",
            "epoch: 7 step:    0/60, lr:0.000075, giou_loss:   5.69, conf_loss:  48.54, prob_loss:  11.44, total_loss:  65.66\n",
            "[[114 289 290 459   0]\n",
            " [282 296 449 448   0]\n",
            " [ 57 284 211 424   0]\n",
            " [481 239 615 387   0]]\n",
            "[[ 16  34 266 289   0]]\n",
            "[[111  49 322 245   2]\n",
            " [ 64 288 273 506   2]\n",
            " [241 368 453 567   2]\n",
            " [176 453 378 623   2]]\n",
            "[[ 46  14 256 276   1]]\n",
            "epoch: 7 step:    1/60, lr:0.000075, giou_loss:   4.25, conf_loss:  46.42, prob_loss:   8.83, total_loss:  59.51\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[155 105 453 436   0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[143  40 415 346   0]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[166  92 638 559   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[145  36 421 286   2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[111  51 372 277   2]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[203 115 584 452   1]]\n",
            "\n",
            "\n",
            "giou_val_loss:   4.21, conf_val_loss:  61.67, prob_val_loss:  10.09, total_val_loss:  75.97\n",
            "\n",
            "\n",
            "[[202  61 445 284   2]]\n",
            "[[ 24  43 200 232   0]]\n",
            "[[218 239 442 481   2]]\n",
            "[[ 23  70 330 402   0]]\n",
            "epoch: 8 step:    2/60, lr:0.000075, giou_loss:   1.33, conf_loss:  39.58, prob_loss:   3.54, total_loss:  44.45\n",
            "[[  8  21 679 278   1]]\n",
            "[[140 156 316 326   0]\n",
            " [308 163 475 315   0]\n",
            " [ 83 151 237 291   0]\n",
            " [507 106 641 254   0]]\n",
            "[[   4    3 1818  926    1]]\n",
            "[[   8  571  553 1088    2]]\n",
            "epoch: 8 step:    3/60, lr:0.000075, giou_loss:   3.23, conf_loss:  42.81, prob_loss:   3.06, total_loss:  49.10\n",
            "[[ 580  160 1333  842    0]]\n",
            "[[205 196 375 374   0]\n",
            " [137 144 303 328   0]]\n",
            "[[ 378    3  801  429    2]\n",
            " [  66   87  546  571    0]\n",
            " [ 176  175 1065  724    1]]\n",
            "[[299 121 464 316   2]\n",
            " [113 116 306 339   0]\n",
            " [ 59 334 604 587   1]]\n",
            "epoch: 8 step:    4/60, lr:0.000075, giou_loss:   2.86, conf_loss:  41.92, prob_loss:   8.08, total_loss:  52.86\n",
            "[[389  11 740 384   0]\n",
            " [ 16  35 423 466   0]]\n",
            "[[ 199   73  963  415    1]\n",
            " [  27   54  677  597    1]\n",
            " [ 247   30 1003  328    1]]\n",
            "[[498   6 718 207   0]\n",
            " [535 198 753 434   0]\n",
            " [406 158 605 392   0]\n",
            " [  1 279 183 494   0]\n",
            " [113 351 290 548   0]\n",
            " [214 220 455 444   0]\n",
            " [415 331 597 524   0]\n",
            " [133 406 345 579   0]]\n",
            "[[ 55  29 947 516   1]]\n",
            "epoch: 8 step:    5/60, lr:0.000075, giou_loss:   5.17, conf_loss:  46.11, prob_loss:   5.24, total_loss:  56.53\n",
            "[[  3  38 253 293   0]]\n",
            "[[100 107 584 397   1]]\n",
            "[[428  58 884 427   2]]\n",
            "[[  8  92 300 208   1]]\n",
            "epoch: 8 step:    6/60, lr:0.000074, giou_loss:   0.82, conf_loss:  38.83, prob_loss:   0.70, total_loss:  40.34\n",
            "[[ 311  277 1073 1012    2]]\n",
            "[[ 94 181 285 370   2]\n",
            " [ 12 173 172 351   2]]\n",
            "[[ 31  26 633 479   1]\n",
            " [ 55   2 626 299   1]\n",
            " [ 92   6 603 215   1]\n",
            " [  1 146 200 479   1]]\n",
            "[[121 161 588 436   1]\n",
            " [145 100 578 330   1]]\n",
            "epoch: 8 step:    7/60, lr:0.000074, giou_loss:   2.73, conf_loss:  41.64, prob_loss:   6.65, total_loss:  51.02\n",
            "[[ 870  457 1496 1595    1]]\n",
            "[[165  43 388 257   2]\n",
            " [366  35 593 246   0]\n",
            " [ 57  70 413 267   1]]\n",
            "[[175  74 538 563   1]]\n",
            "[[117  32 779 735   2]]\n",
            "epoch: 8 step:    8/60, lr:0.000074, giou_loss:   2.65, conf_loss:  41.16, prob_loss:   8.73, total_loss:  52.54\n",
            "[[  63   27 1717  924    1]\n",
            " [ 484   76 1997 1144    1]\n",
            " [   7    1 1600  580    1]]\n",
            "[[ 14  40 301 326   0]]\n",
            "[[193  33 439 258   0]\n",
            " [464  30 651 280   0]\n",
            " [315   5 536 220   0]]\n",
            "[[485 316 795 632   2]\n",
            " [ 28 179 634 396   1]\n",
            " [ 34 237 684 492   1]\n",
            " [ 82  14 344 307   0]]\n",
            "epoch: 8 step:    9/60, lr:0.000074, giou_loss:   3.88, conf_loss:  42.89, prob_loss:   4.61, total_loss:  51.38\n",
            "[[232  96 465 315   2]]\n",
            "[[180  10 381 215   2]]\n",
            "[[197  53 382 145   1]\n",
            " [ 10  52 194 146   1]]\n",
            "[[ 59  72 287 305   2]]\n",
            "epoch: 8 step:   10/60, lr:0.000074, giou_loss:   2.18, conf_loss:  39.80, prob_loss:   6.00, total_loss:  47.98\n",
            "[[ 28  21 114 109   2]\n",
            " [  4   8  82  99   2]]\n",
            "[[106 156 586 360   1]]\n",
            "[[ 23  55 458 497   0]]\n",
            "[[  7  54 411 198   1]]\n",
            "epoch: 8 step:   11/60, lr:0.000074, giou_loss:   1.06, conf_loss:  38.62, prob_loss:   1.41, total_loss:  41.09\n",
            "[[ 14   9 243 233   2]]\n",
            "[[ 172   13 1214  504    1]\n",
            " [  56   16  999  543    1]]\n",
            "[[294 127 543 379   0]\n",
            " [ 46 112 326 385   0]]\n",
            "[[465 443 744 724   2]\n",
            " [274 286 584 544   2]\n",
            " [227 533 560 770   2]]\n",
            "epoch: 8 step:   12/60, lr:0.000074, giou_loss:   2.67, conf_loss:  41.58, prob_loss:   2.83, total_loss:  47.09\n",
            "[[ 650   99 1384  820    2]\n",
            " [ 116   23  804  685    2]]\n",
            "[[134  90 330 335   1]]\n",
            "[[ 55 112 277 331   2]\n",
            " [212  97 412 319   2]]\n",
            "[[  1  25 155 170   2]]\n",
            "epoch: 8 step:   13/60, lr:0.000074, giou_loss:   1.68, conf_loss:  39.26, prob_loss:   5.97, total_loss:  46.91\n",
            "[[  7 105 476 439   0]\n",
            " [305  40 777 439   0]]\n",
            "[[142  42 353 238   2]\n",
            " [191 281 400 499   2]\n",
            " [ 11 361 223 560   2]\n",
            " [ 86 446 288 616   2]]\n",
            "[[134  19 308 194   0]\n",
            " [ 11   6 161 174   0]]\n",
            "[[ 878  381 1256  792    2]\n",
            " [ 230   60  952  488    1]\n",
            " [ 252  192  922  810    1]]\n",
            "epoch: 8 step:   14/60, lr:0.000073, giou_loss:   3.91, conf_loss:  43.09, prob_loss:   5.50, total_loss:  52.50\n",
            "[[ 377  215 1217 1051    2]]\n",
            "[[ 59  81 518 331   1]]\n",
            "[[370  67 826 553   2]]\n",
            "[[ 290   52 1269 1047    0]]\n",
            "epoch: 8 step:   15/60, lr:0.000073, giou_loss:   0.93, conf_loss:  38.44, prob_loss:   1.80, total_loss:  41.17\n",
            "[[201  61 379 239   2]\n",
            " [  8  75 176 218   0]\n",
            " [ 67 292 379 416   1]]\n",
            "[[ 75 228 257 439   0]]\n",
            "[[241  52 438 822   1]]\n",
            "[[ 69  83 238 191   1]]\n",
            "epoch: 8 step:   16/60, lr:0.000073, giou_loss:   1.84, conf_loss:  38.84, prob_loss:   2.55, total_loss:  43.23\n",
            "[[ 18  15 341 349   0]]\n",
            "[[ 16   9 212 192   0]]\n",
            "[[356  17 906 569   0]\n",
            " [ 73  23 514 491   0]\n",
            " [  0  33 277 432   0]]\n",
            "[[351  76 575 307   1]\n",
            " [ 82 102 455 288   1]\n",
            " [115  27 490 175   1]]\n",
            "epoch: 8 step:   17/60, lr:0.000073, giou_loss:   2.70, conf_loss:  41.18, prob_loss:   6.81, total_loss:  50.70\n",
            "[[  48   33 2780 2516    1]]\n",
            "[[167 334 651 810   0]\n",
            " [  1 132 368 605   0]]\n",
            "[[  94  287 1049  923    1]\n",
            " [  94  491  875  999    1]\n",
            " [ 413    2 1593  875    1]]\n",
            "[[483  30 857 408   2]\n",
            " [ 58   1 715 215   1]\n",
            " [ 75 134 661 418   1]]\n",
            "epoch: 8 step:   18/60, lr:0.000073, giou_loss:   2.97, conf_loss:  40.28, prob_loss:   3.06, total_loss:  46.31\n",
            "[[104 273 219 372   2]\n",
            " [514 251 594 326   2]\n",
            " [488 194 586 271   2]\n",
            " [118 161 375 261   1]\n",
            " [156 203 390 298   1]\n",
            " [292 160 449 305   1]\n",
            " [320 210 456 339   1]\n",
            " [297  85 406 188   0]\n",
            " [407 118 521 223   0]]\n",
            "[[363  85 621 333   2]\n",
            " [ 21  47 320 346   2]]\n",
            "[[281 319 448 496   0]\n",
            " [137 311 314 468   0]\n",
            " [195 220 374 389   0]]\n",
            "[[ 33 213 459 427   1]]\n",
            "epoch: 8 step:   19/60, lr:0.000073, giou_loss:   8.75, conf_loss:  48.77, prob_loss:  13.61, total_loss:  71.14\n",
            "[[ 247    3 1560 1070    1]]\n",
            "[[ 95   9 565 451   0]]\n",
            "[[404   4 824 380   2]\n",
            " [  4 257 951 559   1]\n",
            " [ 79  45 448 357   0]]\n",
            "[[345 177 952 780   0]]\n",
            "epoch: 8 step:   20/60, lr:0.000073, giou_loss:   1.86, conf_loss:  39.25, prob_loss:   2.38, total_loss:  43.49\n",
            "[[ 743  282 1092  650    2]\n",
            " [ 414  319  843  670    2]\n",
            " [ 267  259  611  602    2]\n",
            " [ 427   43  867  401    2]]\n",
            "[[ 40  44 226 226   0]]\n",
            "[[ 11 265 390 614   0]]\n",
            "[[291 340 659 729   2]]\n",
            "epoch: 8 step:   21/60, lr:0.000073, giou_loss:   2.65, conf_loss:  40.17, prob_loss:   5.24, total_loss:  48.06\n",
            "[[ 304   21 1109  234    1]]\n",
            "[[ 19  13 317 301   0]]\n",
            "[[ 53  70 507 526   2]]\n",
            "[[130  29 272 174   0]\n",
            " [220 152 342 273   0]\n",
            " [183 268 335 392   0]\n",
            " [  2 138 168 295   0]\n",
            " [310  58 409 148   0]\n",
            " [378 205 452 280   0]\n",
            " [403  34 464  95   0]\n",
            " [396 109 461 176   0]]\n",
            "epoch: 8 step:   22/60, lr:0.000072, giou_loss:   6.31, conf_loss:  47.78, prob_loss:  12.45, total_loss:  66.55\n",
            "[[ 76   5 344 282   0]]\n",
            "[[218 163 546 487   0]\n",
            " [265 448 583 713   0]\n",
            " [  0 470 197 716   0]\n",
            " [ 73 179 332 467   0]\n",
            " [492  63 799 414   0]]\n",
            "[[ 39  26 218 196   2]]\n",
            "[[  78   63 1543 1637    2]]\n",
            "epoch: 8 step:   23/60, lr:0.000072, giou_loss:   2.48, conf_loss:  39.86, prob_loss:   2.60, total_loss:  44.95\n",
            "[[146   6 650 533   0]]\n",
            "[[506 262 670 412   2]\n",
            " [453  44 644 240   0]\n",
            " [337  39 463 426   1]\n",
            " [157 157 421 485   1]\n",
            " [256  38 410 415   1]]\n",
            "[[  5  39 253 295   2]]\n",
            "[[158  54 488 386   2]]\n",
            "epoch: 8 step:   24/60, lr:0.000072, giou_loss:   4.50, conf_loss:  41.68, prob_loss:   7.14, total_loss:  53.32\n",
            "[[267  58 408 299   1]\n",
            " [192  47 276 308   1]\n",
            " [113  64 186 303   1]\n",
            " [ 15  71 110 308   1]]\n",
            "[[  6  13 437 454   2]]\n",
            "[[  7  23 346 225   1]]\n",
            "[[ 89  69 601 626   0]]\n",
            "epoch: 8 step:   25/60, lr:0.000072, giou_loss:   4.78, conf_loss:  40.92, prob_loss:   8.76, total_loss:  54.47\n",
            "[[253  84 426 256   2]\n",
            " [ 25  90 310 373   1]\n",
            " [103   2 267 163   0]]\n",
            "[[  8  17 155 161   0]]\n",
            "[[ 41  44 315 311   2]]\n",
            "[[  48  162 1019  946    1]\n",
            " [ 361  221  985  820    0]]\n",
            "epoch: 8 step:   26/60, lr:0.000072, giou_loss:   1.79, conf_loss:  38.89, prob_loss:   5.45, total_loss:  46.13\n",
            "[[ 219  730 1022 1551    0]\n",
            " [ 725    1 1131  617    0]\n",
            " [ 508    4 1028  557    0]]\n",
            "[[ 74  22 436 349   2]]\n",
            "[[ 17  55 453 292   1]]\n",
            "[[147 218 336 412   0]\n",
            " [ 99 131 264 316   0]\n",
            " [235 142 404 324   0]]\n",
            "epoch: 8 step:   27/60, lr:0.000072, giou_loss:   2.56, conf_loss:  38.95, prob_loss:   2.54, total_loss:  44.05\n",
            "[[ 76   2 474 417   0]]\n",
            "[[  87   99 1444 1419    0]]\n",
            "[[ 13 106 632 720   0]]\n",
            "[[ 32 108 563 613   2]]\n",
            "epoch: 8 step:   28/60, lr:0.000072, giou_loss:   0.87, conf_loss:  37.21, prob_loss:   1.91, total_loss:  40.00\n",
            "[[ 45  52 213 222   0]]\n",
            "[[ 375  368 1024 1072    0]]\n",
            "[[ 889  147 1461 1147    1]]\n",
            "[[145  14 242 119   2]]\n",
            "epoch: 8 step:   29/60, lr:0.000072, giou_loss:   1.25, conf_loss:  36.94, prob_loss:   1.65, total_loss:  39.84\n",
            "[[  7  65 190 255   2]]\n",
            "[[ 58 137 267 358   2]\n",
            " [260  99 465 351   2]\n",
            " [136   1 355 171   2]]\n",
            "[[   0   67 1867 1887    2]]\n",
            "[[415 261 720 567   0]\n",
            " [105 426 393 691   0]\n",
            " [194 290 477 545   0]]\n",
            "epoch: 8 step:   30/60, lr:0.000071, giou_loss:   2.47, conf_loss:  39.72, prob_loss:   7.36, total_loss:  49.55\n",
            "[[ 41 260 943 738   1]]\n",
            "[[116   1 479 348   0]]\n",
            "[[143  12 566 341   1]]\n",
            "[[135  51 568 376   1]\n",
            " [283  66 657 428   1]\n",
            " [ 77  37 516 282   1]]\n",
            "epoch: 8 step:   31/60, lr:0.000071, giou_loss:   1.89, conf_loss:  37.12, prob_loss:   3.90, total_loss:  42.92\n",
            "[[ 74 178 468 559   2]\n",
            " [550 164 951 550   0]\n",
            " [166 604 847 966   1]]\n",
            "[[ 979  334 1596  954    2]\n",
            " [ 433  422 1021 1026    2]]\n",
            "[[222   8 486 280   2]]\n",
            "[[ 48  14 235 216   2]]\n",
            "epoch: 8 step:   32/60, lr:0.000071, giou_loss:   2.29, conf_loss:  38.25, prob_loss:   4.53, total_loss:  45.07\n",
            "[[230  48 782 322   1]]\n",
            "[[  9  59 637 233   1]\n",
            " [ 64 147 651 457   1]\n",
            " [223  55 664 349   1]]\n",
            "[[ 50  98 248 289   2]\n",
            " [214  75 383 248   2]]\n",
            "[[ 82  12 395 138   1]\n",
            " [ 20  31 243 246   1]\n",
            " [ 66  40 358 189   1]]\n",
            "epoch: 8 step:   33/60, lr:0.000071, giou_loss:   3.04, conf_loss:  39.32, prob_loss:   4.71, total_loss:  47.08\n",
            "[[148 109 651 355   1]]\n",
            "[[ 16   8 171 155   0]]\n",
            "[[105  25 385 326   2]]\n",
            "[[102   4 224 127   2]\n",
            " [  1   2 137 146   2]]\n",
            "epoch: 8 step:   34/60, lr:0.000071, giou_loss:   1.17, conf_loss:  36.16, prob_loss:   3.21, total_loss:  40.54\n",
            "[[344  76 640 394   0]]\n",
            "[[  4  29 183 201   0]]\n",
            "[[ 68  19 369 339   2]]\n",
            "[[ 23  17 275 276   2]]\n",
            "epoch: 8 step:   35/60, lr:0.000071, giou_loss:   0.93, conf_loss:  36.34, prob_loss:   1.79, total_loss:  39.06\n",
            "[[ 12  81 182 369   1]\n",
            " [226 104 338 420   1]\n",
            " [258  64 409 321   1]\n",
            " [130 117 241 434   1]]\n",
            "[[ 30  89 433 497   2]]\n",
            "[[147  19 473 294   0]]\n",
            "[[263  94 588 423   0]\n",
            " [ 47  61 318 352   0]]\n",
            "epoch: 8 step:   36/60, lr:0.000071, giou_loss:   4.02, conf_loss:  39.88, prob_loss:   4.97, total_loss:  48.86\n",
            "[[ 50  19 475 428   2]]\n",
            "[[122  31 334 236   2]]\n",
            "[[  5  31 287 276   0]]\n",
            "[[ 31   8 241 270   1]]\n",
            "epoch: 8 step:   37/60, lr:0.000071, giou_loss:   0.72, conf_loss:  35.87, prob_loss:   1.44, total_loss:  38.03\n",
            "[[ 45   1 130  95   1]\n",
            " [ 62  30 195 110   1]\n",
            " [  1  27  47  99   1]\n",
            " [ 17   7  77  91   1]]\n",
            "[[ 158  116 1264 1143    0]]\n",
            "[[  31  291  295  543    2]\n",
            " [ 737  312 1008  579    2]\n",
            " [ 547  257  800  484    2]\n",
            " [ 287  291  521  499    2]]\n",
            "[[ 48  53 255 261   0]]\n",
            "epoch: 8 step:   38/60, lr:0.000070, giou_loss:   4.30, conf_loss:  41.48, prob_loss:   9.60, total_loss:  55.37\n",
            "[[363  57 605 312   2]\n",
            " [160 122 386 378   0]\n",
            " [199 202 772 502   1]]\n",
            "[[ 65  52 311 313   2]]\n",
            "[[ 65 127 753 647   1]\n",
            " [491 127 920 647   1]]\n",
            "[[  9  11 186 178   0]]\n",
            "epoch: 8 step:   39/60, lr:0.000070, giou_loss:   2.02, conf_loss:  37.40, prob_loss:   3.53, total_loss:  42.95\n",
            "[[ 36  47 352 340   1]\n",
            " [129  39 501 214   1]\n",
            " [116  35 482 294   1]\n",
            " [148  41 491 245   1]]\n",
            "[[   8   58  741  833    2]\n",
            " [ 580   21 1273  753    2]]\n",
            "[[ 619    4 1577  744    2]\n",
            " [ 451  646 1249 1069    2]]\n",
            "[[160  37 496 400   2]]\n",
            "epoch: 8 step:   40/60, lr:0.000070, giou_loss:   2.63, conf_loss:  39.09, prob_loss:   3.85, total_loss:  45.57\n",
            "[[173  73 566 475   2]]\n",
            "[[  9 122 428 316   1]\n",
            " [100  59 422 196   1]\n",
            " [106   4 381 139   1]]\n",
            "[[ 19  21 337 325   1]]\n",
            "[[ 317  168 1424  923    1]\n",
            " [ 450  587 1721 1268    1]\n",
            " [ 148    1 1123  611    1]\n",
            " [   4    1  868  550    1]]\n",
            "epoch: 8 step:   41/60, lr:0.000070, giou_loss:   2.89, conf_loss:  38.15, prob_loss:   8.25, total_loss:  49.29\n",
            "[[580 427 863 697   0]\n",
            " [488 130 736 383   0]\n",
            " [172 188 486 455   0]\n",
            " [363   6 615 229   0]\n",
            " [638  24 903 258   0]]\n",
            "[[190 198 613 620   0]\n",
            " [ 21 140 345 466   0]]\n",
            "[[  28  198  882 1076    0]]\n",
            "[[103 104 224 247   2]\n",
            " [ 97   1 203 115   2]\n",
            " [  1  62 131 187   2]\n",
            " [  1   2 111  70   2]\n",
            " [ 30 201 152 250   2]]\n",
            "epoch: 8 step:   42/60, lr:0.000070, giou_loss:   5.51, conf_loss:  42.09, prob_loss:   8.35, total_loss:  55.95\n",
            "[[ 10  28 266 284   0]]\n",
            "[[ 207  160  729  494    1]\n",
            " [  30    0  571  297    1]\n",
            " [ 787    2 1198  343    1]\n",
            " [ 939  248 1198  668    1]\n",
            " [ 489  448  970  673    1]\n",
            " [   1  479  377  673    1]]\n",
            "[[392 193 956 773   0]\n",
            " [ 73 156 585 645   0]]\n",
            "[[373 181 685 469   2]\n",
            " [105 333 930 529   1]\n",
            " [115 452 958 660   1]]\n",
            "epoch: 8 step:   43/60, lr:0.000070, giou_loss:   5.52, conf_loss:  41.07, prob_loss:  11.13, total_loss:  57.71\n",
            "[[ 353  305 1232 1191    2]]\n",
            "[[  1  47 294 163   1]]\n",
            "[[ 53  94 222 249   2]\n",
            " [172  86 332 249   2]\n",
            " [234  39 376 180   2]]\n",
            "[[408 119 545 278   2]\n",
            " [418  44 554 183   2]\n",
            " [ 79 114 211 259   2]\n",
            " [ 37  49 186 179   2]\n",
            " [175  71 318 215   2]\n",
            " [242 123 393 265   2]\n",
            " [324  28 457 154   2]]\n",
            "epoch: 8 step:   44/60, lr:0.000070, giou_loss:   6.34, conf_loss:  42.66, prob_loss:  12.05, total_loss:  61.05\n",
            "[[362 248 698 618   2]\n",
            " [ 15 237 344 559   2]]\n",
            "[[190  12 422 182   1]\n",
            " [170  43 355 216   1]\n",
            " [140  56 290 246   1]\n",
            " [ 86  75 172 290   1]]\n",
            "[[ 81  63 524 458   1]]\n",
            "[[206 168 288 256   2]\n",
            " [148 165 221 240   2]\n",
            " [ 18  51 248 147   1]\n",
            " [  7 186 101 277   0]]\n",
            "epoch: 8 step:   45/60, lr:0.000069, giou_loss:   4.72, conf_loss:  40.62, prob_loss:   7.40, total_loss:  52.73\n",
            "[[ 92  13 269 176   0]]\n",
            "[[259  16 587 318   1]\n",
            " [143  93 541 350   1]\n",
            " [215  10 527 205   1]]\n",
            "[[ 88 120 767 868   0]]\n",
            "[[189  99 445 329   2]\n",
            " [361   1 576 187   2]\n",
            " [  0  47 199 297   2]\n",
            " [183  25 395 209   2]\n",
            " [ 34 272 298 398   2]]\n",
            "epoch: 8 step:   46/60, lr:0.000069, giou_loss:   3.46, conf_loss:  38.66, prob_loss:   6.19, total_loss:  48.30\n",
            "[[ 34  80 591 314   1]]\n",
            "[[ 12  15 373 306   2]]\n",
            "[[434  36 899 492   2]]\n",
            "[[255  22 642 215   1]\n",
            " [ 22  94 171 256   1]\n",
            " [109 141 651 330   1]]\n",
            "epoch: 8 step:   47/60, lr:0.000069, giou_loss:   2.42, conf_loss:  36.88, prob_loss:   4.85, total_loss:  44.15\n",
            "[[319  51 582 339   0]\n",
            " [ 79  18 338 270   0]]\n",
            "[[199  67 309 177   0]\n",
            " [131  40 232 135   0]\n",
            " [ 23  53 127 164   0]]\n",
            "[[ 15  81 255 325   2]\n",
            " [264  76 518 343   2]]\n",
            "[[134  41 372 319   2]\n",
            " [394  31 650 302   2]]\n",
            "epoch: 8 step:   48/60, lr:0.000069, giou_loss:   2.57, conf_loss:  37.60, prob_loss:   4.29, total_loss:  44.46\n",
            "[[139 125 441 439   0]\n",
            " [559 211 838 464   0]\n",
            " [252 272 529 514   0]\n",
            " [699 185 939 453   0]]\n",
            "[[439 130 777 491   2]\n",
            " [129  90 481 453   2]]\n",
            "[[  3  45 408 387   1]]\n",
            "[[232  28 810 540   2]]\n",
            "epoch: 8 step:   49/60, lr:0.000069, giou_loss:   2.51, conf_loss:  38.26, prob_loss:   2.57, total_loss:  43.34\n",
            "[[ 15  87 255 291   0]]\n",
            "[[204 154 624 596   0]]\n",
            "[[ 94 270 358 583   2]\n",
            " [ 85 118 405 373   0]\n",
            " [160  54 709 356   1]]\n",
            "[[113  91 515 180   1]\n",
            " [112  99 508 251   1]\n",
            " [102 135 499 344   1]\n",
            " [ 25 155 376 426   1]]\n",
            "epoch: 8 step:   50/60, lr:0.000069, giou_loss:   3.28, conf_loss:  38.45, prob_loss:   2.99, total_loss:  44.71\n",
            "[[103  57 699 724   1]]\n",
            "[[ 28  29 541 541   2]]\n",
            "[[ 92  35 594 238   1]]\n",
            "[[ 71  39 497 370   1]]\n",
            "epoch: 8 step:   51/60, lr:0.000069, giou_loss:   1.05, conf_loss:  35.54, prob_loss:   1.45, total_loss:  38.04\n",
            "[[163   7 309 150   2]\n",
            " [ 14 149 320 231   1]\n",
            " [ 22   2 159 147   0]]\n",
            "[[433 161 552 276   0]\n",
            " [281 181 412 306   0]\n",
            " [250  64 356 159   0]\n",
            " [154   1 289 105   0]\n",
            " [ 79  46 200 162   0]\n",
            " [182 104 296 211   0]]\n",
            "[[  12    1 1585  624    1]]\n",
            "[[  3   3 203 191   0]]\n",
            "epoch: 8 step:   52/60, lr:0.000069, giou_loss:   5.97, conf_loss:  41.67, prob_loss:  11.48, total_loss:  59.12\n",
            "[[ 99  83 458 416   1]\n",
            " [186  39 624 382   1]\n",
            " [205   8 670 226   1]]\n",
            "[[ 99  15 368 279   0]]\n",
            "[[  3   7 196 209   0]]\n",
            "[[ 321  354  955 1354    1]\n",
            " [  14  175  539 1233    1]]\n",
            "epoch: 8 step:   53/60, lr:0.000068, giou_loss:   1.91, conf_loss:  35.84, prob_loss:   4.43, total_loss:  42.18\n",
            "[[ 175  368 1576 1759    2]]\n",
            "[[ 79 196 512 625   0]]\n",
            "[[155 115 574 513   2]\n",
            " [575  41 830 282   2]]\n",
            "[[ 38 263 271 530   0]\n",
            " [208 351 447 575   0]\n",
            " [572 245 776 477   0]\n",
            " [363 197 593 412   0]\n",
            " [151 182 344 333   0]\n",
            " [253  28 485 222   0]\n",
            " [ 86  61 251 226   0]]\n",
            "epoch: 8 step:   54/60, lr:0.000068, giou_loss:   4.26, conf_loss:  40.79, prob_loss:   7.67, total_loss:  52.73\n",
            "[[ 13   2 262 234   2]]\n",
            "[[ 21   8 340 120   1]]\n",
            "[[ 969  133 1392  540    2]\n",
            " [ 541  230  925  657    2]\n",
            " [ 501  616  981 1048    2]]\n",
            "[[  27  575  834 1514    1]\n",
            " [ 189  524 1154 1202    1]\n",
            " [ 301  399 1199  889    1]\n",
            " [ 255  212 1036  791    1]]\n",
            "epoch: 8 step:   55/60, lr:0.000068, giou_loss:   3.45, conf_loss:  38.40, prob_loss:   7.09, total_loss:  48.94\n",
            "[[ 38  26 444 445   0]]\n",
            "[[ 69  43 218 258   1]]\n",
            "[[ 517  127 1120  738    2]]\n",
            "[[  14   93 1194  559    1]]\n",
            "epoch: 8 step:   56/60, lr:0.000068, giou_loss:   1.28, conf_loss:  33.73, prob_loss:   0.42, total_loss:  35.43\n",
            "[[291  55 588 342   0]]\n",
            "[[ 75  21 563 328   1]]\n",
            "[[ 31  82 157 222   0]]\n",
            "[[141 155 499 513   2]\n",
            " [ 77 133 674 590   1]]\n",
            "epoch: 8 step:   57/60, lr:0.000068, giou_loss:   1.51, conf_loss:  34.72, prob_loss:   1.81, total_loss:  38.05\n",
            "[[168   4 868 711   0]]\n",
            "[[ 193  207 1700  983    1]]\n",
            "[[116  43 257 200   0]\n",
            " [ 15  38 117 147   0]\n",
            " [ 55 106 166 215   0]]\n",
            "[[ 428  133 1075  497    1]]\n",
            "epoch: 8 step:   58/60, lr:0.000068, giou_loss:   1.95, conf_loss:  35.03, prob_loss:   5.72, total_loss:  42.70\n",
            "[[  41   66 2497 3956    1]]\n",
            "[[255   1 562 299   2]]\n",
            "[[141  29 452 300   1]\n",
            " [217  89 478 377   1]]\n",
            "[[177  27 701 385   1]]\n",
            "epoch: 8 step:   59/60, lr:0.000068, giou_loss:   1.59, conf_loss:  34.27, prob_loss:   1.77, total_loss:  37.63\n",
            "[[514   5 810 277   1]\n",
            " [ 53  35 321 378   1]\n",
            " [136  53 425 397   1]\n",
            " [165 133 425 527   1]\n",
            " [379 120 549 546   1]\n",
            " [445 119 736 454   1]\n",
            " [399  34 772 315   1]]\n",
            "[[ 43  18 375 347   2]]\n",
            "[[431 101 793 466   0]]\n",
            "[[  1  41 389 216   1]]\n",
            "epoch: 8 step:    0/60, lr:0.000067, giou_loss:   4.77, conf_loss:  38.48, prob_loss:   7.07, total_loss:  50.33\n",
            "[[ 63  40 644 445   1]\n",
            " [ 76 171 651 654   1]]\n",
            "[[ 10  83 114 190   2]]\n",
            "[[179  38 414 282   0]]\n",
            "[[ 82  13 576 260   1]\n",
            " [105  21 331 247   1]]\n",
            "epoch: 8 step:    1/60, lr:0.000067, giou_loss:   1.87, conf_loss:  34.88, prob_loss:   5.57, total_loss:  42.32\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[279  14 562 295   2]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[111  51 372 277   2]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[145  36 421 286   2]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "\n",
            "\n",
            "giou_val_loss:   3.72, conf_val_loss:  44.88, prob_val_loss:  11.83, total_val_loss:  60.42\n",
            "\n",
            "\n",
            "[[376  37 727 410   0]\n",
            " [  3  61 410 492   0]]\n",
            "[[ 37  52 205 222   0]]\n",
            "[[  73   12 1044  796    1]\n",
            " [ 107   71  731  670    0]]\n",
            "[[ 15   7 283 284   0]]\n",
            "epoch: 9 step:    2/60, lr:0.000067, giou_loss:   1.29, conf_loss:  34.62, prob_loss:   5.15, total_loss:  41.06\n",
            "[[154 278 321 455   0]\n",
            " [ 10 270 187 427   0]\n",
            " [ 68 179 247 348   0]]\n",
            "[[ 11  15 431 457   0]]\n",
            "[[  5 175 431 389   1]]\n",
            "[[231 297 776 814   2]]\n",
            "epoch: 9 step:    3/60, lr:0.000067, giou_loss:   2.31, conf_loss:  35.14, prob_loss:   1.59, total_loss:  39.05\n",
            "[[172  25 414 280   2]\n",
            " [391  90 617 346   0]\n",
            " [  5 170 578 470   1]]\n",
            "[[ 14  10 714 717   0]]\n",
            "[[131  31 529 446   0]]\n",
            "[[  10    7 1190  473    1]]\n",
            "epoch: 9 step:    4/60, lr:0.000067, giou_loss:   1.73, conf_loss:  35.08, prob_loss:   3.39, total_loss:  40.20\n",
            "[[ 15  18 244 242   2]]\n",
            "[[ 18  12 316 300   0]]\n",
            "[[   4  323  382  734    2]\n",
            " [ 308    2 1030  430    1]\n",
            " [ 338  134 1008  752    1]]\n",
            "[[115  14 599 304   1]]\n",
            "epoch: 9 step:    5/60, lr:0.000067, giou_loss:   1.63, conf_loss:  34.11, prob_loss:   1.51, total_loss:  37.25\n",
            "[[ 65  52 311 313   2]]\n",
            "[[ 374  801 1181 1740    1]\n",
            " [  54  750 1019 1428    1]\n",
            " [   9  625  907 1115    1]\n",
            " [ 172  438  953 1017    1]]\n",
            "[[ 45  57 214 212   2]\n",
            " [164  49 324 212   2]\n",
            " [226   2 368 143   2]]\n",
            "[[ 24 144 273 396   0]\n",
            " [241 129 521 402   0]]\n",
            "epoch: 9 step:    6/60, lr:0.000067, giou_loss:   3.44, conf_loss:  35.67, prob_loss:   3.49, total_loss:  42.61\n",
            "[[248 104 573 433   0]\n",
            " [ 32  71 303 362   0]]\n",
            "[[ 44   2 223 172   2]]\n",
            "[[167 218 651 694   0]\n",
            " [  1  16 368 489   0]]\n",
            "[[ 210  222  836 1360    1]]\n",
            "epoch: 9 step:    7/60, lr:0.000066, giou_loss:   1.84, conf_loss:  35.53, prob_loss:   2.19, total_loss:  39.57\n",
            "[[ 207  161  729  495    1]\n",
            " [  30    1  571  298    1]\n",
            " [ 787    3 1198  344    1]\n",
            " [ 939  249 1198  669    1]\n",
            " [ 489  449  970  674    1]\n",
            " [   1  480  377  674    1]]\n",
            "[[ 423   34  846  460    2]\n",
            " [ 678  118 1158  602    0]\n",
            " [ 159  206 1048  755    1]]\n",
            "[[291  43 649 401   2]\n",
            " [116  21 713 478   1]]\n",
            "[[  11  462  745 1183    2]\n",
            " [ 591  386 1279 1048    2]]\n",
            "epoch: 9 step:    8/60, lr:0.000066, giou_loss:   5.51, conf_loss:  39.52, prob_loss:  11.35, total_loss:  56.38\n",
            "[[  5  13 337 342   2]]\n",
            "[[  1 103 122 246   2]\n",
            " [ 22   0 128 114   2]\n",
            " [ 94  61 224 186   2]\n",
            " [114   1 224  69   2]\n",
            " [ 73 200 195 249   2]]\n",
            "[[132  98 591 348   1]]\n",
            "[[222   8 486 280   2]]\n",
            "epoch: 9 step:    9/60, lr:0.000066, giou_loss:   2.38, conf_loss:  34.88, prob_loss:   6.95, total_loss:  44.21\n",
            "[[ 62  42 231 150   1]]\n",
            "[[ 74 178 468 559   2]\n",
            " [550 164 951 550   0]\n",
            " [166 604 847 966   1]]\n",
            "[[  87   99 1444 1419    0]]\n",
            "[[121  19 783 722   2]]\n",
            "epoch: 9 step:   10/60, lr:0.000066, giou_loss:   1.14, conf_loss:  33.42, prob_loss:   1.23, total_loss:  35.79\n",
            "[[148 109 651 355   1]]\n",
            "[[351  80 663 368   2]\n",
            " [106 232 931 428   1]\n",
            " [ 78 351 921 559   1]]\n",
            "[[140  83 980 919   2]]\n",
            "[[102   5 224 128   2]\n",
            " [  1   3 137 147   2]]\n",
            "epoch: 9 step:   11/60, lr:0.000066, giou_loss:   2.39, conf_loss:  34.75, prob_loss:   3.92, total_loss:  41.06\n",
            "[[548   3 844 275   1]\n",
            " [ 87  33 355 376   1]\n",
            " [170  51 459 395   1]\n",
            " [199 131 459 525   1]\n",
            " [413 118 583 544   1]\n",
            " [479 117 770 452   1]\n",
            " [433  32 806 313   1]]\n",
            "[[ 36  22 212 211   0]]\n",
            "[[  2  48 295 164   1]]\n",
            "[[241  46 437 291   1]]\n",
            "epoch: 9 step:   12/60, lr:0.000066, giou_loss:   4.50, conf_loss:  36.95, prob_loss:  11.12, total_loss:  52.57\n",
            "[[ 54  66 508 522   2]]\n",
            "[[ 20  32 269 264   2]]\n",
            "[[ 81  25 683 478   1]\n",
            " [105   1 676 298   1]\n",
            " [142   5 653 214   1]\n",
            " [ 51 145 250 478   1]]\n",
            "[[ 12  52 252 256   0]]\n",
            "epoch: 9 step:   13/60, lr:0.000066, giou_loss:   1.77, conf_loss:  34.45, prob_loss:   2.15, total_loss:  38.37\n",
            "[[ 14  11 337 345   0]]\n",
            "[[ 27  72 521 319   1]\n",
            " [272  80 498 306   1]]\n",
            "[[1036  325 2501 1899    2]]\n",
            "[[437  55 857 431   2]\n",
            " [ 37 308 984 610   1]\n",
            " [112  96 481 408   0]]\n",
            "epoch: 9 step:   14/60, lr:0.000066, giou_loss:   2.30, conf_loss:  34.29, prob_loss:   5.08, total_loss:  41.67\n",
            "[[174 101 576 190   1]\n",
            " [173 109 569 261   1]\n",
            " [163 145 560 354   1]\n",
            " [ 86 165 437 436   1]]\n",
            "[[ 65 250 439 628   2]\n",
            " [207 221 864 435   1]\n",
            " [261 354 847 638   1]]\n",
            "[[ 61   6 240 178   0]]\n",
            "[[  42   81 1148 1108    0]]\n",
            "epoch: 9 step:   15/60, lr:0.000065, giou_loss:   2.71, conf_loss:  34.51, prob_loss:   2.43, total_loss:  39.65\n",
            "[[ 27  35 420 437   2]]\n",
            "[[ 53   6 179 146   0]]\n",
            "[[150  99 406 329   2]\n",
            " [ 19   1 234 187   2]\n",
            " [396  47 595 297   2]\n",
            " [200  25 412 209   2]\n",
            " [297 272 561 398   2]]\n",
            "[[ 85 109 423 470   2]\n",
            " [381  69 733 432   2]]\n",
            "epoch: 9 step:   16/60, lr:0.000065, giou_loss:   2.44, conf_loss:  34.59, prob_loss:   9.13, total_loss:  46.16\n",
            "[[402 163 640 441   2]\n",
            " [124 153 380 424   2]]\n",
            "[[176  38 374 229   2]\n",
            " [ 41  15 210 188   2]]\n",
            "[[ 56 116 526 558   0]]\n",
            "[[ 17  49 194 212   0]]\n",
            "epoch: 9 step:   17/60, lr:0.000065, giou_loss:   1.86, conf_loss:  32.55, prob_loss:   1.88, total_loss:  36.28\n",
            "[[ 37  37 178 278   1]\n",
            " [169  26 253 287   1]\n",
            " [259  43 332 282   1]\n",
            " [335  50 430 287   1]]\n",
            "[[  2 136  84 224   2]\n",
            " [ 69 133 142 208   2]\n",
            " [ 42  19 272 115   1]\n",
            " [189 154 283 245   0]]\n",
            "[[  2 283 755 965   0]]\n",
            "[[ 30  36 127 141   2]]\n",
            "epoch: 9 step:   18/60, lr:0.000065, giou_loss:   6.00, conf_loss:  37.99, prob_loss:   8.28, total_loss:  52.26\n",
            "[[ 542   47 1275  822    2]\n",
            " [  10   10  703  742    2]]\n",
            "[[ 253  183 1015  918    2]]\n",
            "[[ 25  42 275 297   0]]\n",
            "[[ 37 127 633 794   1]]\n",
            "epoch: 9 step:   19/60, lr:0.000065, giou_loss:   0.96, conf_loss:  32.56, prob_loss:   2.26, total_loss:  35.78\n",
            "[[122 231 298 401   0]\n",
            " [290 238 457 390   0]\n",
            " [ 65 226 219 366   0]\n",
            " [489 181 623 329   0]]\n",
            "[[  8 154 427 348   1]\n",
            " [ 14  91 336 228   1]\n",
            " [ 55  36 330 171   1]]\n",
            "[[262 231 485 445   2]\n",
            " [463 223 690 434   0]\n",
            " [154 258 510 455   1]]\n",
            "[[213  31 459 256   0]\n",
            " [  1  28 188 278   0]\n",
            " [116   3 337 218   0]]\n",
            "epoch: 9 step:   20/60, lr:0.000065, giou_loss:   6.76, conf_loss:  41.42, prob_loss:  10.34, total_loss:  58.51\n",
            "[[ 11  43 592 448   1]\n",
            " [ 24 174 599 657   1]]\n",
            "[[ 79  19 290 215   2]\n",
            " [ 32 258 241 476   2]\n",
            " [209 338 421 537   2]\n",
            " [144 423 346 593   2]]\n",
            "[[ 20  48 333 174   1]\n",
            " [172  67 395 282   1]\n",
            " [ 57  76 349 225   1]]\n",
            "[[ 75  21 563 328   1]]\n",
            "epoch: 9 step:   21/60, lr:0.000065, giou_loss:   4.11, conf_loss:  35.86, prob_loss:   3.37, total_loss:  43.33\n",
            "[[ 73  99 210 258   2]\n",
            " [ 64  24 200 163   2]\n",
            " [407  94 539 239   2]\n",
            " [432  29 581 159   2]\n",
            " [300  51 443 195   2]\n",
            " [225 103 376 245   2]\n",
            " [161   8 294 134   2]]\n",
            "[[  1  67 202 272   2]]\n",
            "[[  1  20 208 228   0]]\n",
            "[[ 37 109 440 517   2]]\n",
            "epoch: 9 step:   22/60, lr:0.000064, giou_loss:   5.02, conf_loss:  37.64, prob_loss:   9.51, total_loss:  52.17\n",
            "[[ 25  40 917 527   1]]\n",
            "[[253 163 581 487   0]\n",
            " [216 448 534 713   0]\n",
            " [602 470 799 716   0]\n",
            " [467 179 726 467   0]\n",
            " [  0  63 307 414   0]]\n",
            "[[  6  94 475 428   0]\n",
            " [304  29 776 428   0]]\n",
            "[[  1   1 283 246   0]]\n",
            "epoch: 9 step:   23/60, lr:0.000064, giou_loss:   2.48, conf_loss:  34.78, prob_loss:   6.62, total_loss:  43.87\n",
            "[[120  45 481 336   2]]\n",
            "[[471 436 754 706   0]\n",
            " [379 139 627 392   0]\n",
            " [ 63 197 377 464   0]\n",
            " [254  15 506 238   0]\n",
            " [529  33 794 267   0]]\n",
            "[[ 19 100 571 374   1]]\n",
            "[[ 383  239 1032  943    0]]\n",
            "epoch: 9 step:   24/60, lr:0.000064, giou_loss:   3.50, conf_loss:  35.84, prob_loss:   7.87, total_loss:  47.21\n",
            "[[ 544   67 1223  815    0]]\n",
            "[[1075   84 1647 1084    1]]\n",
            "[[  1  11 249 267   2]]\n",
            "[[  0  14 296 332   0]]\n",
            "epoch: 9 step:   25/60, lr:0.000064, giou_loss:   1.36, conf_loss:  32.56, prob_loss:   1.67, total_loss:  35.60\n",
            "[[148   2 652 529   0]]\n",
            "[[434  36 899 492   2]]\n",
            "[[ 430  329 1033  940    2]]\n",
            "[[ 91  92 710 706   0]]\n",
            "epoch: 9 step:   26/60, lr:0.000064, giou_loss:   0.81, conf_loss:  31.09, prob_loss:   0.55, total_loss:  32.44\n",
            "[[ 62  77 245 267   2]]\n",
            "[[ 69   2 224 149   0]]\n",
            "[[108  77 755 441   1]]\n",
            "[[199  82 384 174   1]\n",
            " [ 12  81 196 175   1]]\n",
            "epoch: 9 step:   27/60, lr:0.000064, giou_loss:   2.11, conf_loss:  32.34, prob_loss:   2.14, total_loss:  36.59\n",
            "[[319 117 559 361   2]\n",
            " [ 56 112 310 379   2]]\n",
            "[[ 97 245 261 395   2]\n",
            " [123  27 314 223   0]\n",
            " [304  22 430 409   1]\n",
            " [346 140 610 468   1]\n",
            " [357  21 511 398   1]]\n",
            "[[ 63  44 319 300   0]]\n",
            "[[257  42 490 261   2]]\n",
            "epoch: 9 step:   28/60, lr:0.000064, giou_loss:   4.04, conf_loss:  34.06, prob_loss:   5.82, total_loss:  43.92\n",
            "[[  1  65 532 570   2]]\n",
            "[[210 137 419 358   2]\n",
            " [ 12  99 217 351   2]\n",
            " [122   1 341 171   2]]\n",
            "[[356  33 906 585   0]\n",
            " [ 73  39 514 507   0]\n",
            " [  0  49 277 448   0]]\n",
            "[[ 188  266 1230  757    1]\n",
            " [  72  269 1015  796    1]]\n",
            "epoch: 9 step:   29/60, lr:0.000063, giou_loss:   2.42, conf_loss:  33.35, prob_loss:   4.05, total_loss:  39.82\n",
            "[[  6  19 147 176   0]\n",
            " [146  14 248 123   0]\n",
            " [ 97  82 208 191   0]]\n",
            "[[ 26  69 604 581   2]]\n",
            "[[  1 236 434 665   0]]\n",
            "[[ 80  36 387 368   0]]\n",
            "epoch: 9 step:   30/60, lr:0.000063, giou_loss:   1.38, conf_loss:  32.27, prob_loss:   2.44, total_loss:  36.08\n",
            "[[  72  352 2528 4242    1]]\n",
            "[[ 79   6 289 268   1]]\n",
            "[[178  40 601 369   1]]\n",
            "[[  2  96 407 438   1]]\n",
            "epoch: 9 step:   31/60, lr:0.000063, giou_loss:   1.00, conf_loss:  33.48, prob_loss:   1.59, total_loss:  36.08\n",
            "[[ 18 177 530 734   0]]\n",
            "[[  75  241 1054 1236    0]]\n",
            "[[133 197 322 391   0]\n",
            " [205 110 370 295   0]\n",
            " [ 65 121 234 303   0]]\n",
            "[[  48  845  851 1666    0]\n",
            " [ 554  116  960  732    0]\n",
            " [ 337  119  857  672    0]]\n",
            "epoch: 9 step:   32/60, lr:0.000063, giou_loss:   2.70, conf_loss:  35.12, prob_loss:   5.80, total_loss:  43.61\n",
            "[[ 19  12 173 157   2]]\n",
            "[[ 17  57 421 201   1]]\n",
            "[[166   9 312 152   2]\n",
            " [ 17 151 323 233   1]\n",
            " [ 25   4 162 149   0]]\n",
            "[[140  27 409 291   0]]\n",
            "epoch: 9 step:   33/60, lr:0.000063, giou_loss:   1.66, conf_loss:  31.27, prob_loss:   1.25, total_loss:  34.18\n",
            "[[ 30  67 207 234   0]]\n",
            "[[  3  51 409 470   0]]\n",
            "[[ 47  19 478 460   2]]\n",
            "[[109  48 633 406   1]]\n",
            "epoch: 9 step:   34/60, lr:0.000063, giou_loss:   0.61, conf_loss:  30.68, prob_loss:   0.73, total_loss:  32.02\n",
            "[[ 46 130 382 493   2]]\n",
            "[[ 51  25 303 284   2]]\n",
            "[[158  54 488 386   2]]\n",
            "[[145   1 425 302   2]]\n",
            "epoch: 9 step:   35/60, lr:0.000063, giou_loss:   0.87, conf_loss:  31.74, prob_loss:   7.28, total_loss:  39.88\n",
            "[[197  72 339 217   0]\n",
            " [127 195 249 316   0]\n",
            " [134 311 286 435   0]\n",
            " [301 181 467 338   0]\n",
            " [ 60 101 159 191   0]\n",
            " [ 17 248  91 323   0]\n",
            " [  5  77  66 138   0]\n",
            " [  8 152  73 219   0]]\n",
            "[[180 161 647 436   1]\n",
            " [190 100 623 330   1]]\n",
            "[[ 20  56 124 163   2]]\n",
            "[[ 55 104 165 214   0]\n",
            " [132  77 233 172   0]\n",
            " [237  90 341 201   0]]\n",
            "epoch: 9 step:   36/60, lr:0.000062, giou_loss:   8.17, conf_loss:  41.39, prob_loss:   8.15, total_loss:  57.72\n",
            "[[ 317  168 1424  923    1]\n",
            " [ 450  587 1721 1268    1]\n",
            " [ 148    1 1123  611    1]\n",
            " [   4    1  868  550    1]]\n",
            "[[466 324 776 640   2]\n",
            " [  9 187 615 404   1]\n",
            " [ 15 245 665 500   1]\n",
            " [ 63  22 325 315   0]]\n",
            "[[181 155 945 497   1]\n",
            " [  9 136 659 679   1]\n",
            " [229 112 985 410   1]]\n",
            "[[ 289  215 1943 1112    1]\n",
            " [   9  264 1522 1332    1]\n",
            " [ 406  189 1999  768    1]]\n",
            "epoch: 9 step:   37/60, lr:0.000062, giou_loss:   4.95, conf_loss:  34.88, prob_loss:  15.44, total_loss:  55.27\n",
            "[[114 105 284 393   1]\n",
            " [328 128 440 444   1]\n",
            " [360  88 511 345   1]\n",
            " [232 141 343 458   1]]\n",
            "[[ 91  45 303 250   2]]\n",
            "[[121 170 303 381   0]]\n",
            "[[ 138    5 1451 1072    1]]\n",
            "epoch: 9 step:   38/60, lr:0.000062, giou_loss:   4.08, conf_loss:  34.78, prob_loss:   6.24, total_loss:  45.10\n",
            "[[255   1 562 299   2]]\n",
            "[[544 255 777 522   0]\n",
            " [368 343 607 567   0]\n",
            " [ 39 237 243 469   0]\n",
            " [222 189 452 404   0]\n",
            " [471 174 664 325   0]\n",
            " [330  20 562 214   0]\n",
            " [564  53 729 218   0]]\n",
            "[[ 59  55 378 167   1]]\n",
            "[[ 458  347 1092 1347    1]\n",
            " [ 151  168  676 1226    1]]\n",
            "epoch: 9 step:   39/60, lr:0.000062, giou_loss:   5.07, conf_loss:  38.20, prob_loss:   6.26, total_loss:  49.53\n",
            "[[ 30  21 258 254   2]]\n",
            "[[  7  11 154 155   0]]\n",
            "[[ 50  61 228 239   2]\n",
            " [253  75 421 218   0]\n",
            " [ 50 292 362 416   1]]\n",
            "[[ 35 124 548 636   2]]\n",
            "epoch: 9 step:   40/60, lr:0.000062, giou_loss:   1.53, conf_loss:  31.75, prob_loss:   2.58, total_loss:  35.86\n",
            "[[ 12   8 244 178   1]\n",
            " [ 79  39 264 212   1]\n",
            " [144  52 294 242   1]\n",
            " [262  71 348 286   1]]\n",
            "[[ 45   1 130  95   1]\n",
            " [ 62  30 195 110   1]\n",
            " [  1  27  47  99   1]\n",
            " [ 17   7  77  91   1]]\n",
            "[[141 235 306 430   2]\n",
            " [299 230 492 453   0]\n",
            " [  1 448 546 701   1]]\n",
            "[[199  32 562 521   1]]\n",
            "epoch: 9 step:   41/60, lr:0.000062, giou_loss:   4.88, conf_loss:  37.06, prob_loss:  11.11, total_loss:  53.05\n",
            "[[  28   70 2760 2553    1]]\n",
            "[[236  51 669 376   1]\n",
            " [147  66 521 428   1]\n",
            " [288  37 727 282   1]]\n",
            "[[372 292 751 641   0]]\n",
            "[[ 71  26 507 263   1]]\n",
            "epoch: 9 step:   42/60, lr:0.000062, giou_loss:   1.78, conf_loss:  30.98, prob_loss:   7.93, total_loss:  40.68\n",
            "[[ 73  51 701 225   1]\n",
            " [ 59 139 646 449   1]\n",
            " [ 46  47 487 341   1]]\n",
            "[[ 220  207 1727  983    1]]\n",
            "[[ 42  21 477 463   0]]\n",
            "[[595 229 957 594   0]]\n",
            "epoch: 9 step:   43/60, lr:0.000061, giou_loss:   1.74, conf_loss:  30.63, prob_loss:   2.97, total_loss:  35.35\n",
            "[[ 31  57 217 239   0]]\n",
            "[[144  55 337 257   0]]\n",
            "[[ 24   5 335 276   1]\n",
            " [100  65 361 353   1]]\n",
            "[[249  62 672 484   0]\n",
            " [517   4 841 330   0]]\n",
            "epoch: 9 step:   44/60, lr:0.000061, giou_loss:   1.74, conf_loss:  30.27, prob_loss:   2.72, total_loss:  34.73\n",
            "[[358  29 814 398   2]]\n",
            "[[ 69  43 218 258   1]]\n",
            "[[ 446    1 1251  214    1]]\n",
            "[[222 230 501 511   2]\n",
            " [382  73 692 331   2]\n",
            " [406 320 739 557   2]]\n",
            "epoch: 9 step:   45/60, lr:0.000061, giou_loss:   2.40, conf_loss:  32.33, prob_loss:   2.80, total_loss:  37.52\n",
            "[[ 12  82 304 198   1]]\n",
            "[[302  45 565 333   0]\n",
            " [ 62  12 321 264   0]]\n",
            "[[ 80 261 385 567   0]\n",
            " [407 426 695 691   0]\n",
            " [323 290 606 545   0]]\n",
            "[[159  54 485 329   0]]\n",
            "epoch: 9 step:   46/60, lr:0.000061, giou_loss:   2.34, conf_loss:  31.33, prob_loss:   5.33, total_loss:  39.00\n",
            "[[205 196 375 374   0]\n",
            " [137 144 303 328   0]]\n",
            "[[332 291 700 680   2]]\n",
            "[[398  81 595 851   1]]\n",
            "[[  2  83 175 255   2]\n",
            " [118  89 403 372   1]\n",
            " [161   1 325 162   0]]\n",
            "epoch: 9 step:   47/60, lr:0.000061, giou_loss:   2.30, conf_loss:  31.03, prob_loss:   2.41, total_loss:  35.74\n",
            "[[366 202 930 782   0]\n",
            " [ 47 165 559 654   0]]\n",
            "[[ 20  28 294 295   2]]\n",
            "[[  1  37 288 323   0]]\n",
            "[[ 94  68 285 257   2]\n",
            " [ 12  60 172 238   2]]\n",
            "epoch: 9 step:   48/60, lr:0.000061, giou_loss:   1.28, conf_loss:  31.06, prob_loss:   4.13, total_loss:  36.47\n",
            "[[  79   86 1946 1906    2]]\n",
            "[[ 68 226 292 468   2]]\n",
            "[[ 11 425 913 903   1]]\n",
            "[[276  80 732 566   2]]\n",
            "epoch: 9 step:   49/60, lr:0.000061, giou_loss:   0.77, conf_loss:  29.73, prob_loss:   2.08, total_loss:  32.59\n",
            "[[ 60   9 448 184   1]]\n",
            "[[  3   3 365 330   2]]\n",
            "[[  37  174  916 1060    2]]\n",
            "[[  1  55 175 230   0]\n",
            " [148  42 298 210   0]]\n",
            "epoch: 9 step:   50/60, lr:0.000061, giou_loss:   1.08, conf_loss:  29.42, prob_loss:   1.60, total_loss:  32.10\n",
            "[[ 44 128 524 332   1]]\n",
            "[[  3  40 342 242   1]]\n",
            "[[ 10  13 197 215   2]]\n",
            "[[197  26 498 346   2]]\n",
            "epoch: 9 step:   51/60, lr:0.000060, giou_loss:   1.09, conf_loss:  29.75, prob_loss:   2.26, total_loss:  33.10\n",
            "[[   0  456  955 1092    1]\n",
            " [   0  660  781 1168    1]\n",
            " [ 319  171 1499 1044    1]]\n",
            "[[ 175  368 1576 1759    2]]\n",
            "[[ 57 161 176 276   0]\n",
            " [197 181 328 306   0]\n",
            " [253  64 359 159   0]\n",
            " [320   1 455 105   0]\n",
            " [409  46 530 162   0]\n",
            " [313 104 427 211   0]]\n",
            "[[  35  132 1608  755    1]]\n",
            "epoch: 9 step:   52/60, lr:0.000060, giou_loss:   5.53, conf_loss:  37.32, prob_loss:   6.98, total_loss:  49.83\n",
            "[[ 18  21 336 325   1]]\n",
            "[[ 36  38 271 282   0]]\n",
            "[[  5 158 562 392   1]]\n",
            "[[ 67  48 493 379   1]]\n",
            "epoch: 9 step:   53/60, lr:0.000060, giou_loss:   0.79, conf_loss:  29.29, prob_loss:   2.05, total_loss:  32.12\n",
            "[[ 13  80  99 168   2]\n",
            " [ 45  67 123 158   2]]\n",
            "[[ 10  63 338 365   1]\n",
            " [ 56 140 454 397   1]\n",
            " [ 70  57 382 252   1]]\n",
            "[[  1  16 201 204   0]]\n",
            "[[ 40  76 399 409   1]\n",
            " [127  32 565 375   1]\n",
            " [146   1 611 219   1]]\n",
            "epoch: 9 step:   54/60, lr:0.000060, giou_loss:   2.15, conf_loss:  31.45, prob_loss:   2.36, total_loss:  35.96\n",
            "[[ 54  29 297 252   2]]\n",
            "[[301  25 989 545   1]\n",
            " [134  25 563 545   1]]\n",
            "[[ 10   1 373 348   0]]\n",
            "[[ 48  73 550 276   1]]\n",
            "epoch: 9 step:   55/60, lr:0.000060, giou_loss:   1.23, conf_loss:  29.26, prob_loss:   2.89, total_loss:  33.38\n",
            "[[ 57  17 444 210   1]\n",
            " [528  89 677 251   1]\n",
            " [ 48 136 590 325   1]]\n",
            "[[ 168    1 1126  741    2]\n",
            " [   0  643  798 1066    2]]\n",
            "[[ 296  255  645  623    2]\n",
            " [ 545  292  974  643    2]\n",
            " [ 777  232 1121  575    2]\n",
            " [ 521   16  961  374    2]]\n",
            "[[ 81  63 524 458   1]]\n",
            "epoch: 9 step:   56/60, lr:0.000060, giou_loss:   3.94, conf_loss:  36.36, prob_loss:   9.46, total_loss:  49.76\n",
            "[[ 38   4 234 187   0]]\n",
            "[[ 826  237 1443  857    2]\n",
            " [ 280  325  868  929    2]]\n",
            "[[156  22 378 241   2]\n",
            " [ 21   7 221 229   2]]\n",
            "[[  5 200 269 452   2]\n",
            " [711 221 982 488   2]\n",
            " [521 166 774 393   2]\n",
            " [261 200 495 408   2]]\n",
            "epoch: 9 step:   57/60, lr:0.000060, giou_loss:   3.20, conf_loss:  32.35, prob_loss:   9.58, total_loss:  45.13\n",
            "[[ 42  27 339 314   0]]\n",
            "[[  3  45 319 338   1]\n",
            " [ 96  37 468 212   1]\n",
            " [ 83  33 449 292   1]\n",
            " [115  39 458 243   1]]\n",
            "[[386 244 650 557   2]\n",
            " [339  92 659 347   0]\n",
            " [ 35  28 584 330   1]]\n",
            "[[  92    3 1906  926    1]]\n",
            "epoch: 9 step:   58/60, lr:0.000059, giou_loss:   2.42, conf_loss:  31.55, prob_loss:   3.52, total_loss:  37.50\n",
            "[[100 167 402 481   0]\n",
            " [520 253 799 506   0]\n",
            " [213 314 490 556   0]\n",
            " [660 227 900 495   0]]\n",
            "[[347  73 605 321   2]\n",
            " [  5  35 304 334   2]]\n",
            "[[268 208 687 606   2]\n",
            " [ 12 134 267 375   2]]\n",
            "[[ 969  133 1392  540    2]\n",
            " [ 541  230  925  657    2]\n",
            " [ 501  616  981 1048    2]]\n",
            "epoch: 9 step:   59/60, lr:0.000059, giou_loss:   4.80, conf_loss:  35.37, prob_loss:  10.87, total_loss:  51.04\n",
            "[[ 47   3 267 204   0]\n",
            " [ 12 195 230 431   0]\n",
            " [160 155 359 389   0]\n",
            " [582 276 764 491   0]\n",
            " [475 348 652 545   0]\n",
            " [310 217 551 441   0]\n",
            " [168 328 350 521   0]\n",
            " [420 403 632 576   0]]\n",
            "[[317 156 541 387   1]\n",
            " [ 48 182 421 368   1]\n",
            " [ 81 107 456 255   1]]\n",
            "[[146  48 753 651   0]]\n",
            "[[381 244 496 343   2]\n",
            " [  6 222  86 297   2]\n",
            " [ 14 165 112 242   2]\n",
            " [225 132 482 232   1]\n",
            " [210 174 444 269   1]\n",
            " [151 131 308 276   1]\n",
            " [144 181 280 310   1]\n",
            " [194  56 303 159   0]\n",
            " [ 79  89 193 194   0]]\n",
            "epoch: 9 step:    0/60, lr:0.000059, giou_loss:  12.23, conf_loss:  46.51, prob_loss:  17.00, total_loss:  75.74\n",
            "[[  54  195  908 1073    0]]\n",
            "[[ 43 128 714 385   1]]\n",
            "[[548  69 884 439   2]\n",
            " [201  58 530 380   2]]\n",
            "[[ 755   37 1180  446    2]]\n",
            "epoch: 9 step:    1/60, lr:0.000059, giou_loss:   1.55, conf_loss:  29.81, prob_loss:   2.22, total_loss:  33.57\n",
            "[[227  62 723 500   0]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[155 105 453 436   0]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[133 165 922 525   1]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[279  14 562 295   2]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[203 115 584 452   1]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[145  36 421 286   2]]\n",
            "[[150 165 923 983   2]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[111  51 372 277   2]]\n",
            "\n",
            "\n",
            "giou_val_loss:   3.00, conf_val_loss:  36.61, prob_val_loss:   8.52, total_val_loss:  48.14\n",
            "\n",
            "\n",
            "[[391  57 742 430   0]\n",
            " [ 18  81 425 512   0]]\n",
            "[[ 77   1 312 245   0]]\n",
            "[[ 26   9 413 202   1]\n",
            " [497  81 646 243   1]\n",
            " [ 17 128 559 317   1]]\n",
            "[[157  42 298 199   0]\n",
            " [ 56  37 158 146   0]\n",
            " [ 96 105 207 214   0]]\n",
            "epoch:10 step:    2/60, lr:0.000059, giou_loss:   2.92, conf_loss:  31.98, prob_loss:   6.05, total_loss:  40.96\n",
            "[[113  75 420 407   0]]\n",
            "[[ 93  79 272 249   2]]\n",
            "[[118  21 620 224   1]]\n",
            "[[119  14 343 256   2]]\n",
            "epoch:10 step:    3/60, lr:0.000059, giou_loss:   0.96, conf_loss:  28.52, prob_loss:   0.86, total_loss:  30.35\n",
            "[[  2 161 121 276   0]\n",
            " [142 181 273 306   0]\n",
            " [198  64 304 159   0]\n",
            " [265   1 400 105   0]\n",
            " [354  46 475 162   0]\n",
            " [258 104 372 211   0]]\n",
            "[[ 55  77 238 267   2]]\n",
            "[[128  20 593 476   2]]\n",
            "[[  59   95 1713  992    1]\n",
            " [ 480  144 1993 1212    1]\n",
            " [   3   69 1596  648    1]]\n",
            "epoch:10 step:    4/60, lr:0.000059, giou_loss:   4.52, conf_loss:  35.85, prob_loss:   6.83, total_loss:  47.20\n",
            "[[137  26 557 402   2]\n",
            " [ 10 279 957 581   1]\n",
            " [513  67 882 379   0]]\n",
            "[[ 46 398 851 611   1]]\n",
            "[[   0  456  955 1092    1]\n",
            " [   0  660  781 1168    1]\n",
            " [ 319  171 1499 1044    1]]\n",
            "[[110 283 348 561   2]\n",
            " [370 273 626 544   2]]\n",
            "epoch:10 step:    5/60, lr:0.000058, giou_loss:   2.93, conf_loss:  30.60, prob_loss:   5.74, total_loss:  39.26\n",
            "[[ 87  27 689 480   1]\n",
            " [ 94   3 665 300   1]\n",
            " [117   7 628 216   1]\n",
            " [520 147 719 480   1]]\n",
            "[[ 203  148 1245  639    1]\n",
            " [  87  151 1030  678    1]]\n",
            "[[  2  14 284 259   0]]\n",
            "[[  6  49 394 224   1]]\n",
            "epoch:10 step:    6/60, lr:0.000058, giou_loss:   2.27, conf_loss:  31.14, prob_loss:   2.55, total_loss:  35.96\n",
            "[[ 35  89 365 421   2]]\n",
            "[[151  44 584 369   1]\n",
            " [ 62  59 436 421   1]\n",
            " [203  30 642 275   1]]\n",
            "[[210  31 666 517   2]]\n",
            "[[201  78 657 447   2]]\n",
            "epoch:10 step:    7/60, lr:0.000058, giou_loss:   1.60, conf_loss:  29.58, prob_loss:   7.64, total_loss:  38.82\n",
            "[[ 285    4 1598 1071    1]]\n",
            "[[162 153 327 348   2]\n",
            " [320 148 513 371   0]\n",
            " [ 22 366 567 619   1]]\n",
            "[[403 217 967 797   0]\n",
            " [ 84 180 596 669   0]]\n",
            "[[ 41  67 151 177   0]\n",
            " [118  40 219 135   0]\n",
            " [223  53 327 164   0]]\n",
            "epoch:10 step:    8/60, lr:0.000058, giou_loss:   3.28, conf_loss:  31.82, prob_loss:   4.99, total_loss:  40.09\n",
            "[[172   6 872 713   0]]\n",
            "[[501  11 721 212   0]\n",
            " [538 203 756 439   0]\n",
            " [409 163 608 397   0]\n",
            " [  4 284 186 499   0]\n",
            " [116 356 293 553   0]\n",
            " [217 225 458 449   0]\n",
            " [418 336 600 529   0]\n",
            " [136 411 348 584   0]]\n",
            "[[ 16  20 193 187   0]]\n",
            "[[   6   61 1873 1881    2]]\n",
            "epoch:10 step:    9/60, lr:0.000058, giou_loss:   3.83, conf_loss:  36.05, prob_loss:   6.32, total_loss:  46.20\n",
            "[[298  75 561 363   0]\n",
            " [ 58  42 317 294   0]]\n",
            "[[114  48 442 350   1]\n",
            " [160 125 558 382   1]\n",
            " [174  42 486 237   1]]\n",
            "[[ 165  264 1136 1048    1]\n",
            " [ 478  323 1102  922    0]]\n",
            "[[ 45   1 130  95   1]\n",
            " [ 62  30 195 110   1]\n",
            " [  1  27  47  99   1]\n",
            " [ 17   7  77  91   1]]\n",
            "epoch:10 step:   10/60, lr:0.000058, giou_loss:   3.44, conf_loss:  31.05, prob_loss:   8.19, total_loss:  42.68\n",
            "[[ 10   7 445 449   0]]\n",
            "[[  58   85 2790 2568    1]]\n",
            "[[ 158  116 1264 1143    0]]\n",
            "[[ 540  156 1273  931    2]\n",
            " [   8  119  701  851    2]]\n",
            "epoch:10 step:   11/60, lr:0.000058, giou_loss:   0.82, conf_loss:  28.74, prob_loss:   0.87, total_loss:  30.42\n",
            "[[ 45  52 213 222   0]]\n",
            "[[171 236 859 756   1]\n",
            " [  4 236 433 756   1]]\n",
            "[[398 250 662 563   2]\n",
            " [351  98 671 353   0]\n",
            " [ 47  34 596 336   1]]\n",
            "[[ 13 172 189 361   0]]\n",
            "epoch:10 step:   12/60, lr:0.000057, giou_loss:   2.09, conf_loss:  30.36, prob_loss:   3.64, total_loss:  36.09\n",
            "[[ 13   8 242 232   2]]\n",
            "[[ 458   60 1036  572    2]]\n",
            "[[ 89 110 313 341   1]\n",
            " [209 136 582 322   1]\n",
            " [174  61 549 209   1]]\n",
            "[[ 72 118 318 379   2]]\n",
            "epoch:10 step:   13/60, lr:0.000057, giou_loss:   1.69, conf_loss:  29.77, prob_loss:   2.44, total_loss:  33.90\n",
            "[[404  88 740 458   2]\n",
            " [ 57  77 386 399   2]]\n",
            "[[ 19   1 921 479   1]]\n",
            "[[226  14 533 312   2]]\n",
            "[[ 64  24 271 232   0]]\n",
            "epoch:10 step:   14/60, lr:0.000057, giou_loss:   1.45, conf_loss:  29.44, prob_loss:   3.23, total_loss:  34.12\n",
            "[[ 21  57 249 290   2]]\n",
            "[[ 38 160 595 394   1]]\n",
            "[[  51   37 1408 1357    0]]\n",
            "[[279 215 746 490   1]\n",
            " [289 154 722 384   1]]\n",
            "epoch:10 step:   15/60, lr:0.000057, giou_loss:   1.25, conf_loss:  28.43, prob_loss:   1.26, total_loss:  30.94\n",
            "[[515 257 748 524   0]\n",
            " [339 345 578 569   0]\n",
            " [ 10 239 214 471   0]\n",
            " [193 191 423 406   0]\n",
            " [442 176 635 327   0]\n",
            " [301  22 533 216   0]\n",
            " [535  55 700 220   0]]\n",
            "[[  8  61 347 263   1]]\n",
            "[[ 84  43 403 155   1]]\n",
            "[[202  61 445 284   2]]\n",
            "epoch:10 step:   16/60, lr:0.000057, giou_loss:   3.97, conf_loss:  33.79, prob_loss:   4.40, total_loss:  42.16\n",
            "[[ 34  95 359 424   0]\n",
            " [304  62 575 353   0]]\n",
            "[[ 47   1 334 287   0]]\n",
            "[[  4  92 409 434   1]]\n",
            "[[ 77 253 192 352   2]\n",
            " [487 231 567 306   2]\n",
            " [461 174 559 251   2]\n",
            " [ 91 141 348 241   1]\n",
            " [129 183 363 278   1]\n",
            " [265 140 422 285   1]\n",
            " [293 190 429 319   1]\n",
            " [270  65 379 168   0]\n",
            " [380  98 494 203   0]]\n",
            "epoch:10 step:   17/60, lr:0.000057, giou_loss:   7.15, conf_loss:  37.63, prob_loss:  11.88, total_loss:  56.65\n",
            "[[192  38 493 358   2]]\n",
            "[[ 80 133 442 498   0]]\n",
            "[[ 22   3 340 307   1]]\n",
            "[[  9 312 319 628   2]\n",
            " [170 175 776 392   1]\n",
            " [120 233 770 488   1]\n",
            " [460  10 722 303   0]]\n",
            "epoch:10 step:   18/60, lr:0.000057, giou_loss:   2.03, conf_loss:  29.05, prob_loss:   1.57, total_loss:  32.65\n",
            "[[  81   82 1588  858    1]]\n",
            "[[ 20  35 514 282   1]\n",
            " [265  43 491 269   1]]\n",
            "[[159  45 520 336   2]]\n",
            "[[  6  60 299 176   1]]\n",
            "epoch:10 step:   19/60, lr:0.000056, giou_loss:   1.52, conf_loss:  28.23, prob_loss:   4.03, total_loss:  33.77\n",
            "[[ 731  293 1465 1014    2]\n",
            " [ 197  217  885  879    2]]\n",
            "[[167  83 603 320   1]]\n",
            "[[  39    8 1853  931    1]]\n",
            "[[515  16 817 330   0]\n",
            " [118 102 397 355   0]\n",
            " [427 163 704 405   0]\n",
            " [ 17  76 257 344   0]]\n",
            "epoch:10 step:   20/60, lr:0.000056, giou_loss:   2.93, conf_loss:  30.14, prob_loss:   4.14, total_loss:  37.21\n",
            "[[ 504  208 1266  943    2]]\n",
            "[[ 22  81 525 327   1]]\n",
            "[[ 326    1 1899  624    1]]\n",
            "[[277 101 418 342   1]\n",
            " [202  90 286 351   1]\n",
            " [123 107 196 346   1]\n",
            " [ 25 114 120 351   1]]\n",
            "epoch:10 step:   21/60, lr:0.000056, giou_loss:   4.45, conf_loss:  30.75, prob_loss:   4.70, total_loss:  39.91\n",
            "[[ 13  83 684 340   1]]\n",
            "[[ 57 297 246 491   0]\n",
            " [  9 210 174 395   0]\n",
            " [145 221 314 403   0]]\n",
            "[[  55   64 2511 3954    1]]\n",
            "[[  4  85 473 419   0]\n",
            " [302  20 774 419   0]]\n",
            "epoch:10 step:   22/60, lr:0.000056, giou_loss:   2.38, conf_loss:  29.83, prob_loss:   1.73, total_loss:  33.94\n",
            "[[345  55 603 303   2]\n",
            " [  3  17 302 316   2]]\n",
            "[[ 568  282 1185  902    2]\n",
            " [  22  370  610  974    2]]\n",
            "[[ 494    1 1452  741    2]\n",
            " [ 822  643 1620 1066    2]]\n",
            "[[ 32   7 660 181   1]\n",
            " [ 87  95 674 405   1]\n",
            " [246   3 687 297   1]]\n",
            "epoch:10 step:   23/60, lr:0.000056, giou_loss:   2.90, conf_loss:  30.87, prob_loss:   5.16, total_loss:  38.93\n",
            "[[ 52   7 174 130   2]\n",
            " [139   5 275 149   2]]\n",
            "[[139  69 502 558   1]]\n",
            "[[ 16  93 102 181   2]\n",
            " [ 48  80 126 171   2]]\n",
            "[[362 300 787 709   2]]\n",
            "epoch:10 step:   24/60, lr:0.000056, giou_loss:   1.95, conf_loss:  28.61, prob_loss:   3.98, total_loss:  34.53\n",
            "[[ 88  50 279 239   2]\n",
            " [  6  42 166 220   2]]\n",
            "[[ 56  72 449 474   2]]\n",
            "[[  2 100 486 390   1]]\n",
            "[[ 85 109 423 470   2]\n",
            " [381  69 733 432   2]]\n",
            "epoch:10 step:   25/60, lr:0.000056, giou_loss:   1.25, conf_loss:  27.41, prob_loss:   3.13, total_loss:  31.78\n",
            "[[ 21 158 861 994   2]]\n",
            "[[190   6 401 202   2]\n",
            " [239 245 448 463   2]\n",
            " [ 59 325 271 524   2]\n",
            " [134 410 336 580   2]]\n",
            "[[ 64  86 595 591   2]]\n",
            "[[ 71  21 559 328   1]]\n",
            "epoch:10 step:   26/60, lr:0.000055, giou_loss:   2.31, conf_loss:  30.28, prob_loss:  10.02, total_loss:  42.61\n",
            "[[  4  71 366 398   2]]\n",
            "[[ 42  24 468 355   1]]\n",
            "[[354 117 530 287   0]\n",
            " [195 124 362 276   0]\n",
            " [433 112 587 252   0]\n",
            " [ 29  67 163 215   0]]\n",
            "[[  3  19 335 348   2]]\n",
            "epoch:10 step:   27/60, lr:0.000055, giou_loss:   2.71, conf_loss:  30.49, prob_loss:   2.74, total_loss:  35.93\n",
            "[[399 238 678 519   2]\n",
            " [559  81 869 339   2]\n",
            " [583 328 916 565   2]]\n",
            "[[  72  166  836  508    1]\n",
            " [ 358  147 1008  690    1]\n",
            " [  32  123  788  421    1]]\n",
            "[[ 98 241 262 391   2]\n",
            " [124  23 315 219   0]\n",
            " [305  18 431 405   1]\n",
            " [347 136 611 464   1]\n",
            " [358  17 512 394   1]]\n",
            "[[ 439   75 1088  779    0]]\n",
            "epoch:10 step:   28/60, lr:0.000055, giou_loss:   5.38, conf_loss:  33.42, prob_loss:   8.44, total_loss:  47.25\n",
            "[[138  19 406 296   0]]\n",
            "[[  68  305  947 1191    2]]\n",
            "[[ 47 117 229 328   0]]\n",
            "[[ 15 194 441 408   1]]\n",
            "epoch:10 step:   29/60, lr:0.000055, giou_loss:   1.07, conf_loss:  27.03, prob_loss:   2.52, total_loss:  30.62\n",
            "[[ 11   5 291 306   2]]\n",
            "[[ 22  78 641 692   0]]\n",
            "[[ 368  576 1175 1515    1]\n",
            " [  48  525 1013 1203    1]\n",
            " [   3  400  901  890    1]\n",
            " [ 166  213  947  792    1]]\n",
            "[[  6  37 303 324   0]]\n",
            "epoch:10 step:   30/60, lr:0.000055, giou_loss:   1.99, conf_loss:  29.22, prob_loss:   1.50, total_loss:  32.71\n",
            "[[228  78 332 185   2]]\n",
            "[[128   8 790 711   2]]\n",
            "[[ 469  761 1103 1761    1]\n",
            " [ 162  582  687 1640    1]]\n",
            "[[ 81  63 524 458   1]]\n",
            "epoch:10 step:   31/60, lr:0.000055, giou_loss:   1.67, conf_loss:  29.19, prob_loss:   2.13, total_loss:  32.99\n",
            "[[ 13 340 305 456   1]]\n",
            "[[105 105 503 520   0]]\n",
            "[[ 94  26 263 134   1]]\n",
            "[[154  63 352 254   2]\n",
            " [ 19  40 188 213   2]]\n",
            "epoch:10 step:   32/60, lr:0.000055, giou_loss:   1.33, conf_loss:  27.13, prob_loss:   1.36, total_loss:  29.82\n",
            "[[  5   7 191 189   0]]\n",
            "[[  39  391 1504 1965    2]]\n",
            "[[  2   7 338 370   2]]\n",
            "[[117 258 587 700   0]]\n",
            "epoch:10 step:   33/60, lr:0.000054, giou_loss:   0.90, conf_loss:  27.45, prob_loss:   1.92, total_loss:  30.27\n",
            "[[365 152 535 440   1]\n",
            " [209 175 321 491   1]\n",
            " [138 135 289 392   1]\n",
            " [306 188 417 505   1]]\n",
            "[[124  62 393 326   0]]\n",
            "[[  2   2 251 234   2]]\n",
            "[[ 10   3 210 191   0]]\n",
            "epoch:10 step:   34/60, lr:0.000054, giou_loss:   3.65, conf_loss:  31.00, prob_loss:   3.39, total_loss:  38.04\n",
            "[[207 128 289 216   2]\n",
            " [149 125 222 200   2]\n",
            " [ 19  11 249 107   1]\n",
            " [  8 146 102 237   0]]\n",
            "[[ 795  369 1173  780    2]\n",
            " [ 147   48  869  476    1]\n",
            " [ 169  180  839  798    1]]\n",
            "[[  3  11 251 267   2]]\n",
            "[[ 148  108 1549 1499    2]]\n",
            "epoch:10 step:   35/60, lr:0.000054, giou_loss:   2.57, conf_loss:  30.48, prob_loss:   4.65, total_loss:  37.70\n",
            "[[261 278 864 889   2]]\n",
            "[[277 185 444 362   0]\n",
            " [133 177 310 334   0]\n",
            " [191  86 370 255   0]]\n",
            "[[ 66  51 267 256   2]]\n",
            "[[ 889  147 1461 1147    1]]\n",
            "epoch:10 step:   36/60, lr:0.000054, giou_loss:   2.24, conf_loss:  29.05, prob_loss:   2.45, total_loss:  33.74\n",
            "[[ 67 163 441 541   2]\n",
            " [209 134 866 348   1]\n",
            " [263 267 849 551   1]]\n",
            "[[ 13  19 287 286   2]]\n",
            "[[218 163 546 487   0]\n",
            " [265 448 583 713   0]\n",
            " [  0 470 197 716   0]\n",
            " [ 73 179 332 467   0]\n",
            " [492  63 799 414   0]]\n",
            "[[ 68  23 664 690   1]]\n",
            "epoch:10 step:   37/60, lr:0.000054, giou_loss:   4.39, conf_loss:  30.31, prob_loss:   2.56, total_loss:  37.25\n",
            "[[ 70  84 503 513   0]]\n",
            "[[110  93 757 457   1]]\n",
            "[[ 24  14 574 566   0]\n",
            " [416  20 857 488   0]\n",
            " [653  30 930 429   0]]\n",
            "[[ 27  48 237 310   1]]\n",
            "epoch:10 step:   38/60, lr:0.000054, giou_loss:   1.53, conf_loss:  28.66, prob_loss:   1.76, total_loss:  31.95\n",
            "[[ 20  13 318 301   0]]\n",
            "[[ 23  10 219 193   0]]\n",
            "[[ 55 149 607 423   1]]\n",
            "[[222 137 431 358   2]\n",
            " [ 24  99 229 351   2]\n",
            " [134   1 353 171   2]]\n",
            "epoch:10 step:   39/60, lr:0.000054, giou_loss:   1.28, conf_loss:  27.02, prob_loss:   0.77, total_loss:  29.06\n",
            "[[ 360  254 1214 1132    0]]\n",
            "[[  3  13 150 157   0]]\n",
            "[[131  16 310 188   0]]\n",
            "[[ 441   41  864  467    2]\n",
            " [ 696  125 1176  609    0]\n",
            " [ 177  213 1066  762    1]]\n",
            "epoch:10 step:   40/60, lr:0.000053, giou_loss:   1.54, conf_loss:  28.04, prob_loss:   4.23, total_loss:  33.81\n",
            "[[ 69 334 553 810   0]\n",
            " [352 132 719 605   0]]\n",
            "[[ 56  63 450 444   2]\n",
            " [532  49 933 435   0]\n",
            " [148 489 829 851   1]]\n",
            "[[133 173 556 595   0]\n",
            " [401 115 725 441   0]]\n",
            "[[155  99 411 329   2]\n",
            " [ 24   1 239 187   2]\n",
            " [401  47 600 297   2]\n",
            " [205  25 417 209   2]\n",
            " [302 272 566 398   2]]\n",
            "epoch:10 step:   41/60, lr:0.000053, giou_loss:   4.34, conf_loss:  31.98, prob_loss:   4.07, total_loss:  40.40\n",
            "[[151   6 297 149   2]\n",
            " [  2 148 308 230   1]\n",
            " [ 10   1 147 146   0]]\n",
            "[[104  38 420 331   1]\n",
            " [197  30 569 205   1]\n",
            " [184  26 550 285   1]\n",
            " [216  32 559 236   1]]\n",
            "[[ 59  12 482 341   1]]\n",
            "[[ 298  121 1277 1116    0]]\n",
            "epoch:10 step:   42/60, lr:0.000053, giou_loss:   2.00, conf_loss:  28.15, prob_loss:   1.66, total_loss:  31.81\n",
            "[[ 39   3 350 274   1]\n",
            " [ 13  63 274 351   1]]\n",
            "[[ 207  161  729  495    1]\n",
            " [  30    1  571  298    1]\n",
            " [ 787    3 1198  344    1]\n",
            " [ 939  249 1198  669    1]\n",
            " [ 489  449  970  674    1]\n",
            " [   1  480  377  674    1]]\n",
            "[[ 18  19 172 164   2]]\n",
            "[[253  87 426 259   2]\n",
            " [ 25  93 310 376   1]\n",
            " [103   5 267 166   0]]\n",
            "epoch:10 step:   43/60, lr:0.000053, giou_loss:   4.53, conf_loss:  33.88, prob_loss:  12.66, total_loss:  51.07\n",
            "[[165  86 377 291   2]]\n",
            "[[ 62   2 318 258   0]]\n",
            "[[ 16  10 266 265   0]]\n",
            "[[165   1 528 348   0]]\n",
            "epoch:10 step:   44/60, lr:0.000053, giou_loss:   0.77, conf_loss:  26.36, prob_loss:   1.16, total_loss:  28.29\n",
            "[[136   3 400 275   2]]\n",
            "[[ 67  19 591 377   1]]\n",
            "[[  1 103 122 246   2]\n",
            " [ 22   0 128 114   2]\n",
            " [ 94  61 224 186   2]\n",
            " [114   1 224  69   2]\n",
            " [ 73 200 195 249   2]]\n",
            "[[ 75  91 478 499   2]]\n",
            "epoch:10 step:   45/60, lr:0.000053, giou_loss:   2.42, conf_loss:  28.60, prob_loss:   7.23, total_loss:  38.26\n",
            "[[196   9 392 254   1]]\n",
            "[[ 84  80 269 172   1]\n",
            " [272  79 456 173   1]]\n",
            "[[ 741  295 1005  547    2]\n",
            " [  28  316  299  583    2]\n",
            " [ 236  261  489  488    2]\n",
            " [ 515  295  749  503    2]]\n",
            "[[411 152 716 458   0]\n",
            " [101 317 389 582   0]\n",
            " [190 181 473 436   0]]\n",
            "epoch:10 step:   46/60, lr:0.000053, giou_loss:   5.66, conf_loss:  33.14, prob_loss:   7.76, total_loss:  46.57\n",
            "[[ 12  29 186 204   0]\n",
            " [159  16 309 184   0]]\n",
            "[[ 31  82 157 222   0]]\n",
            "[[ 114  755  917 1576    0]\n",
            " [   5   26  411  642    0]\n",
            " [ 108   29  628  582    0]]\n",
            "[[ 12  15 904 502   1]]\n",
            "epoch:10 step:   47/60, lr:0.000052, giou_loss:   2.26, conf_loss:  27.83, prob_loss:   4.33, total_loss:  34.43\n",
            "[[ 38  41 135 146   2]]\n",
            "[[ 415  210  727  498    2]\n",
            " [ 147  362  972  558    1]\n",
            " [ 157  481 1000  689    1]]\n",
            "[[232  96 465 315   2]]\n",
            "[[141 155 499 513   2]\n",
            " [ 77 133 674 590   1]]\n",
            "epoch:10 step:   48/60, lr:0.000052, giou_loss:   2.37, conf_loss:  27.95, prob_loss:   6.49, total_loss:  36.81\n",
            "[[164  36 595 477   2]]\n",
            "[[ 56 113 278 332   2]\n",
            " [213  98 413 320   2]]\n",
            "[[ 75   9 230 156   0]]\n",
            "[[ 15  74 264 326   0]\n",
            " [232  59 512 332   0]]\n",
            "epoch:10 step:   49/60, lr:0.000052, giou_loss:   1.39, conf_loss:  26.27, prob_loss:   2.65, total_loss:  30.32\n",
            "[[ 39  55 445 474   0]]\n",
            "[[1284  128 1707  535    2]\n",
            " [ 856  225 1240  652    2]\n",
            " [ 816  611 1296 1043    2]]\n",
            "[[  19  135 1199  601    1]]\n",
            "[[ 643   91 1188  608    2]]\n",
            "epoch:10 step:   50/60, lr:0.000052, giou_loss:   2.37, conf_loss:  28.10, prob_loss:   3.11, total_loss:  33.57\n",
            "[[110   0 614 527   0]]\n",
            "[[232 196 455 410   2]\n",
            " [ 27 188 254 399   0]\n",
            " [207 223 563 420   1]]\n",
            "[[ 14   5 254 209   0]]\n",
            "[[349 227 861 784   0]]\n",
            "epoch:10 step:   51/60, lr:0.000052, giou_loss:   1.05, conf_loss:  26.76, prob_loss:   4.39, total_loss:  32.21\n",
            "[[ 92 124 605 636   2]]\n",
            "[[ 54  18 534 222   1]]\n",
            "[[292  78 711 476   2]\n",
            " [ 36   4 291 245   2]]\n",
            "[[ 18  15 341 349   0]]\n",
            "epoch:10 step:   52/60, lr:0.000052, giou_loss:   1.38, conf_loss:  27.65, prob_loss:   2.98, total_loss:  32.01\n",
            "[[ 12  26 189 189   0]]\n",
            "[[ 91  93 260 248   2]\n",
            " [210  85 370 248   2]\n",
            " [272  38 414 179   2]]\n",
            "[[ 21  80 199 258   2]\n",
            " [224  94 392 237   0]\n",
            " [ 21 311 333 435   1]]\n",
            "[[ 96 154 516 596   0]]\n",
            "epoch:10 step:   53/60, lr:0.000052, giou_loss:   2.58, conf_loss:  28.75, prob_loss:   3.34, total_loss:  34.66\n",
            "[[ 69  43 218 258   1]]\n",
            "[[  5 137 424 331   1]\n",
            " [ 96  74 418 211   1]\n",
            " [102  19 377 154   1]]\n",
            "[[174 101 576 190   1]\n",
            " [173 109 569 261   1]\n",
            " [163 145 560 354   1]\n",
            " [ 86 165 437 436   1]]\n",
            "[[125 196 295 374   0]\n",
            " [197 144 363 328   0]]\n",
            "epoch:10 step:   54/60, lr:0.000051, giou_loss:   3.50, conf_loss:  30.07, prob_loss:   8.78, total_loss:  42.35\n",
            "[[141  50 334 252   0]]\n",
            "[[129  72 271 217   0]\n",
            " [219 195 341 316   0]\n",
            " [182 311 334 435   0]\n",
            " [  1 181 167 338   0]\n",
            " [309 101 408 191   0]\n",
            " [377 248 451 323   0]\n",
            " [402  77 463 138   0]\n",
            " [395 152 460 219   0]]\n",
            "[[ 89  84 543 540   2]]\n",
            "[[145 186 391 411   0]\n",
            " [416 183 603 433   0]\n",
            " [267 158 488 373   0]]\n",
            "epoch:10 step:   55/60, lr:0.000051, giou_loss:   7.05, conf_loss:  35.94, prob_loss:  13.03, total_loss:  56.03\n",
            "[[  2  52 406 196   1]]\n",
            "[[ 477  173 1584  928    1]\n",
            " [ 180  592 1451 1273    1]\n",
            " [ 778    6 1753  616    1]\n",
            " [1033    6 1897  555    1]]\n",
            "[[ 690  258 1039  626    2]\n",
            " [ 361  295  790  646    2]\n",
            " [ 214  235  558  578    2]\n",
            " [ 374   19  814  377    2]]\n",
            "[[ 30   5 227 775   1]]\n",
            "epoch:10 step:   56/60, lr:0.000051, giou_loss:   3.63, conf_loss:  29.45, prob_loss:   9.65, total_loss:  42.73\n",
            "[[293  54 874 459   1]\n",
            " [306 185 881 668   1]]\n",
            "[[171  16 630 266   1]]\n",
            "[[ 32   5 219 207   2]]\n",
            "[[174 141 470 459   0]]\n",
            "epoch:10 step:   57/60, lr:0.000051, giou_loss:   1.26, conf_loss:  26.48, prob_loss:   1.33, total_loss:  29.07\n",
            "[[ 18   9 250 179   1]\n",
            " [ 85  40 270 213   1]\n",
            " [150  53 300 243   1]\n",
            " [268  72 354 287   1]]\n",
            "[[ 229   13  855 1151    1]]\n",
            "[[ 78  38 391 164   1]\n",
            " [ 16  57 239 272   1]\n",
            " [ 62  66 354 215   1]]\n",
            "[[ 643  364 1396 1046    0]]\n",
            "epoch:10 step:   58/60, lr:0.000051, giou_loss:   3.60, conf_loss:  29.42, prob_loss:   3.55, total_loss:  36.57\n",
            "[[324 291 692 680   2]]\n",
            "[[219 121 545 396   0]]\n",
            "[[651  31 947 303   1]\n",
            " [190  61 458 404   1]\n",
            " [273  79 562 423   1]\n",
            " [302 159 562 553   1]\n",
            " [516 146 686 572   1]\n",
            " [582 145 873 480   1]\n",
            " [536  60 909 341   1]]\n",
            "[[221  83 463 338   2]\n",
            " [440 148 666 404   0]\n",
            " [ 54 228 627 528   1]]\n",
            "epoch:10 step:   59/60, lr:0.000051, giou_loss:   5.81, conf_loss:  32.01, prob_loss:   9.72, total_loss:  47.54\n",
            "[[584 438 867 708   0]\n",
            " [492 141 740 394   0]\n",
            " [176 199 490 466   0]\n",
            " [367  17 619 240   0]\n",
            " [642  35 907 269   0]]\n",
            "[[ 51   4 303 263   2]]\n",
            "[[342  83 701 416   1]\n",
            " [176  39 614 382   1]\n",
            " [130   8 595 226   1]]\n",
            "[[389 111 526 270   2]\n",
            " [399  36 535 175   2]\n",
            " [ 60 106 192 251   2]\n",
            " [ 18  41 167 171   2]\n",
            " [156  63 299 207   2]\n",
            " [223 115 374 257   2]\n",
            " [305  20 438 146   2]]\n",
            "epoch:10 step:    0/60, lr:0.000050, giou_loss:   8.51, conf_loss:  36.02, prob_loss:   9.63, total_loss:  54.16\n",
            "[[ 791  317 1170  666    0]]\n",
            "[[ 500   39 1179  787    0]]\n",
            "[[ 44  32 284 276   2]\n",
            " [293  27 547 294   2]]\n",
            "[[374 107 981 710   0]]\n",
            "epoch:10 step:    1/60, lr:0.000050, giou_loss:   1.32, conf_loss:  26.43, prob_loss:   1.33, total_loss:  29.09\n",
            "[[ 23 472 848 880   1]\n",
            " [ 66 233 878 657   1]\n",
            " [212 111 949 611   1]]\n",
            "[[113  36 276 182   0]\n",
            " [280  83 452 255   1]\n",
            " [188  77 412 217   1]\n",
            " [172  78 395 176   1]]\n",
            "[[ 29  11 246 348   1]]\n",
            "[[  1   4 348 309   1]]\n",
            "[[114 114 324 431   1]\n",
            " [ 23  56 170 422   1]]\n",
            "[[  99   96  532  592    2]\n",
            " [ 652  196 1123  710    2]\n",
            " [ 430  108  776  525    2]]\n",
            "[[  1 123 130 299   2]\n",
            " [379   1 558 144   2]]\n",
            "[[ 63  17 560 323   1]]\n",
            "[[ 25 135 663 543   1]\n",
            " [ 86  92 736 385   1]]\n",
            "[[ 88  41 415 393   0]]\n",
            "[[111  51 372 277   2]]\n",
            "[[  90  287  608  809    2]\n",
            " [ 560  217 1145  769    2]\n",
            " [ 323   37  786  551    2]]\n",
            "[[143  40 415 346   0]]\n",
            "[[ 17 195 202 356   2]\n",
            " [200 207 376 365   2]\n",
            " [223  52 389 217   2]\n",
            " [261 157 412 264   2]]\n",
            "[[ 71  60 175 164   0]\n",
            " [ 12  22 105 111   0]\n",
            " [134  23 243 115   0]\n",
            " [107 126 216 229   0]\n",
            " [207 138 298 229   0]]\n",
            "[[ 419  297 2602 1034    1]]\n",
            "[[112 144 173 211   0]\n",
            " [250 140 322 215   0]]\n",
            "[[155 105 453 436   0]]\n",
            "[[ 61 130 415 473   2]]\n",
            "[[383   3 596 308   1]\n",
            " [265  15 513 276   1]\n",
            " [516  52 762 315   1]\n",
            " [639  42 819 325   1]]\n",
            "[[145  36 421 286   2]]\n",
            "[[  45  228 1232  658    1]]\n",
            "[[ 146  361 1112 1254    2]]\n",
            "[[ 77  74 540 518   0]]\n",
            "[[ 65 122 365 404   0]]\n",
            "[[  58  124 1776 1910    0]]\n",
            "[[   1  571  923 1006    1]\n",
            " [  80    6  481  395    0]\n",
            " [ 543   17  942  430    2]]\n",
            "[[227  62 723 500   0]]\n",
            "[[ 242  460  963 1229    0]]\n",
            "[[ 64  84 484 457   0]\n",
            " [550  44 850 356   0]\n",
            " [197   1 559 273   0]]\n",
            "[[  1   1 794 738   0]]\n",
            "[[ 48 234 593 789   2]]\n",
            "[[162  50 310 354   1]\n",
            " [ 81  16 220 348   1]]\n",
            "[[133 165 922 525   1]]\n",
            "[[ 219  494  946 1173    0]\n",
            " [ 930  598 1667 1174    0]\n",
            " [  78  323  744  945    0]\n",
            " [1228  312 1896  916    0]]\n",
            "[[ 75  62 711 745   2]]\n",
            "[[ 85 138 553 352   1]]\n",
            "[[150 165 923 983   2]]\n",
            "[[203 115 584 452   1]]\n",
            "[[ 43 134 358 455   0]]\n",
            "[[ 44 164 369 277   1]\n",
            " [ 87 220 401 355   1]]\n",
            "[[ 10   8 344 336   0]]\n",
            "[[ 43  57 143 160   2]\n",
            " [148  62 249 162   2]]\n",
            "[[ 42  41 193 182   2]\n",
            " [158   1 299 109   2]\n",
            " [284  17 355 147   2]\n",
            " [  1  15  61 138   2]\n",
            " [ 34   1 172  85   2]]\n",
            "[[  5  12 333 353   1]]\n",
            "[[133 246 337 434   2]\n",
            " [270 364 400 476   2]\n",
            " [522 344 649 470   2]\n",
            " [537 302 661 402   2]\n",
            " [328 229 544 422   0]]\n",
            "[[ 19  68 278 343   0]]\n",
            "[[228  83 401 270   2]\n",
            " [ 43  74 183 244   1]\n",
            " [128  56 282 221   0]]\n",
            "[[ 21 216 173 371   2]\n",
            " [108 160 244 310   2]\n",
            " [263 166 415 307   2]]\n",
            "[[165 163 367 336   2]\n",
            " [  1 100 189 266   2]\n",
            " [167  88 329 237   2]]\n",
            "[[123 110 414 427   0]\n",
            " [338   1 638 242   0]]\n",
            "[[279  14 562 295   2]]\n",
            "[[174  20 364 199   2]]\n",
            "[[ 313  228  935 1864    1]\n",
            " [ 401  129 1078 1890    1]\n",
            " [ 135  291  642 1747    1]]\n",
            "[[  70  398 1388 1731    0]]\n",
            "[[166  92 638 559   2]]\n",
            "[[ 146  205  493  594    2]\n",
            " [ 252  528  571  821    0]\n",
            " [ 468   49 1228  287    1]\n",
            " [ 540  208 1244  464    1]\n",
            " [ 506  301 1279  574    1]]\n",
            "[[100  64 381 190   1]\n",
            " [ 95 280 380 405   1]]\n",
            "[[ 77 183 641 716   0]]\n",
            "[[122 127 484 471   1]\n",
            " [212 115 691 407   1]\n",
            " [238  77 743 294   1]]\n",
            "\n",
            "\n",
            "giou_val_loss:   2.76, conf_val_loss:  30.80, prob_val_loss:   4.53, total_val_loss:  38.09\n",
            "\n",
            "\n",
            "[[ 546  230 1279 1005    2]\n",
            " [  14  193  707  925    2]]\n",
            "[[  2  15 284 260   0]]\n",
            "[[ 71  18 497 349   1]]\n",
            "[[164  27 360 272   1]]\n",
            "epoch:11 step:    2/60, lr:0.000050, giou_loss:   1.00, conf_loss:  26.54, prob_loss:   1.13, total_loss:  28.67\n",
            "[[ 29   1 834 214   1]]\n",
            "[[109  22 611 225   1]]\n",
            "[[ 31  39 238 247   0]]\n",
            "[[ 374  409 1008 1409    1]\n",
            " [ 790  230 1315 1288    1]]\n",
            "epoch:11 step:    3/60, lr:0.000050, giou_loss:   1.44, conf_loss:  26.71, prob_loss:   2.62, total_loss:  30.77\n",
            "[[412  60 877 516   2]]\n",
            "[[142  43 340 234   2]\n",
            " [  7  20 176 193   2]]\n",
            "[[132 405 677 922   2]]\n",
            "[[502  23 838 393   2]\n",
            " [155  12 484 334   2]]\n",
            "epoch:11 step:    4/60, lr:0.000050, giou_loss:   1.61, conf_loss:  26.41, prob_loss:   3.51, total_loss:  31.53\n",
            "[[ 54   9 485 450   2]]\n",
            "[[ 207  160  729  494    1]\n",
            " [  30    0  571  297    1]\n",
            " [ 787    2 1198  343    1]\n",
            " [ 939  248 1198  668    1]\n",
            " [ 489  448  970  673    1]\n",
            " [   1  479  377  673    1]]\n",
            "[[ 17  57 421 201   1]]\n",
            "[[  7  17 181 192   0]\n",
            " [154   4 304 172   0]]\n",
            "epoch:11 step:    5/60, lr:0.000050, giou_loss:   3.76, conf_loss:  29.95, prob_loss:   5.95, total_loss:  39.66\n",
            "[[  5  19 146 176   0]\n",
            " [145  14 247 123   0]\n",
            " [ 96  82 207 191   0]]\n",
            "[[  4 181 611 784   0]]\n",
            "[[150  64 383 283   2]]\n",
            "[[ 51  86 402 459   0]\n",
            " [368 110 775 541   0]]\n",
            "epoch:11 step:    6/60, lr:0.000050, giou_loss:   1.79, conf_loss:  27.75, prob_loss:   1.65, total_loss:  31.20\n",
            "[[199  82 384 174   1]\n",
            " [ 12  81 196 175   1]]\n",
            "[[ 16   8 171 155   0]]\n",
            "[[ 26  22 388 349   2]]\n",
            "[[ 11  22 309 310   0]]\n",
            "epoch:11 step:    7/60, lr:0.000049, giou_loss:   2.10, conf_loss:  27.03, prob_loss:   1.73, total_loss:  30.86\n",
            "[[  17    1 1590  624    1]]\n",
            "[[ 248  122 1102 1000    0]]\n",
            "[[472 327 782 643   2]\n",
            " [ 15 190 621 407   1]\n",
            " [ 21 248 671 503   1]\n",
            " [ 69  25 331 318   0]]\n",
            "[[144  55 337 257   0]]\n",
            "epoch:11 step:    8/60, lr:0.000049, giou_loss:   2.01, conf_loss:  26.02, prob_loss:   1.74, total_loss:  29.77\n",
            "[[ 16  60 256 264   0]]\n",
            "[[ 60  43 157 148   2]]\n",
            "[[291  55 588 342   0]]\n",
            "[[502  18 896 399   2]\n",
            " [ 19   4 420 390   0]\n",
            " [123 444 804 806   1]]\n",
            "epoch:11 step:    9/60, lr:0.000049, giou_loss:   1.54, conf_loss:  25.67, prob_loss:   0.66, total_loss:  27.87\n",
            "[[ 12 137 221 358   2]\n",
            " [214  99 419 351   2]\n",
            " [ 90   1 309 171   2]]\n",
            "[[  66   15 1880  938    1]]\n",
            "[[184 178 654 620   0]]\n",
            "[[ 73  58 409 421   2]]\n",
            "epoch:11 step:   10/60, lr:0.000049, giou_loss:   1.38, conf_loss:  25.90, prob_loss:   2.72, total_loss:  30.00\n",
            "[[153  12 275 135   2]\n",
            " [ 52  10 188 154   2]]\n",
            "[[ 65  52 311 313   2]]\n",
            "[[308  81 667 414   1]\n",
            " [142  37 580 380   1]\n",
            " [ 96   6 561 224   1]]\n",
            "[[ 54  66 508 522   2]]\n",
            "epoch:11 step:   11/60, lr:0.000049, giou_loss:   1.80, conf_loss:  26.28, prob_loss:   3.53, total_loss:  31.61\n",
            "[[136 199 569 628   0]]\n",
            "[[291 128 603 416   2]\n",
            " [ 46 280 871 476   1]\n",
            " [ 18 399 861 607   1]]\n",
            "[[129 235 548 633   2]\n",
            " [549 161 804 402   2]]\n",
            "[[ 294  107 1400 1134    0]]\n",
            "epoch:11 step:   12/60, lr:0.000049, giou_loss:   2.24, conf_loss:  26.26, prob_loss:   3.08, total_loss:  31.58\n",
            "[[ 740  478 1118  889    2]\n",
            " [  92  157  814  585    1]\n",
            " [ 114  289  784  907    1]]\n",
            "[[273  54 470 824   1]]\n",
            "[[ 56   6 417 297   2]]\n",
            "[[ 32  61 545 573   2]]\n",
            "epoch:11 step:   13/60, lr:0.000049, giou_loss:   1.41, conf_loss:  25.68, prob_loss:   2.59, total_loss:  29.69\n",
            "[[ 89 104 477 279   1]]\n",
            "[[139 125 441 439   0]\n",
            " [559 211 838 464   0]\n",
            " [252 272 529 514   0]\n",
            " [699 185 939 453   0]]\n",
            "[[ 459  168 1566  923    1]\n",
            " [ 162  587 1433 1268    1]\n",
            " [ 760    1 1735  611    1]\n",
            " [1015    1 1879  550    1]]\n",
            "[[ 213  273 1678 1847    2]]\n",
            "epoch:11 step:   14/60, lr:0.000048, giou_loss:   3.56, conf_loss:  28.53, prob_loss:   5.02, total_loss:  37.10\n",
            "[[ 62   1 425 348   0]]\n",
            "[[ 564  229 1298  950    2]\n",
            " [  30  153  718  815    2]]\n",
            "[[519 164 887 553   2]]\n",
            "[[214  99 383 254   2]\n",
            " [104  91 264 254   2]\n",
            " [ 60  44 202 185   2]]\n",
            "epoch:11 step:   15/60, lr:0.000048, giou_loss:   1.64, conf_loss:  25.76, prob_loss:   3.43, total_loss:  30.83\n",
            "[[  4   5 183 177   0]]\n",
            "[[213 168 571 526   2]\n",
            " [ 38 146 635 603   1]]\n",
            "[[353 149 691 510   2]\n",
            " [ 43 109 395 472   2]]\n",
            "[[155  69 456 389   2]]\n",
            "epoch:11 step:   16/60, lr:0.000048, giou_loss:   1.24, conf_loss:  25.71, prob_loss:   2.87, total_loss:  29.82\n",
            "[[  2  94 203 299   2]]\n",
            "[[108  23 212 130   2]]\n",
            "[[355 289 531 459   0]\n",
            " [196 296 363 448   0]\n",
            " [434 284 588 424   0]\n",
            " [ 30 239 164 387   0]]\n",
            "[[ 774  253 1199  662    2]]\n",
            "epoch:11 step:   17/60, lr:0.000048, giou_loss:   3.42, conf_loss:  29.20, prob_loss:   3.26, total_loss:  35.89\n",
            "[[ 11 126 193 337   0]]\n",
            "[[ 46  27 469 356   1]]\n",
            "[[433 235 597 385   2]\n",
            " [380  17 571 213   0]\n",
            " [264  12 390 399   1]\n",
            " [ 84 130 348 458   1]\n",
            " [183  11 337 388   1]]\n",
            "[[ 80  31 604 389   1]]\n",
            "epoch:11 step:   18/60, lr:0.000048, giou_loss:   4.08, conf_loss:  28.21, prob_loss:   7.92, total_loss:  40.20\n",
            "[[  5 174 562 408   1]]\n",
            "[[ 31  82 157 222   0]]\n",
            "[[231  57 547 350   1]\n",
            " [ 82  49 454 224   1]\n",
            " [101  45 467 304   1]\n",
            " [ 92  51 435 255   1]]\n",
            "[[  5  33 254 265   2]]\n",
            "epoch:11 step:   19/60, lr:0.000048, giou_loss:   1.21, conf_loss:  24.99, prob_loss:   0.90, total_loss:  27.10\n",
            "[[507   5 727 206   0]\n",
            " [544 197 762 433   0]\n",
            " [415 157 614 391   0]\n",
            " [ 10 278 192 493   0]\n",
            " [122 350 299 547   0]\n",
            " [223 219 464 443   0]\n",
            " [424 330 606 523   0]\n",
            " [142 405 354 578   0]]\n",
            "[[ 88 120 767 868   0]]\n",
            "[[ 27  95 113 183   2]\n",
            " [  3  82  81 173   2]]\n",
            "[[ 248  856 1051 1677    0]\n",
            " [ 139  127  545  743    0]\n",
            " [ 242  130  762  683    0]]\n",
            "epoch:11 step:   20/60, lr:0.000048, giou_loss:   6.86, conf_loss:  33.76, prob_loss:   5.78, total_loss:  46.41\n",
            "[[ 18  31 461 426   1]]\n",
            "[[ 353  305 1232 1191    2]]\n",
            "[[ 71  79 434 568   1]]\n",
            "[[294 127 543 379   0]\n",
            " [ 46 112 326 385   0]]\n",
            "epoch:11 step:   21/60, lr:0.000047, giou_loss:   1.34, conf_loss:  25.28, prob_loss:   0.54, total_loss:  27.16\n",
            "[[100   2 411 273   1]\n",
            " [ 74  62 335 350   1]]\n",
            "[[363  98 587 329   1]\n",
            " [ 94 124 467 310   1]\n",
            " [127  49 502 197   1]]\n",
            "[[ 67  32 547 236   1]]\n",
            "[[101  31 537 268   1]]\n",
            "epoch:11 step:   22/60, lr:0.000047, giou_loss:   1.99, conf_loss:  27.24, prob_loss:   2.85, total_loss:  32.09\n",
            "[[185 180 352 357   0]\n",
            " [ 41 172 218 329   0]\n",
            " [ 99  81 278 250   0]]\n",
            "[[ 366  446 1173 1385    1]\n",
            " [  46  395 1011 1073    1]\n",
            " [   1  270  899  760    1]\n",
            " [ 164   83  945  662    1]]\n",
            "[[602  16 898 288   1]\n",
            " [141  46 409 389   1]\n",
            " [224  64 513 408   1]\n",
            " [253 144 513 538   1]\n",
            " [467 131 637 557   1]\n",
            " [533 130 824 465   1]\n",
            " [487  45 860 326   1]]\n",
            "[[ 39 135 710 392   1]]\n",
            "epoch:11 step:   23/60, lr:0.000047, giou_loss:   6.92, conf_loss:  34.15, prob_loss:   9.22, total_loss:  50.28\n",
            "[[ 661  111 1310  815    0]]\n",
            "[[  24   64 2756 2547    1]]\n",
            "[[ 25  42 275 297   0]]\n",
            "[[  7 146 426 340   1]\n",
            " [ 13  83 335 220   1]\n",
            " [ 54  28 329 163   1]]\n",
            "epoch:11 step:   24/60, lr:0.000047, giou_loss:   1.44, conf_loss:  26.80, prob_loss:   3.33, total_loss:  31.57\n",
            "[[  27  247  291  499    2]\n",
            " [ 733  268 1004  535    2]\n",
            " [ 543  213  796  440    2]\n",
            " [ 283  247  517  455    2]]\n",
            "[[ 47  41 506 291   1]]\n",
            "[[261 278 864 889   2]]\n",
            "[[584 438 867 708   0]\n",
            " [492 141 740 394   0]\n",
            " [176 199 490 466   0]\n",
            " [367  17 619 240   0]\n",
            " [642  35 907 269   0]]\n",
            "epoch:11 step:   25/60, lr:0.000047, giou_loss:   4.77, conf_loss:  31.49, prob_loss:   6.68, total_loss:  42.94\n",
            "[[ 26  57 604 569   2]]\n",
            "[[ 47  42 709 745   2]]\n",
            "[[ 37  52 205 222   0]]\n",
            "[[ 286  104 1039  786    0]]\n",
            "epoch:11 step:   26/60, lr:0.000047, giou_loss:   0.44, conf_loss:  24.50, prob_loss:   1.32, total_loss:  26.26\n",
            "[[ 40 106 443 514   2]]\n",
            "[[ 92  48 405 174   1]\n",
            " [ 30  67 253 282   1]\n",
            " [ 76  76 368 225   1]]\n",
            "[[204  98 460 328   2]\n",
            " [376   0 591 186   2]\n",
            " [ 15  46 214 296   2]\n",
            " [198  24 410 208   2]\n",
            " [ 49 271 313 397   2]]\n",
            "[[ 181  395 1582 1786    2]]\n",
            "epoch:11 step:   27/60, lr:0.000047, giou_loss:   2.70, conf_loss:  27.57, prob_loss:   7.18, total_loss:  37.44\n",
            "[[ 11  43 592 448   1]\n",
            " [ 24 174 599 657   1]]\n",
            "[[ 31  68 217 250   0]]\n",
            "[[ 27  12 655 186   1]\n",
            " [ 13 100 600 410   1]\n",
            " [  0   8 441 302   1]]\n",
            "[[441  70 953 627   0]]\n",
            "epoch:11 step:   28/60, lr:0.000046, giou_loss:   1.67, conf_loss:  25.20, prob_loss:   3.38, total_loss:  30.25\n",
            "[[ 28  42 224 225   0]]\n",
            "[[161  43 372 239   2]\n",
            " [210 282 419 500   2]\n",
            " [ 30 362 242 561   2]\n",
            " [105 447 307 617   2]]\n",
            "[[206  79 316 189   0]\n",
            " [138  52 239 147   0]\n",
            " [ 30  65 134 176   0]]\n",
            "[[ 82  43 231 258   1]]\n",
            "epoch:11 step:   29/60, lr:0.000046, giou_loss:   3.74, conf_loss:  28.62, prob_loss:   6.37, total_loss:  38.73\n",
            "[[ 24  11 253 235   2]]\n",
            "[[317  76 557 320   2]\n",
            " [ 54  71 308 338   2]]\n",
            "[[ 78 103 562 393   1]]\n",
            "[[  4  67 473 401   0]\n",
            " [302   2 774 401   0]]\n",
            "epoch:11 step:   30/60, lr:0.000046, giou_loss:   1.33, conf_loss:  24.65, prob_loss:   0.77, total_loss:  26.75\n",
            "[[262 231 485 445   2]\n",
            " [463 223 690 434   0]\n",
            " [154 258 510 455   1]]\n",
            "[[126 136 296 314   0]\n",
            " [ 58  84 224 268   0]]\n",
            "[[229 105 402 277   2]\n",
            " [  1 111 286 394   1]\n",
            " [ 79  23 243 184   0]]\n",
            "[[253 182 581 506   0]\n",
            " [216 467 534 732   0]\n",
            " [602 489 799 735   0]\n",
            " [467 198 726 486   0]\n",
            " [  0  82 307 433   0]]\n",
            "epoch:11 step:   31/60, lr:0.000046, giou_loss:   5.13, conf_loss:  29.19, prob_loss:   5.09, total_loss:  39.41\n",
            "[[  17   12 1524  788    1]]\n",
            "[[ 144    2 1102  742    2]\n",
            " [ 472  644 1270 1067    2]]\n",
            "[[  7   2 154 146   0]]\n",
            "[[   1  135 1181  601    1]]\n",
            "epoch:11 step:   32/60, lr:0.000046, giou_loss:   1.40, conf_loss:  25.40, prob_loss:   2.13, total_loss:  28.93\n",
            "[[356  72 906 624   0]\n",
            " [ 73  78 514 546   0]\n",
            " [  0  88 277 487   0]]\n",
            "[[ 21 237 254 504   0]\n",
            " [191 325 430 549   0]\n",
            " [555 219 759 451   0]\n",
            " [346 171 576 386   0]\n",
            " [134 156 327 307   0]\n",
            " [236   2 468 196   0]\n",
            " [ 69  35 234 200   0]]\n",
            "[[ 31   2 466 444   0]]\n",
            "[[318 188 597 469   2]\n",
            " [127  31 437 289   2]\n",
            " [ 80 278 413 515   2]]\n",
            "epoch:11 step:   33/60, lr:0.000046, giou_loss:   6.18, conf_loss:  34.57, prob_loss:   9.05, total_loss:  49.81\n",
            "[[ 864   80 1287  487    2]\n",
            " [1331  177 1715  604    2]\n",
            " [1275  563 1755  995    2]]\n",
            "[[ 32 233 244 438   2]]\n",
            "[[ 31  84 525 331   1]\n",
            " [276  92 502 318   1]]\n",
            "[[134 231 249 330   2]\n",
            " [544 209 624 284   2]\n",
            " [518 152 616 229   2]\n",
            " [148 119 405 219   1]\n",
            " [186 161 420 256   1]\n",
            " [322 118 479 263   1]\n",
            " [350 168 486 297   1]\n",
            " [327  43 436 146   0]\n",
            " [437  76 551 181   0]]\n",
            "epoch:11 step:   34/60, lr:0.000046, giou_loss:   9.48, conf_loss:  35.97, prob_loss:  20.62, total_loss:  66.08\n",
            "[[ 213  331  975 1066    2]]\n",
            "[[195  61 438 284   2]]\n",
            "[[142  37 470 339   1]\n",
            " [188 114 586 371   1]\n",
            " [202  31 514 226   1]]\n",
            "[[125 204 314 398   0]\n",
            " [197 117 362 302   0]\n",
            " [ 57 128 226 310   0]]\n",
            "epoch:11 step:   35/60, lr:0.000045, giou_loss:   2.30, conf_loss:  26.26, prob_loss:   2.40, total_loss:  30.96\n",
            "[[140 300 489 668   2]\n",
            " [389 337 818 688   2]\n",
            " [621 277 965 620   2]\n",
            " [365  61 805 419   2]]\n",
            "[[  45  173 1402 1493    0]]\n",
            "[[ 43 125 422 474   0]]\n",
            "[[134  80 622 387   1]]\n",
            "epoch:11 step:   36/60, lr:0.000045, giou_loss:   2.36, conf_loss:  26.53, prob_loss:   2.91, total_loss:  31.80\n",
            "[[203  72 659 558   2]]\n",
            "[[  62   74 1716  971    1]\n",
            " [ 483  123 1996 1191    1]\n",
            " [   6   48 1599  627    1]]\n",
            "[[  14  226  854 1062    2]]\n",
            "[[ 16 117 418 206   1]\n",
            " [ 23 125 419 277   1]\n",
            " [ 32 161 429 370   1]\n",
            " [155 181 506 452   1]]\n",
            "epoch:11 step:   37/60, lr:0.000045, giou_loss:   2.66, conf_loss:  25.99, prob_loss:   6.00, total_loss:  34.64\n",
            "[[103 104 224 247   2]\n",
            " [ 97   1 203 115   2]\n",
            " [  1  62 131 187   2]\n",
            " [  1   2 111  70   2]\n",
            " [ 30 201 152 250   2]]\n",
            "[[ 12  10 330 314   1]]\n",
            "[[ 102   31  674 1031    1]]\n",
            "[[407 130 544 289   2]\n",
            " [417  55 553 194   2]\n",
            " [ 78 125 210 270   2]\n",
            " [ 36  60 185 190   2]\n",
            " [174  82 317 226   2]\n",
            " [241 134 392 276   2]\n",
            " [323  39 456 165   2]]\n",
            "epoch:11 step:   38/60, lr:0.000045, giou_loss:   5.67, conf_loss:  32.09, prob_loss:  11.60, total_loss:  49.36\n",
            "[[ 67 185 363 503   0]]\n",
            "[[364  99 726 464   0]]\n",
            "[[ 38  42 930 529   1]]\n",
            "[[394 220 658 533   2]\n",
            " [347  68 667 323   0]\n",
            " [ 43   4 592 306   1]]\n",
            "epoch:11 step:   39/60, lr:0.000045, giou_loss:   1.92, conf_loss:  25.84, prob_loss:   3.56, total_loss:  31.32\n",
            "[[ 39  28 226 230   2]]\n",
            "[[ 13  45 287 312   2]]\n",
            "[[183 230 947 572   1]\n",
            " [ 11 211 661 754   1]\n",
            " [231 187 987 485   1]]\n",
            "[[ 20  82 572 356   1]]\n",
            "epoch:11 step:   40/60, lr:0.000045, giou_loss:   1.36, conf_loss:  24.52, prob_loss:   1.08, total_loss:  26.95\n",
            "[[  3   1 235 171   1]\n",
            " [ 70  32 255 205   1]\n",
            " [135  45 285 235   1]\n",
            " [253  64 339 279   1]]\n",
            "[[ 69  42 325 298   0]]\n",
            "[[196 145 663 420   1]\n",
            " [220  84 653 314   1]]\n",
            "[[ 51 137 297 362   0]\n",
            " [322 134 509 384   0]\n",
            " [173 109 394 324   0]]\n",
            "epoch:11 step:   41/60, lr:0.000045, giou_loss:   3.70, conf_loss:  28.26, prob_loss:   4.13, total_loss:  36.09\n",
            "[[ 102   99 1969 1919    2]]\n",
            "[[ 10  16 657 380   1]]\n",
            "[[ 32 145 324 261   1]]\n",
            "[[167 219 651 695   0]\n",
            " [  1  17 368 490   0]]\n",
            "epoch:11 step:   42/60, lr:0.000044, giou_loss:   1.21, conf_loss:  25.47, prob_loss:   2.68, total_loss:  29.35\n",
            "[[  1  43 253 302   2]]\n",
            "[[ 85  57 254 165   1]]\n",
            "[[ 13  25 615 478   1]\n",
            " [ 20   1 591 298   1]\n",
            " [ 43   5 554 214   1]\n",
            " [446 145 645 478   1]]\n",
            "[[ 93  11 270 174   0]]\n",
            "epoch:11 step:   43/60, lr:0.000044, giou_loss:   1.77, conf_loss:  25.59, prob_loss:   4.80, total_loss:  32.16\n",
            "[[  2  42 295 158   1]]\n",
            "[[ 25  64 202 231   0]]\n",
            "[[130  56 272 201   0]\n",
            " [220 179 342 300   0]\n",
            " [183 295 335 419   0]\n",
            " [  2 165 168 322   0]\n",
            " [310  85 409 175   0]\n",
            " [378 232 452 307   0]\n",
            " [403  61 464 122   0]\n",
            " [396 136 461 203   0]]\n",
            "[[ 37  24 656 638   0]]\n",
            "epoch:11 step:   44/60, lr:0.000044, giou_loss:   5.67, conf_loss:  32.28, prob_loss:   7.54, total_loss:  45.49\n",
            "[[227  97 625 512   0]]\n",
            "[[ 176  217 1155 1212    0]]\n",
            "[[106 104 247 345   1]\n",
            " [238  93 322 354   1]\n",
            " [328 110 401 349   1]\n",
            " [404 117 499 354   1]]\n",
            "[[179  31 599 407   2]\n",
            " [ 52 284 999 586   1]\n",
            " [555  72 924 384   0]]\n",
            "epoch:11 step:   45/60, lr:0.000044, giou_loss:   4.95, conf_loss:  27.95, prob_loss:   5.74, total_loss:  38.64\n",
            "[[141  81 409 358   0]]\n",
            "[[  7  87 190 277   2]]\n",
            "[[ 15   1 263 257   2]]\n",
            "[[  1 213 427 427   1]]\n",
            "epoch:11 step:   46/60, lr:0.000044, giou_loss:   0.94, conf_loss:  24.51, prob_loss:   1.29, total_loss:  26.74\n",
            "[[312  30 477 225   2]\n",
            " [126  25 319 248   0]\n",
            " [ 72 243 617 496   1]]\n",
            "[[131  50 457 325   0]]\n",
            "[[140  72 368 305   2]]\n",
            "[[  61   29 1032  813    1]\n",
            " [ 374   88  998  687    0]]\n",
            "epoch:11 step:   47/60, lr:0.000044, giou_loss:   2.44, conf_loss:  25.13, prob_loss:   3.17, total_loss:  30.73\n",
            "[[ 42  10 374 339   2]]\n",
            "[[ 36  48 323 334   0]]\n",
            "[[152   1 656 528   0]]\n",
            "[[ 91  54 261 342   1]\n",
            " [305  77 417 393   1]\n",
            " [337  37 488 294   1]\n",
            " [209  90 320 407   1]]\n",
            "epoch:11 step:   48/60, lr:0.000044, giou_loss:   3.86, conf_loss:  27.50, prob_loss:   3.69, total_loss:  35.05\n",
            "[[ 160  176  848  696    1]\n",
            " [ 586  176 1015  696    1]]\n",
            "[[374 161 493 276   0]\n",
            " [222 181 353 306   0]\n",
            " [191  64 297 159   0]\n",
            " [ 95   1 230 105   0]\n",
            " [ 20  46 141 162   0]\n",
            " [123 104 237 211   0]]\n",
            "[[ 49  78 356 410   0]]\n",
            "[[352  30 808 399   2]]\n",
            "epoch:11 step:   49/60, lr:0.000043, giou_loss:   4.51, conf_loss:  30.32, prob_loss:   5.49, total_loss:  40.31\n",
            "[[ 332   36  755  462    2]\n",
            " [  20  120  500  604    0]\n",
            " [ 130  208 1019  757    1]]\n",
            "[[ 87 137 325 415   2]\n",
            " [347 127 603 398   2]]\n",
            "[[ 26   8 172 151   2]\n",
            " [ 15 150 321 232   1]\n",
            " [176   3 313 148   0]]\n",
            "[[  3  52 342 254   1]]\n",
            "epoch:11 step:   50/60, lr:0.000043, giou_loss:   2.47, conf_loss:  25.58, prob_loss:   2.37, total_loss:  30.42\n",
            "[[283 118 474 307   2]\n",
            " [201 110 361 288   2]]\n",
            "[[ 93  79 272 249   2]]\n",
            "[[  1  52 406 394   1]]\n",
            "[[ 554  411 1509 1047    1]\n",
            " [ 728  615 1509 1123    1]\n",
            " [  10  126 1190  999    1]]\n",
            "epoch:11 step:   51/60, lr:0.000043, giou_loss:   1.88, conf_loss:  24.98, prob_loss:   4.29, total_loss:  31.15\n",
            "[[ 48 131 272 373   2]]\n",
            "[[  6  16 206 204   0]]\n",
            "[[ 79 135 255 324   0]]\n",
            "[[255  64 580 393   0]\n",
            " [ 39  31 310 322   0]]\n",
            "epoch:11 step:   52/60, lr:0.000043, giou_loss:   0.63, conf_loss:  23.38, prob_loss:   0.94, total_loss:  24.95\n",
            "[[216 138 298 226   2]\n",
            " [158 135 231 210   2]\n",
            " [ 28  21 258 117   1]\n",
            " [ 17 156 111 247   0]]\n",
            "[[ 18  15 341 349   0]]\n",
            "[[ 78  99 609 604   2]]\n",
            "[[ 45  28 451 447   0]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b62ac4a814f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mcur_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         print(\"epoch:{:2.0f} step:{:5.0f}/{}, lr:{:.6f}, giou_loss:{:7.2f}, conf_loss:{:7.2f}, prob_loss:{:7.2f}, total_loss:{:7.2f}\"\n",
            "\u001b[0;32m<ipython-input-34-b62ac4a814f4>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(image_data, target)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mpred_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mgiou_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# optimizing process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m    451\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 452\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1095\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1096\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_call_info_injected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mconvolution_op\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         name=self.__class__.__name__)\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1153\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1285\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1288\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2763\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2764\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2765\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss"
      ],
      "metadata": {
        "id": "tImm0UtwnGMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "YOLO_INPUT_SIZE = 416\n",
        "input_size=YOLO_INPUT_SIZE\n",
        "\n",
        "\n",
        "\n",
        "image_path =\"./fruits/test/banana_77.jpg\" #image_info[0]\n",
        "\n",
        "yolo = Create_Yolov3(input_size=input_size, CLASSES=TRAIN_CLASSES)\n",
        "yolo.load_weights(\"./checkpoints/yolov3_custom\")\n",
        "\n",
        "\n",
        "\n",
        "img = detect_image(yolo, image_path, \"\", input_size=input_size, show=True, CLASSES=TRAIN_CLASSES, rectangle_colors=(255,0,0))\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "Dr1ENwqyfvU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path"
      ],
      "metadata": {
        "id": "KFEL2UH0fxvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}